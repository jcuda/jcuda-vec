//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-19805474
// Cuda compilation tools, release 7.5, V7.5.16
// Based on LLVM 3.4svn
//

.version 4.3
.target sm_20
.address_size 32

	// .globl	vec_set
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b32 __internal_trig_reduction_slowpathd_param_1
)
;
.func  (.param .b64 func_retval0) __internal_accurate_pow
(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
;
.func  (.param .b64 func_retval0) __internal_lgamma_pos
(
	.param .b64 __internal_lgamma_pos_param_0
)
;
.const .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.const .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};

.visible .entry vec_set(
	.param .u32 vec_set_param_0,
	.param .u32 vec_set_param_1,
	.param .f64 vec_set_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<10>;
	.reg .f64 	%fd<2>;


	ld.param.u32 	%r3, [vec_set_param_0];
	ld.param.u32 	%r2, [vec_set_param_1];
	ld.param.f64 	%fd1, [vec_set_param_2];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r4;
	setp.ge.u32	%p1, %r1, %r3;
	@%p1 bra 	BB0_2;

	cvta.to.global.u32 	%r7, %r2;
	shl.b32 	%r8, %r1, 3;
	add.s32 	%r9, %r7, %r8;
	st.global.f64 	[%r9], %fd1;

BB0_2:
	ret;
}

	// .globl	vec_add
.visible .entry vec_add(
	.param .u32 vec_add_param_0,
	.param .u32 vec_add_param_1,
	.param .u32 vec_add_param_2,
	.param .u32 vec_add_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<16>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r5, [vec_add_param_0];
	ld.param.u32 	%r2, [vec_add_param_1];
	ld.param.u32 	%r3, [vec_add_param_2];
	ld.param.u32 	%r4, [vec_add_param_3];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB1_2;

	cvta.to.global.u32 	%r9, %r3;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	cvta.to.global.u32 	%r12, %r4;
	add.s32 	%r13, %r12, %r10;
	ld.global.f64 	%fd1, [%r13];
	ld.global.f64 	%fd2, [%r11];
	add.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u32 	%r14, %r2;
	add.s32 	%r15, %r14, %r10;
	st.global.f64 	[%r15], %fd3;

BB1_2:
	ret;
}

	// .globl	vec_sub
.visible .entry vec_sub(
	.param .u32 vec_sub_param_0,
	.param .u32 vec_sub_param_1,
	.param .u32 vec_sub_param_2,
	.param .u32 vec_sub_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<16>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r5, [vec_sub_param_0];
	ld.param.u32 	%r2, [vec_sub_param_1];
	ld.param.u32 	%r3, [vec_sub_param_2];
	ld.param.u32 	%r4, [vec_sub_param_3];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB2_2;

	cvta.to.global.u32 	%r9, %r3;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	cvta.to.global.u32 	%r12, %r4;
	add.s32 	%r13, %r12, %r10;
	ld.global.f64 	%fd1, [%r13];
	ld.global.f64 	%fd2, [%r11];
	sub.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u32 	%r14, %r2;
	add.s32 	%r15, %r14, %r10;
	st.global.f64 	[%r15], %fd3;

BB2_2:
	ret;
}

	// .globl	vec_mul
.visible .entry vec_mul(
	.param .u32 vec_mul_param_0,
	.param .u32 vec_mul_param_1,
	.param .u32 vec_mul_param_2,
	.param .u32 vec_mul_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<16>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r5, [vec_mul_param_0];
	ld.param.u32 	%r2, [vec_mul_param_1];
	ld.param.u32 	%r3, [vec_mul_param_2];
	ld.param.u32 	%r4, [vec_mul_param_3];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB3_2;

	cvta.to.global.u32 	%r9, %r3;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	cvta.to.global.u32 	%r12, %r4;
	add.s32 	%r13, %r12, %r10;
	ld.global.f64 	%fd1, [%r13];
	ld.global.f64 	%fd2, [%r11];
	mul.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u32 	%r14, %r2;
	add.s32 	%r15, %r14, %r10;
	st.global.f64 	[%r15], %fd3;

BB3_2:
	ret;
}

	// .globl	vec_div
.visible .entry vec_div(
	.param .u32 vec_div_param_0,
	.param .u32 vec_div_param_1,
	.param .u32 vec_div_param_2,
	.param .u32 vec_div_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<16>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r5, [vec_div_param_0];
	ld.param.u32 	%r2, [vec_div_param_1];
	ld.param.u32 	%r3, [vec_div_param_2];
	ld.param.u32 	%r4, [vec_div_param_3];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB4_2;

	cvta.to.global.u32 	%r9, %r3;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	cvta.to.global.u32 	%r12, %r4;
	add.s32 	%r13, %r12, %r10;
	ld.global.f64 	%fd1, [%r13];
	ld.global.f64 	%fd2, [%r11];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u32 	%r14, %r2;
	add.s32 	%r15, %r14, %r10;
	st.global.f64 	[%r15], %fd3;

BB4_2:
	ret;
}

	// .globl	vec_negate
.visible .entry vec_negate(
	.param .u32 vec_negate_param_0,
	.param .u32 vec_negate_param_1,
	.param .u32 vec_negate_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<3>;


	ld.param.u32 	%r4, [vec_negate_param_0];
	ld.param.u32 	%r2, [vec_negate_param_1];
	ld.param.u32 	%r3, [vec_negate_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB5_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd1, [%r10];
	neg.f64 	%fd2, %fd1;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd2;

BB5_2:
	ret;
}

	// .globl	vec_addScalar
.visible .entry vec_addScalar(
	.param .u32 vec_addScalar_param_0,
	.param .u32 vec_addScalar_param_1,
	.param .u32 vec_addScalar_param_2,
	.param .f64 vec_addScalar_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r4, [vec_addScalar_param_0];
	ld.param.u32 	%r2, [vec_addScalar_param_1];
	ld.param.u32 	%r3, [vec_addScalar_param_2];
	ld.param.f64 	%fd1, [vec_addScalar_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB6_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd2, [%r10];
	add.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd3;

BB6_2:
	ret;
}

	// .globl	vec_subScalar
.visible .entry vec_subScalar(
	.param .u32 vec_subScalar_param_0,
	.param .u32 vec_subScalar_param_1,
	.param .u32 vec_subScalar_param_2,
	.param .f64 vec_subScalar_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r4, [vec_subScalar_param_0];
	ld.param.u32 	%r2, [vec_subScalar_param_1];
	ld.param.u32 	%r3, [vec_subScalar_param_2];
	ld.param.f64 	%fd1, [vec_subScalar_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB7_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd2, [%r10];
	sub.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd3;

BB7_2:
	ret;
}

	// .globl	vec_mulScalar
.visible .entry vec_mulScalar(
	.param .u32 vec_mulScalar_param_0,
	.param .u32 vec_mulScalar_param_1,
	.param .u32 vec_mulScalar_param_2,
	.param .f64 vec_mulScalar_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r4, [vec_mulScalar_param_0];
	ld.param.u32 	%r2, [vec_mulScalar_param_1];
	ld.param.u32 	%r3, [vec_mulScalar_param_2];
	ld.param.f64 	%fd1, [vec_mulScalar_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB8_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd2, [%r10];
	mul.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd3;

BB8_2:
	ret;
}

	// .globl	vec_divScalar
.visible .entry vec_divScalar(
	.param .u32 vec_divScalar_param_0,
	.param .u32 vec_divScalar_param_1,
	.param .u32 vec_divScalar_param_2,
	.param .f64 vec_divScalar_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r4, [vec_divScalar_param_0];
	ld.param.u32 	%r2, [vec_divScalar_param_1];
	ld.param.u32 	%r3, [vec_divScalar_param_2];
	ld.param.f64 	%fd1, [vec_divScalar_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB9_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd2, [%r10];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd3;

BB9_2:
	ret;
}

	// .globl	vec_scalarAdd
.visible .entry vec_scalarAdd(
	.param .u32 vec_scalarAdd_param_0,
	.param .u32 vec_scalarAdd_param_1,
	.param .f64 vec_scalarAdd_param_2,
	.param .u32 vec_scalarAdd_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r4, [vec_scalarAdd_param_0];
	ld.param.u32 	%r2, [vec_scalarAdd_param_1];
	ld.param.f64 	%fd1, [vec_scalarAdd_param_2];
	ld.param.u32 	%r3, [vec_scalarAdd_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB10_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd2, [%r10];
	add.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd3;

BB10_2:
	ret;
}

	// .globl	vec_scalarSub
.visible .entry vec_scalarSub(
	.param .u32 vec_scalarSub_param_0,
	.param .u32 vec_scalarSub_param_1,
	.param .f64 vec_scalarSub_param_2,
	.param .u32 vec_scalarSub_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r4, [vec_scalarSub_param_0];
	ld.param.u32 	%r2, [vec_scalarSub_param_1];
	ld.param.f64 	%fd1, [vec_scalarSub_param_2];
	ld.param.u32 	%r3, [vec_scalarSub_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB11_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd2, [%r10];
	sub.f64 	%fd3, %fd1, %fd2;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd3;

BB11_2:
	ret;
}

	// .globl	vec_scalarMul
.visible .entry vec_scalarMul(
	.param .u32 vec_scalarMul_param_0,
	.param .u32 vec_scalarMul_param_1,
	.param .f64 vec_scalarMul_param_2,
	.param .u32 vec_scalarMul_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r4, [vec_scalarMul_param_0];
	ld.param.u32 	%r2, [vec_scalarMul_param_1];
	ld.param.f64 	%fd1, [vec_scalarMul_param_2];
	ld.param.u32 	%r3, [vec_scalarMul_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB12_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd2, [%r10];
	mul.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd3;

BB12_2:
	ret;
}

	// .globl	vec_scalarDiv
.visible .entry vec_scalarDiv(
	.param .u32 vec_scalarDiv_param_0,
	.param .u32 vec_scalarDiv_param_1,
	.param .f64 vec_scalarDiv_param_2,
	.param .u32 vec_scalarDiv_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r4, [vec_scalarDiv_param_0];
	ld.param.u32 	%r2, [vec_scalarDiv_param_1];
	ld.param.f64 	%fd1, [vec_scalarDiv_param_2];
	ld.param.u32 	%r3, [vec_scalarDiv_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB13_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd2, [%r10];
	div.rn.f64 	%fd3, %fd1, %fd2;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd3;

BB13_2:
	ret;
}

	// .globl	vec_lt
.visible .entry vec_lt(
	.param .u32 vec_lt_param_0,
	.param .u32 vec_lt_param_1,
	.param .u32 vec_lt_param_2,
	.param .u32 vec_lt_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<16>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r5, [vec_lt_param_0];
	ld.param.u32 	%r2, [vec_lt_param_1];
	ld.param.u32 	%r3, [vec_lt_param_2];
	ld.param.u32 	%r4, [vec_lt_param_3];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB14_2;

	cvta.to.global.u32 	%r9, %r3;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	cvta.to.global.u32 	%r12, %r4;
	add.s32 	%r13, %r12, %r10;
	ld.global.f64 	%fd1, [%r13];
	ld.global.f64 	%fd2, [%r11];
	setp.lt.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	cvta.to.global.u32 	%r14, %r2;
	add.s32 	%r15, %r14, %r10;
	st.global.f64 	[%r15], %fd3;

BB14_2:
	ret;
}

	// .globl	vec_lte
.visible .entry vec_lte(
	.param .u32 vec_lte_param_0,
	.param .u32 vec_lte_param_1,
	.param .u32 vec_lte_param_2,
	.param .u32 vec_lte_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<16>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r5, [vec_lte_param_0];
	ld.param.u32 	%r2, [vec_lte_param_1];
	ld.param.u32 	%r3, [vec_lte_param_2];
	ld.param.u32 	%r4, [vec_lte_param_3];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB15_2;

	cvta.to.global.u32 	%r9, %r3;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	cvta.to.global.u32 	%r12, %r4;
	add.s32 	%r13, %r12, %r10;
	ld.global.f64 	%fd1, [%r13];
	ld.global.f64 	%fd2, [%r11];
	setp.gtu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d0000000000000000, 0d3FF0000000000000, %p2;
	cvta.to.global.u32 	%r14, %r2;
	add.s32 	%r15, %r14, %r10;
	st.global.f64 	[%r15], %fd3;

BB15_2:
	ret;
}

	// .globl	vec_eq
.visible .entry vec_eq(
	.param .u32 vec_eq_param_0,
	.param .u32 vec_eq_param_1,
	.param .u32 vec_eq_param_2,
	.param .u32 vec_eq_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<16>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r5, [vec_eq_param_0];
	ld.param.u32 	%r2, [vec_eq_param_1];
	ld.param.u32 	%r3, [vec_eq_param_2];
	ld.param.u32 	%r4, [vec_eq_param_3];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB16_2;

	cvta.to.global.u32 	%r9, %r3;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	cvta.to.global.u32 	%r12, %r4;
	add.s32 	%r13, %r12, %r10;
	ld.global.f64 	%fd1, [%r13];
	ld.global.f64 	%fd2, [%r11];
	setp.eq.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	cvta.to.global.u32 	%r14, %r2;
	add.s32 	%r15, %r14, %r10;
	st.global.f64 	[%r15], %fd3;

BB16_2:
	ret;
}

	// .globl	vec_gte
.visible .entry vec_gte(
	.param .u32 vec_gte_param_0,
	.param .u32 vec_gte_param_1,
	.param .u32 vec_gte_param_2,
	.param .u32 vec_gte_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<16>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r5, [vec_gte_param_0];
	ld.param.u32 	%r2, [vec_gte_param_1];
	ld.param.u32 	%r3, [vec_gte_param_2];
	ld.param.u32 	%r4, [vec_gte_param_3];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB17_2;

	cvta.to.global.u32 	%r9, %r3;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	cvta.to.global.u32 	%r12, %r4;
	add.s32 	%r13, %r12, %r10;
	ld.global.f64 	%fd1, [%r13];
	ld.global.f64 	%fd2, [%r11];
	setp.ltu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d0000000000000000, 0d3FF0000000000000, %p2;
	cvta.to.global.u32 	%r14, %r2;
	add.s32 	%r15, %r14, %r10;
	st.global.f64 	[%r15], %fd3;

BB17_2:
	ret;
}

	// .globl	vec_gt
.visible .entry vec_gt(
	.param .u32 vec_gt_param_0,
	.param .u32 vec_gt_param_1,
	.param .u32 vec_gt_param_2,
	.param .u32 vec_gt_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<16>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r5, [vec_gt_param_0];
	ld.param.u32 	%r2, [vec_gt_param_1];
	ld.param.u32 	%r3, [vec_gt_param_2];
	ld.param.u32 	%r4, [vec_gt_param_3];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB18_2;

	cvta.to.global.u32 	%r9, %r3;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	cvta.to.global.u32 	%r12, %r4;
	add.s32 	%r13, %r12, %r10;
	ld.global.f64 	%fd1, [%r13];
	ld.global.f64 	%fd2, [%r11];
	setp.gt.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	cvta.to.global.u32 	%r14, %r2;
	add.s32 	%r15, %r14, %r10;
	st.global.f64 	[%r15], %fd3;

BB18_2:
	ret;
}

	// .globl	vec_ne
.visible .entry vec_ne(
	.param .u32 vec_ne_param_0,
	.param .u32 vec_ne_param_1,
	.param .u32 vec_ne_param_2,
	.param .u32 vec_ne_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<16>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r5, [vec_ne_param_0];
	ld.param.u32 	%r2, [vec_ne_param_1];
	ld.param.u32 	%r3, [vec_ne_param_2];
	ld.param.u32 	%r4, [vec_ne_param_3];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB19_2;

	cvta.to.global.u32 	%r9, %r3;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	cvta.to.global.u32 	%r12, %r4;
	add.s32 	%r13, %r12, %r10;
	ld.global.f64 	%fd1, [%r13];
	ld.global.f64 	%fd2, [%r11];
	setp.neu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	cvta.to.global.u32 	%r14, %r2;
	add.s32 	%r15, %r14, %r10;
	st.global.f64 	[%r15], %fd3;

BB19_2:
	ret;
}

	// .globl	vec_ltScalar
.visible .entry vec_ltScalar(
	.param .u32 vec_ltScalar_param_0,
	.param .u32 vec_ltScalar_param_1,
	.param .u32 vec_ltScalar_param_2,
	.param .f64 vec_ltScalar_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r4, [vec_ltScalar_param_0];
	ld.param.u32 	%r2, [vec_ltScalar_param_1];
	ld.param.u32 	%r3, [vec_ltScalar_param_2];
	ld.param.f64 	%fd1, [vec_ltScalar_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB20_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd2, [%r10];
	setp.lt.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd3;

BB20_2:
	ret;
}

	// .globl	vec_lteScalar
.visible .entry vec_lteScalar(
	.param .u32 vec_lteScalar_param_0,
	.param .u32 vec_lteScalar_param_1,
	.param .u32 vec_lteScalar_param_2,
	.param .f64 vec_lteScalar_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r4, [vec_lteScalar_param_0];
	ld.param.u32 	%r2, [vec_lteScalar_param_1];
	ld.param.u32 	%r3, [vec_lteScalar_param_2];
	ld.param.f64 	%fd1, [vec_lteScalar_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB21_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd2, [%r10];
	setp.gtu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d0000000000000000, 0d3FF0000000000000, %p2;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd3;

BB21_2:
	ret;
}

	// .globl	vec_eqScalar
.visible .entry vec_eqScalar(
	.param .u32 vec_eqScalar_param_0,
	.param .u32 vec_eqScalar_param_1,
	.param .u32 vec_eqScalar_param_2,
	.param .f64 vec_eqScalar_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r4, [vec_eqScalar_param_0];
	ld.param.u32 	%r2, [vec_eqScalar_param_1];
	ld.param.u32 	%r3, [vec_eqScalar_param_2];
	ld.param.f64 	%fd1, [vec_eqScalar_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB22_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd2, [%r10];
	setp.eq.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd3;

BB22_2:
	ret;
}

	// .globl	vec_gteScalar
.visible .entry vec_gteScalar(
	.param .u32 vec_gteScalar_param_0,
	.param .u32 vec_gteScalar_param_1,
	.param .u32 vec_gteScalar_param_2,
	.param .f64 vec_gteScalar_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r4, [vec_gteScalar_param_0];
	ld.param.u32 	%r2, [vec_gteScalar_param_1];
	ld.param.u32 	%r3, [vec_gteScalar_param_2];
	ld.param.f64 	%fd1, [vec_gteScalar_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB23_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd2, [%r10];
	setp.ltu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d0000000000000000, 0d3FF0000000000000, %p2;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd3;

BB23_2:
	ret;
}

	// .globl	vec_gtScalar
.visible .entry vec_gtScalar(
	.param .u32 vec_gtScalar_param_0,
	.param .u32 vec_gtScalar_param_1,
	.param .u32 vec_gtScalar_param_2,
	.param .f64 vec_gtScalar_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r4, [vec_gtScalar_param_0];
	ld.param.u32 	%r2, [vec_gtScalar_param_1];
	ld.param.u32 	%r3, [vec_gtScalar_param_2];
	ld.param.f64 	%fd1, [vec_gtScalar_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB24_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd2, [%r10];
	setp.gt.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd3;

BB24_2:
	ret;
}

	// .globl	vec_neScalar
.visible .entry vec_neScalar(
	.param .u32 vec_neScalar_param_0,
	.param .u32 vec_neScalar_param_1,
	.param .u32 vec_neScalar_param_2,
	.param .f64 vec_neScalar_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r4, [vec_neScalar_param_0];
	ld.param.u32 	%r2, [vec_neScalar_param_1];
	ld.param.u32 	%r3, [vec_neScalar_param_2];
	ld.param.f64 	%fd1, [vec_neScalar_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB25_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd2, [%r10];
	setp.neu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd3;

BB25_2:
	ret;
}

	// .globl	vec_acos
.visible .entry vec_acos(
	.param .u32 vec_acos_param_0,
	.param .u32 vec_acos_param_1,
	.param .u32 vec_acos_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<25>;
	.reg .f64 	%fd<95>;


	ld.param.u32 	%r6, [vec_acos_param_0];
	ld.param.u32 	%r4, [vec_acos_param_1];
	ld.param.u32 	%r5, [vec_acos_param_2];
	mov.u32 	%r7, %tid.x;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r1, %r8, %r9, %r7;
	setp.ge.u32	%p1, %r1, %r6;
	@%p1 bra 	BB26_14;

	cvta.to.global.u32 	%r10, %r5;
	shl.b32 	%r11, %r1, 3;
	add.s32 	%r12, %r10, %r11;
	ld.global.f64 	%fd16, [%r12];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd16;
	}
	abs.f64 	%fd1, %fd16;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd1;
	}
	setp.lt.s32	%p2, %r13, 1071801958;
	@%p2 bra 	BB26_9;
	bra.uni 	BB26_2;

BB26_9:
	mul.f64 	%fd62, %fd1, %fd1;
	mov.f64 	%fd63, 0dBFB3823B180754AF;
	mov.f64 	%fd64, 0d3FB0066BDC1895E9;
	fma.rn.f64 	%fd65, %fd64, %fd62, %fd63;
	mov.f64 	%fd66, 0d3FB11E52CC2F79AE;
	fma.rn.f64 	%fd67, %fd65, %fd62, %fd66;
	mov.f64 	%fd68, 0dBF924EAF3526861B;
	fma.rn.f64 	%fd69, %fd67, %fd62, %fd68;
	mov.f64 	%fd70, 0d3F91DF02A31E6CB7;
	fma.rn.f64 	%fd71, %fd69, %fd62, %fd70;
	mov.f64 	%fd72, 0d3F847D18B0EEC6CC;
	fma.rn.f64 	%fd73, %fd71, %fd62, %fd72;
	mov.f64 	%fd74, 0d3F8D0AF961BA53B0;
	fma.rn.f64 	%fd75, %fd73, %fd62, %fd74;
	mov.f64 	%fd76, 0d3F91BF7734CF1C48;
	fma.rn.f64 	%fd77, %fd75, %fd62, %fd76;
	mov.f64 	%fd78, 0d3F96E91483144EF7;
	fma.rn.f64 	%fd79, %fd77, %fd62, %fd78;
	mov.f64 	%fd80, 0d3F9F1C6E0A4F9F81;
	fma.rn.f64 	%fd81, %fd79, %fd62, %fd80;
	mov.f64 	%fd82, 0d3FA6DB6DC27FA92B;
	fma.rn.f64 	%fd83, %fd81, %fd62, %fd82;
	mov.f64 	%fd84, 0d3FB333333320F91B;
	fma.rn.f64 	%fd85, %fd83, %fd62, %fd84;
	mov.f64 	%fd86, 0d3FC5555555555F4D;
	fma.rn.f64 	%fd87, %fd85, %fd62, %fd86;
	mul.f64 	%fd88, %fd62, %fd87;
	fma.rn.f64 	%fd10, %fd88, %fd1, %fd1;
	setp.lt.s32	%p6, %r2, 0;
	@%p6 bra 	BB26_11;

	mov.f64 	%fd89, 0dBC91A62633145C07;
	add.rn.f64 	%fd90, %fd10, %fd89;
	neg.f64 	%fd93, %fd90;
	bra.uni 	BB26_12;

BB26_2:
	mov.f64 	%fd19, 0d3FF0000000000000;
	sub.f64 	%fd2, %fd19, %fd1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd2;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd2;
	}
	add.s32 	%r15, %r3, -1048576;
	mov.b64 	%fd18, {%r14, %r15};
	// inline asm
	rsqrt.approx.ftz.f64 %fd17, %fd18;
	// inline asm
	{
	.reg .b32 %temp; 
	mov.b64 	{%r16, %temp}, %fd17;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r17}, %fd17;
	}
	add.s32 	%r18, %r17, -1048576;
	mov.b64 	%fd20, {%r16, %r18};
	mul.f64 	%fd21, %fd18, %fd17;
	neg.f64 	%fd22, %fd21;
	fma.rn.f64 	%fd23, %fd21, %fd22, %fd18;
	fma.rn.f64 	%fd24, %fd23, %fd20, %fd21;
	neg.f64 	%fd25, %fd24;
	fma.rn.f64 	%fd26, %fd17, %fd25, %fd19;
	fma.rn.f64 	%fd27, %fd26, %fd20, %fd20;
	fma.rn.f64 	%fd28, %fd24, %fd25, %fd18;
	fma.rn.f64 	%fd3, %fd28, %fd27, %fd24;
	setp.lt.s32	%p3, %r3, 1;
	@%p3 bra 	BB26_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd3;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd3;
	}
	add.s32 	%r21, %r20, 1048576;
	mov.b64 	%fd29, {%r19, %r21};
	mov.f64 	%fd30, 0dBEBAC2FE66FAAC4B;
	mov.f64 	%fd31, 0d3EC715B371155F70;
	fma.rn.f64 	%fd32, %fd31, %fd2, %fd30;
	mov.f64 	%fd33, 0d3ED9A9B88EFCD9B8;
	fma.rn.f64 	%fd34, %fd32, %fd2, %fd33;
	mov.f64 	%fd35, 0d3EDD0F40A8A0C4C3;
	fma.rn.f64 	%fd36, %fd34, %fd2, %fd35;
	mov.f64 	%fd37, 0d3EF46D4CFA9E0E1F;
	fma.rn.f64 	%fd38, %fd36, %fd2, %fd37;
	mov.f64 	%fd39, 0d3F079C168D1E2422;
	fma.rn.f64 	%fd40, %fd38, %fd2, %fd39;
	mov.f64 	%fd41, 0d3F1C9A88C3BCA540;
	fma.rn.f64 	%fd42, %fd40, %fd2, %fd41;
	mov.f64 	%fd43, 0d3F31C4E64BD476DF;
	fma.rn.f64 	%fd44, %fd42, %fd2, %fd43;
	mov.f64 	%fd45, 0d3F46E8BA60009C8F;
	fma.rn.f64 	%fd46, %fd44, %fd2, %fd45;
	mov.f64 	%fd47, 0d3F5F1C71C62B05A2;
	fma.rn.f64 	%fd48, %fd46, %fd2, %fd47;
	mov.f64 	%fd49, 0d3F76DB6DB6DC9F2C;
	fma.rn.f64 	%fd50, %fd48, %fd2, %fd49;
	mov.f64 	%fd51, 0d3F9333333333329C;
	fma.rn.f64 	%fd52, %fd50, %fd2, %fd51;
	mov.f64 	%fd53, 0d3FB5555555555555;
	fma.rn.f64 	%fd54, %fd52, %fd2, %fd53;
	mul.f64 	%fd55, %fd2, %fd54;
	fma.rn.f64 	%fd94, %fd55, %fd29, %fd29;
	bra.uni 	BB26_5;

BB26_11:
	mov.f64 	%fd91, 0d3C91A62633145C07;
	add.rn.f64 	%fd93, %fd10, %fd91;

BB26_12:
	mov.f64 	%fd92, 0d3FF921FB54442D18;
	add.rn.f64 	%fd94, %fd92, %fd93;
	bra.uni 	BB26_13;

BB26_4:
	mov.f64 	%fd56, 0d0000000000000000;
	mul.rn.f64 	%fd94, %fd1, %fd56;

BB26_5:
	setp.gt.s32	%p4, %r3, -1;
	@%p4 bra 	BB26_7;

	mov.f64 	%fd57, 0d7FF0000000000000;
	mul.rn.f64 	%fd94, %fd94, %fd57;

BB26_7:
	setp.gt.s32	%p5, %r2, -1;
	@%p5 bra 	BB26_13;

	mov.f64 	%fd58, 0dBCA1A62633145C07;
	add.rn.f64 	%fd59, %fd94, %fd58;
	neg.f64 	%fd60, %fd59;
	mov.f64 	%fd61, 0d400921FB54442D18;
	add.rn.f64 	%fd94, %fd61, %fd60;

BB26_13:
	cvta.to.global.u32 	%r22, %r4;
	add.s32 	%r24, %r22, %r11;
	st.global.f64 	[%r24], %fd94;

BB26_14:
	ret;
}

	// .globl	vec_acosh
.visible .entry vec_acosh(
	.param .u32 vec_acosh_param_0,
	.param .u32 vec_acosh_param_1,
	.param .u32 vec_acosh_param_2
)
{
	.reg .pred 	%p<23>;
	.reg .b32 	%r<68>;
	.reg .f64 	%fd<158>;


	ld.param.u32 	%r24, [vec_acosh_param_0];
	ld.param.u32 	%r22, [vec_acosh_param_1];
	ld.param.u32 	%r23, [vec_acosh_param_2];
	mov.u32 	%r25, %tid.x;
	mov.u32 	%r26, %ntid.x;
	mov.u32 	%r27, %ctaid.x;
	mad.lo.s32 	%r1, %r26, %r27, %r25;
	setp.ge.u32	%p1, %r1, %r24;
	@%p1 bra 	BB27_27;

	cvta.to.global.u32 	%r28, %r23;
	shl.b32 	%r29, %r1, 3;
	add.s32 	%r30, %r28, %r29;
	ld.global.f64 	%fd1, [%r30];
	add.f64 	%fd2, %fd1, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r64}, %fd2;
	}
	setp.gt.u32	%p2, %r64, 1127219199;
	@%p2 bra 	BB27_15;
	bra.uni 	BB27_2;

BB27_15:
	setp.gt.f64	%p15, %fd2, 0d0000000000000000;
	setp.lt.s32	%p16, %r64, 2146435072;
	and.pred  	%p17, %p15, %p16;
	@%p17 bra 	BB27_20;
	bra.uni 	BB27_16;

BB27_20:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r65, %temp}, %fd2;
	}
	mov.u32 	%r66, -1023;
	setp.gt.s32	%p21, %r64, 1048575;
	@%p21 bra 	BB27_22;

	mul.f64 	%fd108, %fd2, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r64}, %fd108;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r65, %temp}, %fd108;
	}
	mov.u32 	%r66, -1077;

BB27_22:
	shr.u32 	%r48, %r64, 20;
	add.s32 	%r67, %r66, %r48;
	and.b32  	%r49, %r64, -2146435073;
	or.b32  	%r50, %r49, 1072693248;
	mov.b64 	%fd155, {%r65, %r50};
	setp.lt.s32	%p22, %r50, 1073127583;
	@%p22 bra 	BB27_24;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r51, %temp}, %fd155;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r52}, %fd155;
	}
	add.s32 	%r53, %r52, -1048576;
	mov.b64 	%fd155, {%r51, %r53};
	add.s32 	%r67, %r67, 1;

BB27_24:
	add.f64 	%fd110, %fd155, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd109,%fd110;
	// inline asm
	neg.f64 	%fd111, %fd110;
	mov.f64 	%fd112, 0d3FF0000000000000;
	fma.rn.f64 	%fd113, %fd111, %fd109, %fd112;
	fma.rn.f64 	%fd114, %fd113, %fd113, %fd113;
	fma.rn.f64 	%fd115, %fd114, %fd109, %fd109;
	add.f64 	%fd116, %fd155, 0dBFF0000000000000;
	mul.f64 	%fd117, %fd116, %fd115;
	fma.rn.f64 	%fd118, %fd116, %fd115, %fd117;
	mul.f64 	%fd119, %fd118, %fd118;
	mov.f64 	%fd120, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd121, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd122, %fd121, %fd119, %fd120;
	mov.f64 	%fd123, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd124, %fd122, %fd119, %fd123;
	mov.f64 	%fd125, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd126, %fd124, %fd119, %fd125;
	mov.f64 	%fd127, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd128, %fd126, %fd119, %fd127;
	mov.f64 	%fd129, 0d3F624924923BE72D;
	fma.rn.f64 	%fd130, %fd128, %fd119, %fd129;
	mov.f64 	%fd131, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd132, %fd130, %fd119, %fd131;
	mov.f64 	%fd133, 0d3FB5555555555554;
	fma.rn.f64 	%fd134, %fd132, %fd119, %fd133;
	sub.f64 	%fd135, %fd116, %fd118;
	add.f64 	%fd136, %fd135, %fd135;
	neg.f64 	%fd137, %fd118;
	fma.rn.f64 	%fd138, %fd137, %fd116, %fd136;
	mul.f64 	%fd139, %fd115, %fd138;
	mul.f64 	%fd140, %fd119, %fd134;
	fma.rn.f64 	%fd141, %fd140, %fd118, %fd139;
	xor.b32  	%r54, %r67, -2147483648;
	mov.u32 	%r55, 1127219200;
	mov.b64 	%fd142, {%r54, %r55};
	mov.u32 	%r56, -2147483648;
	mov.b64 	%fd143, {%r56, %r55};
	sub.f64 	%fd144, %fd142, %fd143;
	mov.f64 	%fd145, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd146, %fd144, %fd145, %fd118;
	neg.f64 	%fd147, %fd144;
	fma.rn.f64 	%fd148, %fd147, %fd145, %fd146;
	sub.f64 	%fd149, %fd148, %fd118;
	sub.f64 	%fd150, %fd141, %fd149;
	mov.f64 	%fd151, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd152, %fd144, %fd151, %fd150;
	add.f64 	%fd156, %fd146, %fd152;
	bra.uni 	BB27_25;

BB27_2:
	fma.rn.f64 	%fd24, %fd1, %fd2, %fd2;
	// inline asm
	rsqrt.approx.ftz.f64 %fd23, %fd24;
	// inline asm
	{
	.reg .b32 %temp; 
	mov.b64 	{%r31, %temp}, %fd23;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r32}, %fd23;
	}
	add.s32 	%r33, %r32, -1048576;
	mov.b64 	%fd25, {%r31, %r33};
	mul.f64 	%fd26, %fd24, %fd23;
	neg.f64 	%fd27, %fd26;
	fma.rn.f64 	%fd28, %fd26, %fd27, %fd24;
	fma.rn.f64 	%fd29, %fd28, %fd25, %fd26;
	neg.f64 	%fd30, %fd29;
	mov.f64 	%fd31, 0d3FF0000000000000;
	fma.rn.f64 	%fd32, %fd23, %fd30, %fd31;
	fma.rn.f64 	%fd33, %fd32, %fd25, %fd25;
	fma.rn.f64 	%fd34, %fd29, %fd30, %fd24;
	fma.rn.f64 	%fd35, %fd34, %fd33, %fd29;
	add.f64 	%fd3, %fd2, %fd35;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r34}, %fd3;
	}
	setp.lt.u32	%p3, %r34, 1071994197;
	setp.lt.s32	%p4, %r34, -1076258407;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB27_13;
	bra.uni 	BB27_3;

BB27_13:
	add.f64 	%fd83, %fd3, 0d4000000000000000;
	div.rn.f64 	%fd84, %fd3, %fd83;
	mul.f64 	%fd85, %fd3, %fd84;
	neg.f64 	%fd86, %fd85;
	sub.f64 	%fd87, %fd3, %fd85;
	mul.f64 	%fd88, %fd87, %fd87;
	mov.f64 	%fd89, 0d3ED087FFCEB2DC44;
	mov.f64 	%fd90, 0d3EB372FB2FBE14B5;
	fma.rn.f64 	%fd91, %fd90, %fd88, %fd89;
	mov.f64 	%fd92, 0d3EF3B9FF890F468C;
	fma.rn.f64 	%fd93, %fd91, %fd88, %fd92;
	mov.f64 	%fd94, 0d3F17457EFD51BAF8;
	fma.rn.f64 	%fd95, %fd93, %fd88, %fd94;
	mov.f64 	%fd96, 0d3F3C71C8DE3CE825;
	fma.rn.f64 	%fd97, %fd95, %fd88, %fd96;
	mov.f64 	%fd98, 0d3F6249248FA4661F;
	fma.rn.f64 	%fd99, %fd97, %fd88, %fd98;
	mov.f64 	%fd100, 0d3F899999999D70C4;
	fma.rn.f64 	%fd101, %fd99, %fd88, %fd100;
	mov.f64 	%fd102, 0d3FB5555555555462;
	fma.rn.f64 	%fd103, %fd101, %fd88, %fd102;
	mul.f64 	%fd104, %fd88, %fd103;
	fma.rn.f64 	%fd105, %fd104, %fd87, %fd86;
	add.f64 	%fd154, %fd3, %fd105;
	bra.uni 	BB27_14;

BB27_16:
	abs.f64 	%fd106, %fd2;
	setp.gtu.f64	%p18, %fd106, 0d7FF0000000000000;
	@%p18 bra 	BB27_19;
	bra.uni 	BB27_17;

BB27_19:
	add.f64 	%fd156, %fd2, %fd2;
	bra.uni 	BB27_25;

BB27_3:
	add.f64 	%fd4, %fd3, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r60}, %fd4;
	}
	setp.gt.f64	%p6, %fd4, 0d0000000000000000;
	setp.lt.s32	%p7, %r60, 2146435072;
	and.pred  	%p8, %p6, %p7;
	@%p8 bra 	BB27_8;
	bra.uni 	BB27_4;

BB27_8:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r61, %temp}, %fd4;
	}
	mov.u32 	%r62, -1023;
	setp.gt.s32	%p12, %r60, 1048575;
	@%p12 bra 	BB27_10;

	mul.f64 	%fd38, %fd4, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r60}, %fd38;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r61, %temp}, %fd38;
	}
	mov.u32 	%r62, -1077;

BB27_10:
	shr.u32 	%r37, %r60, 20;
	add.s32 	%r63, %r62, %r37;
	and.b32  	%r38, %r60, -2146435073;
	or.b32  	%r39, %r38, 1072693248;
	mov.b64 	%fd153, {%r61, %r39};
	setp.lt.s32	%p13, %r39, 1073127583;
	@%p13 bra 	BB27_12;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r40, %temp}, %fd153;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r41}, %fd153;
	}
	add.s32 	%r42, %r41, -1048576;
	mov.b64 	%fd153, {%r40, %r42};
	add.s32 	%r63, %r63, 1;

BB27_12:
	add.f64 	%fd40, %fd153, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd39,%fd40;
	// inline asm
	neg.f64 	%fd41, %fd40;
	fma.rn.f64 	%fd43, %fd41, %fd39, %fd31;
	fma.rn.f64 	%fd44, %fd43, %fd43, %fd43;
	fma.rn.f64 	%fd45, %fd44, %fd39, %fd39;
	add.f64 	%fd46, %fd153, 0dBFF0000000000000;
	mul.f64 	%fd47, %fd46, %fd45;
	fma.rn.f64 	%fd48, %fd46, %fd45, %fd47;
	mul.f64 	%fd49, %fd48, %fd48;
	mov.f64 	%fd50, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd51, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd52, %fd51, %fd49, %fd50;
	mov.f64 	%fd53, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd54, %fd52, %fd49, %fd53;
	mov.f64 	%fd55, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd56, %fd54, %fd49, %fd55;
	mov.f64 	%fd57, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd58, %fd56, %fd49, %fd57;
	mov.f64 	%fd59, 0d3F624924923BE72D;
	fma.rn.f64 	%fd60, %fd58, %fd49, %fd59;
	mov.f64 	%fd61, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd62, %fd60, %fd49, %fd61;
	mov.f64 	%fd63, 0d3FB5555555555554;
	fma.rn.f64 	%fd64, %fd62, %fd49, %fd63;
	sub.f64 	%fd65, %fd46, %fd48;
	add.f64 	%fd66, %fd65, %fd65;
	neg.f64 	%fd67, %fd48;
	fma.rn.f64 	%fd68, %fd67, %fd46, %fd66;
	mul.f64 	%fd69, %fd45, %fd68;
	mul.f64 	%fd70, %fd49, %fd64;
	fma.rn.f64 	%fd71, %fd70, %fd48, %fd69;
	xor.b32  	%r43, %r63, -2147483648;
	mov.u32 	%r44, 1127219200;
	mov.b64 	%fd72, {%r43, %r44};
	mov.u32 	%r45, -2147483648;
	mov.b64 	%fd73, {%r45, %r44};
	sub.f64 	%fd74, %fd72, %fd73;
	mov.f64 	%fd75, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd76, %fd74, %fd75, %fd48;
	neg.f64 	%fd77, %fd74;
	fma.rn.f64 	%fd78, %fd77, %fd75, %fd76;
	sub.f64 	%fd79, %fd78, %fd48;
	sub.f64 	%fd80, %fd71, %fd79;
	mov.f64 	%fd81, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd82, %fd74, %fd81, %fd80;
	add.f64 	%fd154, %fd76, %fd82;
	bra.uni 	BB27_14;

BB27_17:
	setp.eq.f64	%p19, %fd2, 0d0000000000000000;
	mov.f64 	%fd156, 0dFFF0000000000000;
	@%p19 bra 	BB27_25;

	setp.eq.f64	%p20, %fd2, 0d7FF0000000000000;
	selp.f64	%fd156, %fd2, 0dFFF8000000000000, %p20;

BB27_25:
	add.f64 	%fd157, %fd156, 0d3FE62E42FEFA39EF;
	bra.uni 	BB27_26;

BB27_4:
	abs.f64 	%fd36, %fd4;
	setp.gtu.f64	%p9, %fd36, 0d7FF0000000000000;
	@%p9 bra 	BB27_7;
	bra.uni 	BB27_5;

BB27_7:
	add.f64 	%fd154, %fd4, %fd4;
	bra.uni 	BB27_14;

BB27_5:
	setp.eq.f64	%p10, %fd4, 0d0000000000000000;
	mov.f64 	%fd154, 0dFFF0000000000000;
	@%p10 bra 	BB27_14;

	setp.eq.f64	%p11, %fd4, 0d7FF0000000000000;
	selp.f64	%fd154, %fd4, 0dFFF8000000000000, %p11;

BB27_14:
	setp.eq.s32	%p14, %r64, 0;
	selp.f64	%fd157, %fd2, %fd154, %p14;

BB27_26:
	cvta.to.global.u32 	%r57, %r22;
	add.s32 	%r59, %r57, %r29;
	st.global.f64 	[%r59], %fd157;

BB27_27:
	ret;
}

	// .globl	vec_asin
.visible .entry vec_asin(
	.param .u32 vec_asin_param_0,
	.param .u32 vec_asin_param_1,
	.param .u32 vec_asin_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<83>;


	ld.param.u32 	%r5, [vec_asin_param_0];
	ld.param.u32 	%r3, [vec_asin_param_1];
	ld.param.u32 	%r4, [vec_asin_param_2];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB28_5;

	cvta.to.global.u32 	%r9, %r4;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	ld.global.f64 	%fd1, [%r11];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	mov.b32 	 %f1, %r2;
	abs.f32 	%f2, %f1;
	setp.lt.f32	%p2, %f2, 0f3FE26666;
	@%p2 bra 	BB28_3;
	bra.uni 	BB28_2;

BB28_3:
	mul.f64 	%fd55, %fd1, %fd1;
	mov.f64 	%fd56, 0dBFB3823B180754AF;
	mov.f64 	%fd57, 0d3FB0066BDC1895E9;
	fma.rn.f64 	%fd58, %fd57, %fd55, %fd56;
	mov.f64 	%fd59, 0d3FB11E52CC2F79AE;
	fma.rn.f64 	%fd60, %fd58, %fd55, %fd59;
	mov.f64 	%fd61, 0dBF924EAF3526861B;
	fma.rn.f64 	%fd62, %fd60, %fd55, %fd61;
	mov.f64 	%fd63, 0d3F91DF02A31E6CB7;
	fma.rn.f64 	%fd64, %fd62, %fd55, %fd63;
	mov.f64 	%fd65, 0d3F847D18B0EEC6CC;
	fma.rn.f64 	%fd66, %fd64, %fd55, %fd65;
	mov.f64 	%fd67, 0d3F8D0AF961BA53B0;
	fma.rn.f64 	%fd68, %fd66, %fd55, %fd67;
	mov.f64 	%fd69, 0d3F91BF7734CF1C48;
	fma.rn.f64 	%fd70, %fd68, %fd55, %fd69;
	mov.f64 	%fd71, 0d3F96E91483144EF7;
	fma.rn.f64 	%fd72, %fd70, %fd55, %fd71;
	mov.f64 	%fd73, 0d3F9F1C6E0A4F9F81;
	fma.rn.f64 	%fd74, %fd72, %fd55, %fd73;
	mov.f64 	%fd75, 0d3FA6DB6DC27FA92B;
	fma.rn.f64 	%fd76, %fd74, %fd55, %fd75;
	mov.f64 	%fd77, 0d3FB333333320F91B;
	fma.rn.f64 	%fd78, %fd76, %fd55, %fd77;
	mov.f64 	%fd79, 0d3FC5555555555F4D;
	fma.rn.f64 	%fd80, %fd78, %fd55, %fd79;
	mul.f64 	%fd81, %fd55, %fd80;
	fma.rn.f64 	%fd82, %fd81, %fd1, %fd1;
	bra.uni 	BB28_4;

BB28_2:
	abs.f64 	%fd7, %fd1;
	mov.f64 	%fd8, 0d3FE0000000000000;
	mov.f64 	%fd9, 0dBFE0000000000000;
	fma.rn.f64 	%fd6, %fd9, %fd7, %fd8;
	// inline asm
	rsqrt.approx.ftz.f64 %fd5, %fd6;
	// inline asm
	{
	.reg .b32 %temp; 
	mov.b64 	{%r12, %temp}, %fd5;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd5;
	}
	add.s32 	%r14, %r13, -1048576;
	mov.b64 	%fd10, {%r12, %r14};
	mul.f64 	%fd11, %fd6, %fd5;
	neg.f64 	%fd12, %fd11;
	fma.rn.f64 	%fd13, %fd11, %fd12, %fd6;
	fma.rn.f64 	%fd14, %fd13, %fd10, %fd11;
	neg.f64 	%fd15, %fd14;
	mov.f64 	%fd16, 0d3FF0000000000000;
	fma.rn.f64 	%fd17, %fd5, %fd15, %fd16;
	fma.rn.f64 	%fd18, %fd17, %fd10, %fd10;
	fma.rn.f64 	%fd19, %fd14, %fd15, %fd6;
	fma.rn.f64 	%fd20, %fd19, %fd18, %fd14;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd6;
	}
	setp.lt.s32	%p3, %r15, 0;
	selp.f64	%fd21, 0dFFF8000000000000, %fd20, %p3;
	setp.equ.f64	%p4, %fd6, 0d0000000000000000;
	selp.f64	%fd22, %fd6, %fd21, %p4;
	mov.f64 	%fd23, 0dBFB3823B180754AF;
	mov.f64 	%fd24, 0d3FB0066BDC1895E9;
	fma.rn.f64 	%fd25, %fd24, %fd6, %fd23;
	mov.f64 	%fd26, 0d3FB11E52CC2F79AE;
	fma.rn.f64 	%fd27, %fd25, %fd6, %fd26;
	mov.f64 	%fd28, 0dBF924EAF3526861B;
	fma.rn.f64 	%fd29, %fd27, %fd6, %fd28;
	mov.f64 	%fd30, 0d3F91DF02A31E6CB7;
	fma.rn.f64 	%fd31, %fd29, %fd6, %fd30;
	mov.f64 	%fd32, 0d3F847D18B0EEC6CC;
	fma.rn.f64 	%fd33, %fd31, %fd6, %fd32;
	mov.f64 	%fd34, 0d3F8D0AF961BA53B0;
	fma.rn.f64 	%fd35, %fd33, %fd6, %fd34;
	mov.f64 	%fd36, 0d3F91BF7734CF1C48;
	fma.rn.f64 	%fd37, %fd35, %fd6, %fd36;
	mov.f64 	%fd38, 0d3F96E91483144EF7;
	fma.rn.f64 	%fd39, %fd37, %fd6, %fd38;
	mov.f64 	%fd40, 0d3F9F1C6E0A4F9F81;
	fma.rn.f64 	%fd41, %fd39, %fd6, %fd40;
	mov.f64 	%fd42, 0d3FA6DB6DC27FA92B;
	fma.rn.f64 	%fd43, %fd41, %fd6, %fd42;
	mov.f64 	%fd44, 0d3FB333333320F91B;
	fma.rn.f64 	%fd45, %fd43, %fd6, %fd44;
	mov.f64 	%fd46, 0d3FC5555555555F4D;
	fma.rn.f64 	%fd47, %fd45, %fd6, %fd46;
	mul.f64 	%fd48, %fd6, %fd47;
	mul.f64 	%fd49, %fd22, 0dC000000000000000;
	mov.f64 	%fd50, 0d3C91A62633145C07;
	fma.rn.f64 	%fd51, %fd49, %fd48, %fd50;
	add.f64 	%fd52, %fd49, 0d3FE921FB54442D18;
	add.f64 	%fd53, %fd52, %fd51;
	add.f64 	%fd54, %fd53, 0d3FE921FB54442D18;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r16, %temp}, %fd54;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r17}, %fd54;
	}
	and.b32  	%r18, %r2, -2147483648;
	or.b32  	%r19, %r17, %r18;
	mov.b64 	%fd82, {%r16, %r19};

BB28_4:
	cvta.to.global.u32 	%r20, %r3;
	add.s32 	%r22, %r20, %r10;
	st.global.f64 	[%r22], %fd82;

BB28_5:
	ret;
}

	// .globl	vec_asinh
.visible .entry vec_asinh(
	.param .u32 vec_asinh_param_0,
	.param .u32 vec_asinh_param_1,
	.param .u32 vec_asinh_param_2
)
{
	.reg .pred 	%p<22>;
	.reg .b32 	%r<75>;
	.reg .f64 	%fd<159>;


	ld.param.u32 	%r25, [vec_asinh_param_0];
	ld.param.u32 	%r23, [vec_asinh_param_1];
	ld.param.u32 	%r24, [vec_asinh_param_2];
	mov.u32 	%r26, %tid.x;
	mov.u32 	%r27, %ntid.x;
	mov.u32 	%r28, %ctaid.x;
	mad.lo.s32 	%r1, %r27, %r28, %r26;
	setp.ge.u32	%p1, %r1, %r25;
	@%p1 bra 	BB29_26;

	cvta.to.global.u32 	%r29, %r24;
	shl.b32 	%r30, %r1, 3;
	add.s32 	%r31, %r29, %r30;
	ld.global.f64 	%fd1, [%r31];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd1;
	}
	and.b32  	%r33, %r2, 2147483647;
	mov.b64 	%fd2, {%r32, %r33};
	setp.gt.u32	%p2, %r33, 1138753535;
	@%p2 bra 	BB29_14;
	bra.uni 	BB29_2;

BB29_14:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r71}, %fd2;
	}
	setp.lt.s32	%p14, %r71, 2146435072;
	setp.gt.f64	%p15, %fd2, 0d0000000000000000;
	and.pred  	%p16, %p15, %p14;
	@%p16 bra 	BB29_19;
	bra.uni 	BB29_15;

BB29_19:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r72, %temp}, %fd2;
	}
	mov.u32 	%r73, -1023;
	setp.gt.s32	%p20, %r71, 1048575;
	@%p20 bra 	BB29_21;

	mul.f64 	%fd109, %fd2, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r71}, %fd109;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r72, %temp}, %fd109;
	}
	mov.u32 	%r73, -1077;

BB29_21:
	shr.u32 	%r51, %r71, 20;
	add.s32 	%r74, %r73, %r51;
	and.b32  	%r52, %r71, -2146435073;
	or.b32  	%r53, %r52, 1072693248;
	mov.b64 	%fd156, {%r72, %r53};
	setp.lt.s32	%p21, %r53, 1073127583;
	@%p21 bra 	BB29_23;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r54, %temp}, %fd156;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r55}, %fd156;
	}
	add.s32 	%r56, %r55, -1048576;
	mov.b64 	%fd156, {%r54, %r56};
	add.s32 	%r74, %r74, 1;

BB29_23:
	add.f64 	%fd111, %fd156, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd110,%fd111;
	// inline asm
	neg.f64 	%fd112, %fd111;
	mov.f64 	%fd113, 0d3FF0000000000000;
	fma.rn.f64 	%fd114, %fd112, %fd110, %fd113;
	fma.rn.f64 	%fd115, %fd114, %fd114, %fd114;
	fma.rn.f64 	%fd116, %fd115, %fd110, %fd110;
	add.f64 	%fd117, %fd156, 0dBFF0000000000000;
	mul.f64 	%fd118, %fd117, %fd116;
	fma.rn.f64 	%fd119, %fd117, %fd116, %fd118;
	mul.f64 	%fd120, %fd119, %fd119;
	mov.f64 	%fd121, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd122, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd123, %fd122, %fd120, %fd121;
	mov.f64 	%fd124, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd125, %fd123, %fd120, %fd124;
	mov.f64 	%fd126, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd127, %fd125, %fd120, %fd126;
	mov.f64 	%fd128, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd129, %fd127, %fd120, %fd128;
	mov.f64 	%fd130, 0d3F624924923BE72D;
	fma.rn.f64 	%fd131, %fd129, %fd120, %fd130;
	mov.f64 	%fd132, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd133, %fd131, %fd120, %fd132;
	mov.f64 	%fd134, 0d3FB5555555555554;
	fma.rn.f64 	%fd135, %fd133, %fd120, %fd134;
	sub.f64 	%fd136, %fd117, %fd119;
	add.f64 	%fd137, %fd136, %fd136;
	neg.f64 	%fd138, %fd119;
	fma.rn.f64 	%fd139, %fd138, %fd117, %fd137;
	mul.f64 	%fd140, %fd116, %fd139;
	mul.f64 	%fd141, %fd120, %fd135;
	fma.rn.f64 	%fd142, %fd141, %fd119, %fd140;
	xor.b32  	%r57, %r74, -2147483648;
	mov.u32 	%r58, 1127219200;
	mov.b64 	%fd143, {%r57, %r58};
	mov.u32 	%r59, -2147483648;
	mov.b64 	%fd144, {%r59, %r58};
	sub.f64 	%fd145, %fd143, %fd144;
	mov.f64 	%fd146, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd147, %fd145, %fd146, %fd119;
	neg.f64 	%fd148, %fd145;
	fma.rn.f64 	%fd149, %fd148, %fd146, %fd147;
	sub.f64 	%fd150, %fd149, %fd119;
	sub.f64 	%fd151, %fd142, %fd150;
	mov.f64 	%fd152, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd153, %fd145, %fd152, %fd151;
	add.f64 	%fd157, %fd147, %fd153;
	bra.uni 	BB29_24;

BB29_2:
	mul.rn.f64 	%fd23, %fd1, %fd1;
	mov.f64 	%fd24, 0d3FF0000000000000;
	fma.rn.f64 	%fd22, %fd1, %fd1, %fd24;
	// inline asm
	rsqrt.approx.ftz.f64 %fd21, %fd22;
	// inline asm
	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd21;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd21;
	}
	add.s32 	%r36, %r35, -1048576;
	mov.b64 	%fd25, {%r34, %r36};
	mul.f64 	%fd26, %fd22, %fd21;
	neg.f64 	%fd27, %fd26;
	fma.rn.f64 	%fd28, %fd26, %fd27, %fd22;
	fma.rn.f64 	%fd29, %fd28, %fd25, %fd26;
	neg.f64 	%fd30, %fd29;
	fma.rn.f64 	%fd31, %fd21, %fd30, %fd24;
	fma.rn.f64 	%fd32, %fd31, %fd25, %fd25;
	fma.rn.f64 	%fd33, %fd29, %fd30, %fd22;
	fma.rn.f64 	%fd34, %fd33, %fd32, %fd29;
	add.f64 	%fd35, %fd34, 0d3FF0000000000000;
	div.rn.f64 	%fd36, %fd23, %fd35;
	add.f64 	%fd3, %fd2, %fd36;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r37}, %fd3;
	}
	setp.lt.u32	%p3, %r37, 1071994197;
	setp.lt.s32	%p4, %r37, -1076258407;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB29_13;
	bra.uni 	BB29_3;

BB29_13:
	add.f64 	%fd84, %fd3, 0d4000000000000000;
	div.rn.f64 	%fd85, %fd3, %fd84;
	mul.f64 	%fd86, %fd3, %fd85;
	neg.f64 	%fd87, %fd86;
	sub.f64 	%fd88, %fd3, %fd86;
	mul.f64 	%fd89, %fd88, %fd88;
	mov.f64 	%fd90, 0d3ED087FFCEB2DC44;
	mov.f64 	%fd91, 0d3EB372FB2FBE14B5;
	fma.rn.f64 	%fd92, %fd91, %fd89, %fd90;
	mov.f64 	%fd93, 0d3EF3B9FF890F468C;
	fma.rn.f64 	%fd94, %fd92, %fd89, %fd93;
	mov.f64 	%fd95, 0d3F17457EFD51BAF8;
	fma.rn.f64 	%fd96, %fd94, %fd89, %fd95;
	mov.f64 	%fd97, 0d3F3C71C8DE3CE825;
	fma.rn.f64 	%fd98, %fd96, %fd89, %fd97;
	mov.f64 	%fd99, 0d3F6249248FA4661F;
	fma.rn.f64 	%fd100, %fd98, %fd89, %fd99;
	mov.f64 	%fd101, 0d3F899999999D70C4;
	fma.rn.f64 	%fd102, %fd100, %fd89, %fd101;
	mov.f64 	%fd103, 0d3FB5555555555462;
	fma.rn.f64 	%fd104, %fd102, %fd89, %fd103;
	mul.f64 	%fd105, %fd89, %fd104;
	fma.rn.f64 	%fd106, %fd105, %fd88, %fd87;
	add.f64 	%fd158, %fd3, %fd106;
	bra.uni 	BB29_25;

BB29_15:
	abs.f64 	%fd107, %fd2;
	setp.gtu.f64	%p17, %fd107, 0d7FF0000000000000;
	@%p17 bra 	BB29_18;
	bra.uni 	BB29_16;

BB29_18:
	add.f64 	%fd157, %fd2, %fd2;
	bra.uni 	BB29_24;

BB29_3:
	add.f64 	%fd4, %fd3, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r67}, %fd4;
	}
	setp.gt.f64	%p6, %fd4, 0d0000000000000000;
	setp.lt.s32	%p7, %r67, 2146435072;
	and.pred  	%p8, %p6, %p7;
	@%p8 bra 	BB29_8;
	bra.uni 	BB29_4;

BB29_8:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r68, %temp}, %fd4;
	}
	mov.u32 	%r69, -1023;
	setp.gt.s32	%p12, %r67, 1048575;
	@%p12 bra 	BB29_10;

	mul.f64 	%fd39, %fd4, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r67}, %fd39;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r68, %temp}, %fd39;
	}
	mov.u32 	%r69, -1077;

BB29_10:
	shr.u32 	%r40, %r67, 20;
	add.s32 	%r70, %r69, %r40;
	and.b32  	%r41, %r67, -2146435073;
	or.b32  	%r42, %r41, 1072693248;
	mov.b64 	%fd155, {%r68, %r42};
	setp.lt.s32	%p13, %r42, 1073127583;
	@%p13 bra 	BB29_12;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r43, %temp}, %fd155;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd155;
	}
	add.s32 	%r45, %r44, -1048576;
	mov.b64 	%fd155, {%r43, %r45};
	add.s32 	%r70, %r70, 1;

BB29_12:
	add.f64 	%fd41, %fd155, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd40,%fd41;
	// inline asm
	neg.f64 	%fd42, %fd41;
	fma.rn.f64 	%fd44, %fd42, %fd40, %fd24;
	fma.rn.f64 	%fd45, %fd44, %fd44, %fd44;
	fma.rn.f64 	%fd46, %fd45, %fd40, %fd40;
	add.f64 	%fd47, %fd155, 0dBFF0000000000000;
	mul.f64 	%fd48, %fd47, %fd46;
	fma.rn.f64 	%fd49, %fd47, %fd46, %fd48;
	mul.f64 	%fd50, %fd49, %fd49;
	mov.f64 	%fd51, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd52, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd53, %fd52, %fd50, %fd51;
	mov.f64 	%fd54, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd55, %fd53, %fd50, %fd54;
	mov.f64 	%fd56, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd57, %fd55, %fd50, %fd56;
	mov.f64 	%fd58, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd59, %fd57, %fd50, %fd58;
	mov.f64 	%fd60, 0d3F624924923BE72D;
	fma.rn.f64 	%fd61, %fd59, %fd50, %fd60;
	mov.f64 	%fd62, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd63, %fd61, %fd50, %fd62;
	mov.f64 	%fd64, 0d3FB5555555555554;
	fma.rn.f64 	%fd65, %fd63, %fd50, %fd64;
	sub.f64 	%fd66, %fd47, %fd49;
	add.f64 	%fd67, %fd66, %fd66;
	neg.f64 	%fd68, %fd49;
	fma.rn.f64 	%fd69, %fd68, %fd47, %fd67;
	mul.f64 	%fd70, %fd46, %fd69;
	mul.f64 	%fd71, %fd50, %fd65;
	fma.rn.f64 	%fd72, %fd71, %fd49, %fd70;
	xor.b32  	%r46, %r70, -2147483648;
	mov.u32 	%r47, 1127219200;
	mov.b64 	%fd73, {%r46, %r47};
	mov.u32 	%r48, -2147483648;
	mov.b64 	%fd74, {%r48, %r47};
	sub.f64 	%fd75, %fd73, %fd74;
	mov.f64 	%fd76, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd77, %fd75, %fd76, %fd49;
	neg.f64 	%fd78, %fd75;
	fma.rn.f64 	%fd79, %fd78, %fd76, %fd77;
	sub.f64 	%fd80, %fd79, %fd49;
	sub.f64 	%fd81, %fd72, %fd80;
	mov.f64 	%fd82, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd83, %fd75, %fd82, %fd81;
	add.f64 	%fd158, %fd77, %fd83;
	bra.uni 	BB29_25;

BB29_16:
	setp.eq.f64	%p18, %fd2, 0d0000000000000000;
	mov.f64 	%fd157, 0dFFF0000000000000;
	@%p18 bra 	BB29_24;

	setp.eq.f64	%p19, %fd2, 0d7FF0000000000000;
	selp.f64	%fd157, %fd2, 0dFFF8000000000000, %p19;

BB29_24:
	add.f64 	%fd158, %fd157, 0d3FE62E42FEFA39EF;

BB29_25:
	cvta.to.global.u32 	%r60, %r23;
	and.b32  	%r61, %r2, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd158;
	}
	or.b32  	%r63, %r62, %r61;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r64, %temp}, %fd158;
	}
	mov.b64 	%fd154, {%r64, %r63};
	add.s32 	%r66, %r60, %r30;
	st.global.f64 	[%r66], %fd154;

BB29_26:
	ret;

BB29_4:
	abs.f64 	%fd37, %fd4;
	setp.gtu.f64	%p9, %fd37, 0d7FF0000000000000;
	@%p9 bra 	BB29_7;
	bra.uni 	BB29_5;

BB29_7:
	add.f64 	%fd158, %fd4, %fd4;
	bra.uni 	BB29_25;

BB29_5:
	setp.eq.f64	%p10, %fd4, 0d0000000000000000;
	mov.f64 	%fd158, 0dFFF0000000000000;
	@%p10 bra 	BB29_25;

	setp.eq.f64	%p11, %fd4, 0d7FF0000000000000;
	selp.f64	%fd158, %fd4, 0dFFF8000000000000, %p11;
	bra.uni 	BB29_25;
}

	// .globl	vec_atan
.visible .entry vec_atan(
	.param .u32 vec_atan_param_0,
	.param .u32 vec_atan_param_1,
	.param .u32 vec_atan_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<19>;
	.reg .f64 	%fd<57>;


	ld.param.u32 	%r4, [vec_atan_param_0];
	ld.param.u32 	%r2, [vec_atan_param_1];
	ld.param.u32 	%r3, [vec_atan_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB30_4;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd1, [%r10];
	abs.f64 	%fd2, %fd1;
	setp.leu.f64	%p2, %fd2, 0d3FF0000000000000;
	mov.f64 	%fd56, %fd2;
	@%p2 bra 	BB30_3;

	// inline asm
	rcp.approx.ftz.f64 %fd5,%fd2;
	// inline asm
	neg.f64 	%fd7, %fd2;
	mov.f64 	%fd8, 0d3FF0000000000000;
	fma.rn.f64 	%fd9, %fd7, %fd5, %fd8;
	fma.rn.f64 	%fd10, %fd9, %fd9, %fd9;
	fma.rn.f64 	%fd11, %fd10, %fd5, %fd5;
	setp.eq.f64	%p3, %fd2, 0d7FF0000000000000;
	selp.f64	%fd3, 0d0000000000000000, %fd11, %p3;
	mov.f64 	%fd56, %fd3;

BB30_3:
	mov.f64 	%fd4, %fd56;
	cvta.to.global.u32 	%r11, %r2;
	mul.f64 	%fd12, %fd4, %fd4;
	mov.f64 	%fd13, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd14, 0dBEF53E1D2A25FF7E;
	fma.rn.f64 	%fd15, %fd14, %fd12, %fd13;
	mov.f64 	%fd16, 0dBF5312788DDE082E;
	fma.rn.f64 	%fd17, %fd15, %fd12, %fd16;
	mov.f64 	%fd18, 0d3F6F9690C8249315;
	fma.rn.f64 	%fd19, %fd17, %fd12, %fd18;
	mov.f64 	%fd20, 0dBF82CF5AABC7CF0D;
	fma.rn.f64 	%fd21, %fd19, %fd12, %fd20;
	mov.f64 	%fd22, 0d3F9162B0B2A3BFDE;
	fma.rn.f64 	%fd23, %fd21, %fd12, %fd22;
	mov.f64 	%fd24, 0dBF9A7256FEB6FC6B;
	fma.rn.f64 	%fd25, %fd23, %fd12, %fd24;
	mov.f64 	%fd26, 0d3FA171560CE4A489;
	fma.rn.f64 	%fd27, %fd25, %fd12, %fd26;
	mov.f64 	%fd28, 0dBFA4F44D841450E4;
	fma.rn.f64 	%fd29, %fd27, %fd12, %fd28;
	mov.f64 	%fd30, 0d3FA7EE3D3F36BB95;
	fma.rn.f64 	%fd31, %fd29, %fd12, %fd30;
	mov.f64 	%fd32, 0dBFAAD32AE04A9FD1;
	fma.rn.f64 	%fd33, %fd31, %fd12, %fd32;
	mov.f64 	%fd34, 0d3FAE17813D66954F;
	fma.rn.f64 	%fd35, %fd33, %fd12, %fd34;
	mov.f64 	%fd36, 0dBFB11089CA9A5BCD;
	fma.rn.f64 	%fd37, %fd35, %fd12, %fd36;
	mov.f64 	%fd38, 0d3FB3B12B2DB51738;
	fma.rn.f64 	%fd39, %fd37, %fd12, %fd38;
	mov.f64 	%fd40, 0dBFB745D022F8DC5C;
	fma.rn.f64 	%fd41, %fd39, %fd12, %fd40;
	mov.f64 	%fd42, 0d3FBC71C709DFE927;
	fma.rn.f64 	%fd43, %fd41, %fd12, %fd42;
	mov.f64 	%fd44, 0dBFC2492491FA1744;
	fma.rn.f64 	%fd45, %fd43, %fd12, %fd44;
	mov.f64 	%fd46, 0d3FC99999999840D2;
	fma.rn.f64 	%fd47, %fd45, %fd12, %fd46;
	mov.f64 	%fd48, 0dBFD555555555544C;
	fma.rn.f64 	%fd49, %fd47, %fd12, %fd48;
	mul.f64 	%fd50, %fd12, %fd49;
	fma.rn.f64 	%fd51, %fd50, %fd4, %fd4;
	mov.f64 	%fd52, 0d3FF921FB54442D18;
	sub.f64 	%fd53, %fd52, %fd51;
	setp.gt.f64	%p4, %fd2, 0d3FF0000000000000;
	selp.f64	%fd54, %fd53, %fd51, %p4;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r12, %temp}, %fd54;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd54;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd1;
	}
	and.b32  	%r15, %r14, -2147483648;
	or.b32  	%r16, %r13, %r15;
	mov.b64 	%fd55, {%r12, %r16};
	add.s32 	%r18, %r11, %r9;
	st.global.f64 	[%r18], %fd55;

BB30_4:
	ret;
}

	// .globl	vec_atanh
.visible .entry vec_atanh(
	.param .u32 vec_atanh_param_0,
	.param .u32 vec_atanh_param_1,
	.param .u32 vec_atanh_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<45>;
	.reg .f64 	%fd<90>;


	ld.param.u32 	%r14, [vec_atanh_param_0];
	ld.param.u32 	%r12, [vec_atanh_param_1];
	ld.param.u32 	%r13, [vec_atanh_param_2];
	mov.u32 	%r15, %tid.x;
	mov.u32 	%r16, %ntid.x;
	mov.u32 	%r17, %ctaid.x;
	mad.lo.s32 	%r1, %r16, %r17, %r15;
	setp.ge.u32	%p1, %r1, %r14;
	@%p1 bra 	BB31_14;

	cvta.to.global.u32 	%r18, %r13;
	shl.b32 	%r19, %r1, 3;
	add.s32 	%r20, %r18, %r19;
	ld.global.f64 	%fd1, [%r20];
	abs.f64 	%fd12, %fd1;
	add.f64 	%fd13, %fd12, %fd12;
	mov.f64 	%fd14, 0d3FF0000000000000;
	sub.f64 	%fd15, %fd14, %fd12;
	div.rn.f64 	%fd2, %fd13, %fd15;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd2;
	}
	setp.lt.u32	%p2, %r21, 1071994197;
	setp.lt.s32	%p3, %r21, -1076258407;
	or.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB31_12;
	bra.uni 	BB31_2;

BB31_12:
	add.f64 	%fd63, %fd2, 0d4000000000000000;
	div.rn.f64 	%fd64, %fd2, %fd63;
	mul.f64 	%fd65, %fd2, %fd64;
	neg.f64 	%fd66, %fd65;
	sub.f64 	%fd67, %fd2, %fd65;
	mul.f64 	%fd68, %fd67, %fd67;
	mov.f64 	%fd69, 0d3ED087FFCEB2DC44;
	mov.f64 	%fd70, 0d3EB372FB2FBE14B5;
	fma.rn.f64 	%fd71, %fd70, %fd68, %fd69;
	mov.f64 	%fd72, 0d3EF3B9FF890F468C;
	fma.rn.f64 	%fd73, %fd71, %fd68, %fd72;
	mov.f64 	%fd74, 0d3F17457EFD51BAF8;
	fma.rn.f64 	%fd75, %fd73, %fd68, %fd74;
	mov.f64 	%fd76, 0d3F3C71C8DE3CE825;
	fma.rn.f64 	%fd77, %fd75, %fd68, %fd76;
	mov.f64 	%fd78, 0d3F6249248FA4661F;
	fma.rn.f64 	%fd79, %fd77, %fd68, %fd78;
	mov.f64 	%fd80, 0d3F899999999D70C4;
	fma.rn.f64 	%fd81, %fd79, %fd68, %fd80;
	mov.f64 	%fd82, 0d3FB5555555555462;
	fma.rn.f64 	%fd83, %fd81, %fd68, %fd82;
	mul.f64 	%fd84, %fd68, %fd83;
	fma.rn.f64 	%fd85, %fd84, %fd67, %fd66;
	add.f64 	%fd89, %fd2, %fd85;
	bra.uni 	BB31_13;

BB31_2:
	add.f64 	%fd3, %fd2, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r41}, %fd3;
	}
	setp.gt.f64	%p5, %fd3, 0d0000000000000000;
	setp.lt.s32	%p6, %r41, 2146435072;
	and.pred  	%p7, %p5, %p6;
	@%p7 bra 	BB31_7;
	bra.uni 	BB31_3;

BB31_7:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r42, %temp}, %fd3;
	}
	mov.u32 	%r43, -1023;
	setp.gt.s32	%p11, %r41, 1048575;
	@%p11 bra 	BB31_9;

	mul.f64 	%fd18, %fd3, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r41}, %fd18;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r42, %temp}, %fd18;
	}
	mov.u32 	%r43, -1077;

BB31_9:
	shr.u32 	%r24, %r41, 20;
	add.s32 	%r44, %r43, %r24;
	and.b32  	%r25, %r41, -2146435073;
	or.b32  	%r26, %r25, 1072693248;
	mov.b64 	%fd88, {%r42, %r26};
	setp.lt.s32	%p12, %r26, 1073127583;
	@%p12 bra 	BB31_11;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r27, %temp}, %fd88;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd88;
	}
	add.s32 	%r29, %r28, -1048576;
	mov.b64 	%fd88, {%r27, %r29};
	add.s32 	%r44, %r44, 1;

BB31_11:
	add.f64 	%fd20, %fd88, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd19,%fd20;
	// inline asm
	neg.f64 	%fd21, %fd20;
	fma.rn.f64 	%fd23, %fd21, %fd19, %fd14;
	fma.rn.f64 	%fd24, %fd23, %fd23, %fd23;
	fma.rn.f64 	%fd25, %fd24, %fd19, %fd19;
	add.f64 	%fd26, %fd88, 0dBFF0000000000000;
	mul.f64 	%fd27, %fd26, %fd25;
	fma.rn.f64 	%fd28, %fd26, %fd25, %fd27;
	mul.f64 	%fd29, %fd28, %fd28;
	mov.f64 	%fd30, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd31, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd32, %fd31, %fd29, %fd30;
	mov.f64 	%fd33, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd34, %fd32, %fd29, %fd33;
	mov.f64 	%fd35, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd36, %fd34, %fd29, %fd35;
	mov.f64 	%fd37, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd38, %fd36, %fd29, %fd37;
	mov.f64 	%fd39, 0d3F624924923BE72D;
	fma.rn.f64 	%fd40, %fd38, %fd29, %fd39;
	mov.f64 	%fd41, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd42, %fd40, %fd29, %fd41;
	mov.f64 	%fd43, 0d3FB5555555555554;
	fma.rn.f64 	%fd44, %fd42, %fd29, %fd43;
	sub.f64 	%fd45, %fd26, %fd28;
	add.f64 	%fd46, %fd45, %fd45;
	neg.f64 	%fd47, %fd28;
	fma.rn.f64 	%fd48, %fd47, %fd26, %fd46;
	mul.f64 	%fd49, %fd25, %fd48;
	mul.f64 	%fd50, %fd29, %fd44;
	fma.rn.f64 	%fd51, %fd50, %fd28, %fd49;
	xor.b32  	%r30, %r44, -2147483648;
	mov.u32 	%r31, 1127219200;
	mov.b64 	%fd52, {%r30, %r31};
	mov.u32 	%r32, -2147483648;
	mov.b64 	%fd53, {%r32, %r31};
	sub.f64 	%fd54, %fd52, %fd53;
	mov.f64 	%fd55, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd56, %fd54, %fd55, %fd28;
	neg.f64 	%fd57, %fd54;
	fma.rn.f64 	%fd58, %fd57, %fd55, %fd56;
	sub.f64 	%fd59, %fd58, %fd28;
	sub.f64 	%fd60, %fd51, %fd59;
	mov.f64 	%fd61, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd62, %fd54, %fd61, %fd60;
	add.f64 	%fd89, %fd56, %fd62;
	bra.uni 	BB31_13;

BB31_3:
	abs.f64 	%fd16, %fd3;
	setp.gtu.f64	%p8, %fd16, 0d7FF0000000000000;
	@%p8 bra 	BB31_6;
	bra.uni 	BB31_4;

BB31_6:
	add.f64 	%fd89, %fd3, %fd3;
	bra.uni 	BB31_13;

BB31_4:
	setp.eq.f64	%p9, %fd3, 0d0000000000000000;
	mov.f64 	%fd89, 0dFFF0000000000000;
	@%p9 bra 	BB31_13;

	setp.eq.f64	%p10, %fd3, 0d7FF0000000000000;
	selp.f64	%fd89, %fd3, 0dFFF8000000000000, %p10;

BB31_13:
	cvta.to.global.u32 	%r33, %r12;
	mul.f64 	%fd86, %fd89, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd86;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd86;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r36}, %fd1;
	}
	and.b32  	%r37, %r36, -2147483648;
	or.b32  	%r38, %r35, %r37;
	mov.b64 	%fd87, {%r34, %r38};
	add.s32 	%r40, %r33, %r19;
	st.global.f64 	[%r40], %fd87;

BB31_14:
	ret;
}

	// .globl	vec_cbrt
.visible .entry vec_cbrt(
	.param .u32 vec_cbrt_param_0,
	.param .u32 vec_cbrt_param_1,
	.param .u32 vec_cbrt_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<46>;
	.reg .f64 	%fd<24>;


	ld.param.u32 	%r15, [vec_cbrt_param_0];
	ld.param.u32 	%r13, [vec_cbrt_param_1];
	ld.param.u32 	%r14, [vec_cbrt_param_2];
	mov.u32 	%r16, %tid.x;
	mov.u32 	%r17, %ntid.x;
	mov.u32 	%r18, %ctaid.x;
	mad.lo.s32 	%r1, %r17, %r18, %r16;
	setp.ge.u32	%p1, %r1, %r15;
	@%p1 bra 	BB32_7;

	cvta.to.global.u32 	%r19, %r14;
	shl.b32 	%r20, %r1, 3;
	add.s32 	%r21, %r19, %r20;
	ld.global.f64 	%fd1, [%r21];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r42, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd1;
	}
	and.b32  	%r43, %r3, 2147483647;
	setp.neu.f64	%p2, %fd1, 0d0000000000000000;
	setp.lt.u32	%p3, %r43, 2146435072;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB32_3;
	bra.uni 	BB32_2;

BB32_3:
	shr.u32 	%r44, %r43, 20;
	mov.u32 	%r45, 0;
	setp.ne.s32	%p5, %r44, 0;
	@%p5 bra 	BB32_5;

	mov.b64 	%fd5, {%r42, %r43};
	mul.f64 	%fd6, %fd5, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r42, %temp}, %fd6;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r43}, %fd6;
	}
	shr.u32 	%r44, %r43, 20;
	mov.u32 	%r45, 18;

BB32_5:
	add.s32 	%r24, %r44, -1022;
	cvt.rn.f32.s32	%f5, %r24;
	mul.f32 	%f6, %f5, 0f3EAAAAAB;
	cvt.rni.s32.f32	%r25, %f6;
	mad.lo.s32 	%r26, %r25, -3145728, %r43;
	mov.b64 	%fd9, {%r42, %r26};
	cvt.rn.f32.f64	%f2, %fd9;
	// inline asm
	lg2.approx.ftz.f32 %f1,%f2;
	// inline asm
	mul.f32 	%f4, %f1, 0f3EAAAAAB;
	// inline asm
	ex2.approx.ftz.f32 %f3,%f4;
	// inline asm
	cvt.f64.f32	%fd10, %f3;
	mul.f64 	%fd11, %fd10, %fd10;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r27, %temp}, %fd11;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd11;
	}
	add.s32 	%r29, %r28, 1048576;
	mov.b64 	%fd12, {%r27, %r29};
	fma.rn.f64 	%fd8, %fd12, %fd10, %fd9;
	// inline asm
	rcp.approx.ftz.f64 %fd7,%fd8;
	// inline asm
	neg.f64 	%fd13, %fd8;
	mov.f64 	%fd14, 0d3FF0000000000000;
	fma.rn.f64 	%fd15, %fd13, %fd7, %fd14;
	fma.rn.f64 	%fd16, %fd15, %fd15, %fd15;
	fma.rn.f64 	%fd17, %fd16, %fd7, %fd7;
	neg.f64 	%fd18, %fd10;
	fma.rn.f64 	%fd19, %fd11, %fd18, %fd9;
	mul.f64 	%fd20, %fd17, %fd19;
	fma.rn.f64 	%fd21, %fd10, %fd20, %fd10;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r30, %temp}, %fd21;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r31}, %fd21;
	}
	sub.s32 	%r32, %r25, %r45;
	shl.b32 	%r33, %r32, 20;
	add.s32 	%r34, %r31, %r33;
	mov.b64 	%fd22, {%r30, %r34};
	{
	.reg .b32 %temp; 
	mov.b64 	{%r35, %temp}, %fd22;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r36}, %fd22;
	}
	and.b32  	%r37, %r3, -2147483648;
	or.b32  	%r38, %r36, %r37;
	mov.b64 	%fd23, {%r35, %r38};
	bra.uni 	BB32_6;

BB32_2:
	add.f64 	%fd23, %fd1, %fd1;

BB32_6:
	cvta.to.global.u32 	%r39, %r13;
	add.s32 	%r41, %r39, %r20;
	st.global.f64 	[%r41], %fd23;

BB32_7:
	ret;
}

	// .globl	vec_ceil
.visible .entry vec_ceil(
	.param .u32 vec_ceil_param_0,
	.param .u32 vec_ceil_param_1,
	.param .u32 vec_ceil_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<3>;


	ld.param.u32 	%r4, [vec_ceil_param_0];
	ld.param.u32 	%r2, [vec_ceil_param_1];
	ld.param.u32 	%r3, [vec_ceil_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB33_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd1, [%r10];
	cvt.rpi.f64.f64	%fd2, %fd1;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd2;

BB33_2:
	ret;
}

	// .globl	vec_cos
.visible .entry vec_cos(
	.param .u32 vec_cos_param_0,
	.param .u32 vec_cos_param_1,
	.param .u32 vec_cos_param_2
)
{
	.local .align 4 .b8 	__local_depot34[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<30>;
	.reg .f64 	%fd<42>;


	mov.u32 	%r29, __local_depot34;
	cvta.local.u32 	%SP, %r29;
	ld.param.u32 	%r9, [vec_cos_param_0];
	ld.param.u32 	%r7, [vec_cos_param_1];
	ld.param.u32 	%r8, [vec_cos_param_2];
	add.u32 	%r10, %SP, 0;
	cvta.to.local.u32 	%r1, %r10;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r2, %r11, %r12, %r13;
	setp.ge.u32	%p1, %r2, %r9;
	@%p1 bra 	BB34_10;

	cvta.to.global.u32 	%r14, %r8;
	shl.b32 	%r15, %r2, 3;
	add.s32 	%r16, %r14, %r15;
	ld.global.f64 	%fd39, [%r16];
	abs.f64 	%fd14, %fd39;
	setp.neu.f64	%p2, %fd14, 0d7FF0000000000000;
	@%p2 bra 	BB34_3;

	mov.f64 	%fd15, 0d0000000000000000;
	mul.rn.f64 	%fd39, %fd39, %fd15;

BB34_3:
	mul.f64 	%fd16, %fd39, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r28, %fd16;
	st.local.u32 	[%r1], %r28;
	cvt.rn.f64.s32	%fd17, %r28;
	neg.f64 	%fd18, %fd17;
	mov.f64 	%fd19, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd20, %fd18, %fd19, %fd39;
	mov.f64 	%fd21, 0d3C91A62633145C00;
	fma.rn.f64 	%fd22, %fd18, %fd21, %fd20;
	mov.f64 	%fd23, 0d397B839A252049C0;
	fma.rn.f64 	%fd40, %fd18, %fd23, %fd22;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r17}, %fd39;
	}
	and.b32  	%r18, %r17, 2145386496;
	setp.lt.u32	%p3, %r18, 1105199104;
	@%p3 bra 	BB34_5;

	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd39;
	.param .b32 param1;
	st.param.b32	[param1+0], %r10;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd40, [retval0+0];
	
	//{
	}// Callseq End 0
	ld.local.u32 	%r28, [%r1];

BB34_5:
	add.s32 	%r6, %r28, 1;
	and.b32  	%r20, %r6, 1;
	setp.eq.s32	%p4, %r20, 0;
	selp.f64	%fd24, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p4;
	shl.b32 	%r21, %r20, 6;
	mov.u32 	%r22, __cudart_sin_cos_coeffs;
	add.s32 	%r23, %r21, %r22;
	ld.const.f64 	%fd25, [%r23+8];
	mul.rn.f64 	%fd7, %fd40, %fd40;
	fma.rn.f64 	%fd26, %fd24, %fd7, %fd25;
	ld.const.f64 	%fd27, [%r23+16];
	fma.rn.f64 	%fd28, %fd26, %fd7, %fd27;
	ld.const.f64 	%fd29, [%r23+24];
	fma.rn.f64 	%fd30, %fd28, %fd7, %fd29;
	ld.const.f64 	%fd31, [%r23+32];
	fma.rn.f64 	%fd32, %fd30, %fd7, %fd31;
	ld.const.f64 	%fd33, [%r23+40];
	fma.rn.f64 	%fd34, %fd32, %fd7, %fd33;
	ld.const.f64 	%fd35, [%r23+48];
	fma.rn.f64 	%fd8, %fd34, %fd7, %fd35;
	fma.rn.f64 	%fd41, %fd8, %fd40, %fd40;
	@%p4 bra 	BB34_7;

	mov.f64 	%fd36, 0d3FF0000000000000;
	fma.rn.f64 	%fd41, %fd8, %fd7, %fd36;

BB34_7:
	and.b32  	%r24, %r6, 2;
	setp.eq.s32	%p5, %r24, 0;
	@%p5 bra 	BB34_9;

	mov.f64 	%fd37, 0d0000000000000000;
	mov.f64 	%fd38, 0dBFF0000000000000;
	fma.rn.f64 	%fd41, %fd41, %fd38, %fd37;

BB34_9:
	cvta.to.global.u32 	%r25, %r7;
	add.s32 	%r27, %r25, %r15;
	st.global.f64 	[%r27], %fd41;

BB34_10:
	ret;
}

	// .globl	vec_cosh
.visible .entry vec_cosh(
	.param .u32 vec_cosh_param_0,
	.param .u32 vec_cosh_param_1,
	.param .u32 vec_cosh_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<24>;
	.reg .f64 	%fd<47>;


	ld.param.u32 	%r4, [vec_cosh_param_0];
	ld.param.u32 	%r2, [vec_cosh_param_1];
	ld.param.u32 	%r3, [vec_cosh_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB35_5;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd1, [%r10];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r11}, %fd1;
	}
	and.b32  	%r12, %r11, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd1;
	}
	mov.b64 	%fd2, {%r13, %r12};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd2;
	}
	setp.lt.u32	%p2, %r14, 1082536911;
	@%p2 bra 	BB35_3;
	bra.uni 	BB35_2;

BB35_3:
	mov.f64 	%fd8, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd9, %fd2, %fd8;
	mov.f64 	%fd10, 0d4338000000000000;
	add.rn.f64 	%fd11, %fd9, %fd10;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r15, %temp}, %fd11;
	}
	mov.f64 	%fd12, 0dC338000000000000;
	add.rn.f64 	%fd13, %fd11, %fd12;
	mov.f64 	%fd14, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd15, %fd13, %fd14, %fd2;
	mov.f64 	%fd16, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd17, %fd13, %fd16, %fd15;
	mov.f64 	%fd18, 0d3E928AF3FCA213EA;
	mov.f64 	%fd19, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd20, %fd19, %fd17, %fd18;
	mov.f64 	%fd21, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd22, %fd20, %fd17, %fd21;
	mov.f64 	%fd23, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd24, %fd22, %fd17, %fd23;
	mov.f64 	%fd25, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd26, %fd24, %fd17, %fd25;
	mov.f64 	%fd27, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd28, %fd26, %fd17, %fd27;
	mov.f64 	%fd29, 0d3F81111111122322;
	fma.rn.f64 	%fd30, %fd28, %fd17, %fd29;
	mov.f64 	%fd31, 0d3FA55555555502A1;
	fma.rn.f64 	%fd32, %fd30, %fd17, %fd31;
	mov.f64 	%fd33, 0d3FC5555555555511;
	fma.rn.f64 	%fd34, %fd32, %fd17, %fd33;
	mov.f64 	%fd35, 0d3FE000000000000B;
	fma.rn.f64 	%fd36, %fd34, %fd17, %fd35;
	mov.f64 	%fd37, 0d3FF0000000000000;
	fma.rn.f64 	%fd38, %fd36, %fd17, %fd37;
	fma.rn.f64 	%fd39, %fd38, %fd17, %fd37;
	shl.b32 	%r16, %r15, 20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r17, %temp}, %fd39;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd39;
	}
	add.s32 	%r19, %r16, %r18;
	add.s32 	%r20, %r19, -2097152;
	mov.b64 	%fd7, {%r17, %r20};
	// inline asm
	rcp.approx.ftz.f64 %fd6,%fd7;
	// inline asm
	neg.f64 	%fd40, %fd7;
	fma.rn.f64 	%fd41, %fd40, %fd6, %fd37;
	fma.rn.f64 	%fd42, %fd41, %fd41, %fd41;
	fma.rn.f64 	%fd43, %fd42, %fd6, %fd6;
	mov.f64 	%fd44, 0d3FB0000000000000;
	fma.rn.f64 	%fd46, %fd43, %fd44, %fd7;
	bra.uni 	BB35_4;

BB35_2:
	setp.gtu.f64	%p3, %fd1, 0d7FF0000000000000;
	selp.f64	%fd46, %fd1, 0d7FF0000000000000, %p3;

BB35_4:
	cvta.to.global.u32 	%r21, %r2;
	add.s32 	%r23, %r21, %r9;
	add.f64 	%fd45, %fd46, %fd46;
	st.global.f64 	[%r23], %fd45;

BB35_5:
	ret;
}

	// .globl	vec_cospi
.visible .entry vec_cospi(
	.param .u32 vec_cospi_param_0,
	.param .u32 vec_cospi_param_1,
	.param .u32 vec_cospi_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<26>;
	.reg .f64 	%fd<37>;
	.reg .b64 	%rd<2>;


	ld.param.u32 	%r5, [vec_cospi_param_0];
	ld.param.u32 	%r3, [vec_cospi_param_1];
	ld.param.u32 	%r4, [vec_cospi_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB36_8;

	cvta.to.global.u32 	%r9, %r4;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	ld.global.f64 	%fd35, [%r11];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd35;
	}
	add.s32 	%r13, %r12, %r12;
	setp.lt.u32	%p2, %r13, -2038431743;
	@%p2 bra 	BB36_3;

	mov.f64 	%fd11, 0d0000000000000000;
	mul.rn.f64 	%fd35, %fd35, %fd11;

BB36_3:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd35;
	}
	add.s32 	%r15, %r14, 1048576;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r16, %temp}, %fd35;
	}
	mov.b64 	%fd12, {%r16, %r15};
	cvt.rni.f64.f64	%fd13, %fd12;
	cvt.rzi.s64.f64	%rd1, %fd13;
	cvt.u32.u64	%r17, %rd1;
	neg.f64 	%fd14, %fd13;
	mov.f64 	%fd15, 0d3FE0000000000000;
	fma.rn.f64 	%fd16, %fd14, %fd15, %fd35;
	mul.f64 	%fd17, %fd16, 0d3CA1A62633145C07;
	mov.f64 	%fd18, 0d400921FB54442D18;
	fma.rn.f64 	%fd19, %fd16, %fd18, %fd17;
	add.s32 	%r2, %r17, 1;
	and.b32  	%r18, %r2, 1;
	mul.rn.f64 	%fd4, %fd19, %fd19;
	setp.eq.s32	%p3, %r18, 0;
	selp.f64	%fd20, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p3;
	shl.b32 	%r19, %r18, 6;
	mov.u32 	%r20, __cudart_sin_cos_coeffs;
	add.s32 	%r21, %r19, %r20;
	ld.const.f64 	%fd21, [%r21+8];
	fma.rn.f64 	%fd22, %fd20, %fd4, %fd21;
	ld.const.f64 	%fd23, [%r21+16];
	fma.rn.f64 	%fd24, %fd22, %fd4, %fd23;
	ld.const.f64 	%fd25, [%r21+24];
	fma.rn.f64 	%fd26, %fd24, %fd4, %fd25;
	ld.const.f64 	%fd27, [%r21+32];
	fma.rn.f64 	%fd28, %fd26, %fd4, %fd27;
	ld.const.f64 	%fd29, [%r21+40];
	fma.rn.f64 	%fd30, %fd28, %fd4, %fd29;
	ld.const.f64 	%fd31, [%r21+48];
	fma.rn.f64 	%fd5, %fd30, %fd4, %fd31;
	fma.rn.f64 	%fd36, %fd5, %fd19, %fd19;
	@%p3 bra 	BB36_5;

	mov.f64 	%fd32, 0d3FF0000000000000;
	fma.rn.f64 	%fd36, %fd5, %fd4, %fd32;

BB36_5:
	and.b32  	%r22, %r2, 2;
	setp.eq.s32	%p4, %r22, 0;
	@%p4 bra 	BB36_7;

	mov.f64 	%fd33, 0d0000000000000000;
	mov.f64 	%fd34, 0dBFF0000000000000;
	fma.rn.f64 	%fd36, %fd36, %fd34, %fd33;

BB36_7:
	cvta.to.global.u32 	%r23, %r3;
	add.s32 	%r25, %r23, %r10;
	st.global.f64 	[%r25], %fd36;

BB36_8:
	ret;
}

	// .globl	vec_erfc
.visible .entry vec_erfc(
	.param .u32 vec_erfc_param_0,
	.param .u32 vec_erfc_param_1,
	.param .u32 vec_erfc_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<30>;
	.reg .f64 	%fd<124>;


	ld.param.u32 	%r7, [vec_erfc_param_0];
	ld.param.u32 	%r6, [vec_erfc_param_2];
	mov.u32 	%r8, %tid.x;
	mov.u32 	%r9, %ntid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r1, %r9, %r10, %r8;
	setp.ge.u32	%p1, %r1, %r7;
	@%p1 bra 	BB37_5;

	cvta.to.global.u32 	%r11, %r6;
	shl.b32 	%r12, %r1, 3;
	add.s32 	%r13, %r11, %r12;
	ld.global.f64 	%fd1, [%r13];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	and.b32  	%r3, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r4, %temp}, %fd1;
	}
	setp.lt.u32	%p2, %r3, 2146435072;
	@%p2 bra 	BB37_3;
	bra.uni 	BB37_2;

BB37_3:
	setp.lt.s32	%p7, %r2, 0;
	mov.b64 	%fd11, {%r4, %r3};
	add.f64 	%fd12, %fd11, 0dC010000000000000;
	add.f64 	%fd8, %fd11, 0d4010000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd7,%fd8;
	// inline asm
	neg.f64 	%fd13, %fd8;
	mov.f64 	%fd14, 0d3FF0000000000000;
	fma.rn.f64 	%fd15, %fd13, %fd7, %fd14;
	fma.rn.f64 	%fd16, %fd15, %fd15, %fd15;
	fma.rn.f64 	%fd17, %fd16, %fd7, %fd7;
	mul.f64 	%fd18, %fd12, %fd17;
	add.rn.f64 	%fd19, %fd18, %fd14;
	mov.f64 	%fd20, 0dC010000000000000;
	fma.rn.f64 	%fd21, %fd20, %fd19, %fd11;
	neg.f64 	%fd22, %fd18;
	fma.rn.f64 	%fd23, %fd22, %fd11, %fd21;
	fma.rn.f64 	%fd24, %fd17, %fd23, %fd18;
	mov.f64 	%fd25, 0dBE44E1C6FD03D328;
	mov.f64 	%fd26, 0dBDF8774AD4E0BFD7;
	fma.rn.f64 	%fd27, %fd26, %fd24, %fd25;
	mov.f64 	%fd28, 0dBE4330149F7A56B6;
	fma.rn.f64 	%fd29, %fd27, %fd24, %fd28;
	mov.f64 	%fd30, 0d3E7BEDDED8376273;
	fma.rn.f64 	%fd31, %fd29, %fd24, %fd30;
	mov.f64 	%fd32, 0d3E6F9254C3ABF22B;
	fma.rn.f64 	%fd33, %fd31, %fd24, %fd32;
	mov.f64 	%fd34, 0dBEAB9068C2148CF0;
	fma.rn.f64 	%fd35, %fd33, %fd24, %fd34;
	mov.f64 	%fd36, 0d3E94C6454DB34009;
	fma.rn.f64 	%fd37, %fd35, %fd24, %fd36;
	mov.f64 	%fd38, 0d3ED7F1C378F2311D;
	fma.rn.f64 	%fd39, %fd37, %fd24, %fd38;
	mov.f64 	%fd40, 0dBEE78E051C6D5C58;
	fma.rn.f64 	%fd41, %fd39, %fd24, %fd40;
	mov.f64 	%fd42, 0dBEF995B4EAD14A90;
	fma.rn.f64 	%fd43, %fd41, %fd24, %fd42;
	mov.f64 	%fd44, 0d3F23BE27CF0A29B2;
	fma.rn.f64 	%fd45, %fd43, %fd24, %fd44;
	mov.f64 	%fd46, 0dBF2A1DEF3E81672E;
	fma.rn.f64 	%fd47, %fd45, %fd24, %fd46;
	mov.f64 	%fd48, 0dBF48D4ABE68C1713;
	fma.rn.f64 	%fd49, %fd47, %fd24, %fd48;
	mov.f64 	%fd50, 0d3F749C67210DD6B4;
	fma.rn.f64 	%fd51, %fd49, %fd24, %fd50;
	mov.f64 	%fd52, 0dBF9096238568E357;
	fma.rn.f64 	%fd53, %fd51, %fd24, %fd52;
	mov.f64 	%fd54, 0d3FA3079EDF8C2DC9;
	fma.rn.f64 	%fd55, %fd53, %fd24, %fd54;
	mov.f64 	%fd56, 0dBFB0FB06DFF601FC;
	fma.rn.f64 	%fd57, %fd55, %fd24, %fd56;
	mov.f64 	%fd58, 0d3FB7FEE004DFBCDC;
	fma.rn.f64 	%fd59, %fd57, %fd24, %fd58;
	mov.f64 	%fd60, 0dBFB9DDB23C3DB8C6;
	fma.rn.f64 	%fd61, %fd59, %fd24, %fd60;
	mov.f64 	%fd62, 0d3FB16ECEFCFA5FDA;
	fma.rn.f64 	%fd63, %fd61, %fd24, %fd62;
	mov.f64 	%fd64, 0d3F8F7F5DF66FB6D6;
	fma.rn.f64 	%fd65, %fd63, %fd24, %fd64;
	mov.f64 	%fd66, 0dBFC1DF1AD154A29D;
	fma.rn.f64 	%fd67, %fd65, %fd24, %fd66;
	mov.f64 	%fd68, 0d3FF3BA5916E9FD7F;
	fma.rn.f64 	%fd69, %fd67, %fd24, %fd68;
	mov.f64 	%fd70, 0d4000000000000000;
	fma.rn.f64 	%fd10, %fd70, %fd11, %fd14;
	// inline asm
	rcp.approx.ftz.f64 %fd9,%fd10;
	// inline asm
	neg.f64 	%fd71, %fd10;
	fma.rn.f64 	%fd72, %fd71, %fd9, %fd14;
	fma.rn.f64 	%fd73, %fd72, %fd72, %fd72;
	fma.rn.f64 	%fd74, %fd73, %fd9, %fd9;
	mul.f64 	%fd75, %fd69, %fd74;
	mul.f64 	%fd76, %fd75, 0dC000000000000000;
	fma.rn.f64 	%fd77, %fd11, %fd76, %fd69;
	neg.f64 	%fd78, %fd75;
	add.rn.f64 	%fd79, %fd77, %fd78;
	fma.rn.f64 	%fd80, %fd79, %fd74, %fd75;
	mul.f64 	%fd81, %fd11, %fd11;
	neg.f64 	%fd82, %fd81;
	mov.f64 	%fd83, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd84, %fd82, %fd83;
	mov.f64 	%fd85, 0d4338000000000000;
	add.rn.f64 	%fd86, %fd84, %fd85;
	mov.f64 	%fd87, 0dC338000000000000;
	add.rn.f64 	%fd88, %fd86, %fd87;
	mov.f64 	%fd89, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd90, %fd88, %fd89, %fd82;
	mov.f64 	%fd91, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd92, %fd88, %fd91, %fd90;
	mov.f64 	%fd93, 0d3E928AF3FCA213EA;
	mov.f64 	%fd94, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd95, %fd94, %fd92, %fd93;
	mov.f64 	%fd96, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd97, %fd95, %fd92, %fd96;
	mov.f64 	%fd98, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd99, %fd97, %fd92, %fd98;
	mov.f64 	%fd100, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd101, %fd99, %fd92, %fd100;
	mov.f64 	%fd102, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd103, %fd101, %fd92, %fd102;
	mov.f64 	%fd104, 0d3F81111111122322;
	fma.rn.f64 	%fd105, %fd103, %fd92, %fd104;
	mov.f64 	%fd106, 0d3FA55555555502A1;
	fma.rn.f64 	%fd107, %fd105, %fd92, %fd106;
	mov.f64 	%fd108, 0d3FC5555555555511;
	fma.rn.f64 	%fd109, %fd107, %fd92, %fd108;
	mov.f64 	%fd110, 0d3FE000000000000B;
	fma.rn.f64 	%fd111, %fd109, %fd92, %fd110;
	fma.rn.f64 	%fd112, %fd111, %fd92, %fd14;
	fma.rn.f64 	%fd113, %fd112, %fd92, %fd14;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd86;
	}
	shr.u32 	%r15, %r14, 31;
	add.s32 	%r16, %r14, %r15;
	shr.s32 	%r17, %r16, 1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r18, %temp}, %fd113;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd113;
	}
	shl.b32 	%r20, %r17, 20;
	add.s32 	%r21, %r19, %r20;
	mov.b64 	%fd114, {%r18, %r21};
	sub.s32 	%r22, %r14, %r17;
	shl.b32 	%r23, %r22, 20;
	add.s32 	%r24, %r23, 1072693248;
	mov.u32 	%r25, 0;
	mov.b64 	%fd115, {%r25, %r24};
	mul.f64 	%fd116, %fd114, %fd115;
	neg.f64 	%fd117, %fd11;
	fma.rn.f64 	%fd118, %fd117, %fd11, %fd81;
	fma.rn.f64 	%fd119, %fd116, %fd118, %fd116;
	mul.f64 	%fd120, %fd80, %fd119;
	setp.gt.u32	%p8, %r3, 1077624832;
	selp.f64	%fd121, 0d0000000000000000, %fd120, %p8;
	sub.f64 	%fd122, %fd70, %fd121;
	selp.f64	%fd123, %fd122, %fd121, %p7;
	bra.uni 	BB37_4;

BB37_2:
	setp.lt.s32	%p3, %r2, 0;
	setp.eq.s32	%p4, %r4, 0;
	setp.eq.s32	%p5, %r3, 2146435072;
	and.pred  	%p6, %p5, %p4;
	selp.f64	%fd5, 0d4000000000000000, 0d0000000000000000, %p3;
	add.f64 	%fd6, %fd1, %fd1;
	selp.f64	%fd123, %fd5, %fd6, %p6;

BB37_4:
	ld.param.u32 	%r29, [vec_erfc_param_1];
	cvta.to.global.u32 	%r26, %r29;
	add.s32 	%r28, %r26, %r12;
	st.global.f64 	[%r28], %fd123;

BB37_5:
	ret;
}

	// .globl	vec_erfcinv
.visible .entry vec_erfcinv(
	.param .u32 vec_erfcinv_param_0,
	.param .u32 vec_erfcinv_param_1,
	.param .u32 vec_erfcinv_param_2
)
{
	.reg .pred 	%p<16>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<62>;
	.reg .f64 	%fd<267>;


	ld.param.u32 	%r15, [vec_erfcinv_param_0];
	ld.param.u32 	%r14, [vec_erfcinv_param_2];
	mov.u32 	%r16, %tid.x;
	mov.u32 	%r17, %ntid.x;
	mov.u32 	%r18, %ctaid.x;
	mad.lo.s32 	%r1, %r17, %r18, %r16;
	setp.ge.u32	%p1, %r1, %r15;
	@%p1 bra 	BB38_18;

	cvta.to.global.u32 	%r19, %r14;
	shl.b32 	%r20, %r1, 3;
	add.s32 	%r21, %r19, %r20;
	ld.global.f64 	%fd1, [%r21];
	neg.f64 	%fd2, %fd1;
	mov.f64 	%fd18, 0d4000000000000000;
	add.rn.f64 	%fd3, %fd18, %fd2;
	setp.le.f64	%p2, %fd1, 0d3FFFFC0B65AA4E0E;
	setp.ge.f64	%p3, %fd1, 0d3F4FA4D2AD8F904D;
	and.pred  	%p4, %p3, %p2;
	@%p4 bra 	BB38_16;
	bra.uni 	BB38_2;

BB38_16:
	mul.rn.f64 	%fd174, %fd3, %fd1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r40}, %fd174;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r41, %temp}, %fd174;
	}
	shr.u32 	%r42, %r40, 20;
	and.b32  	%r43, %r42, 2046;
	add.s32 	%r44, %r43, 2147482626;
	mov.u32 	%r45, 1127219200;
	mov.b64 	%fd175, {%r44, %r45};
	mov.u32 	%r46, -2147483648;
	mov.b64 	%fd176, {%r46, %r45};
	sub.f64 	%fd177, %fd175, %fd176;
	and.b32  	%r47, %r40, -2145386497;
	add.s32 	%r48, %r47, 1071644672;
	mov.b64 	%fd178, {%r41, %r48};
	add.f64 	%fd179, %fd178, 0dBFF0000000000000;
	add.f64 	%fd173, %fd178, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd172,%fd173;
	// inline asm
	neg.f64 	%fd180, %fd173;
	mov.f64 	%fd181, 0d3FF0000000000000;
	fma.rn.f64 	%fd182, %fd180, %fd172, %fd181;
	fma.rn.f64 	%fd183, %fd182, %fd182, %fd182;
	fma.rn.f64 	%fd184, %fd183, %fd172, %fd172;
	mul.f64 	%fd185, %fd179, %fd184;
	mov.f64 	%fd186, 0dC000000000000000;
	fma.rn.f64 	%fd187, %fd186, %fd185, %fd179;
	neg.f64 	%fd188, %fd185;
	fma.rn.f64 	%fd189, %fd188, %fd179, %fd187;
	fma.rn.f64 	%fd190, %fd189, %fd184, %fd185;
	mul.f64 	%fd191, %fd190, %fd190;
	mov.f64 	%fd192, 0d3FA55CF59CDC5D89;
	mov.f64 	%fd193, 0d3FB5C5C218C775C9;
	fma.rn.f64 	%fd194, %fd193, %fd191, %fd192;
	mov.f64 	%fd195, 0d3FAEFD18CF6EBB9C;
	fma.rn.f64 	%fd196, %fd194, %fd191, %fd195;
	mov.f64 	%fd197, 0d3FB10682EDCB8D1B;
	fma.rn.f64 	%fd198, %fd196, %fd191, %fd197;
	mov.f64 	%fd199, 0d3FB3B1DD3AC7FC96;
	fma.rn.f64 	%fd200, %fd198, %fd191, %fd199;
	mov.f64 	%fd201, 0d3FB745CB459B54A6;
	fma.rn.f64 	%fd202, %fd200, %fd191, %fd201;
	mov.f64 	%fd203, 0d3FBC71C741A0669F;
	fma.rn.f64 	%fd204, %fd202, %fd191, %fd203;
	mov.f64 	%fd205, 0d3FC249249209112E;
	fma.rn.f64 	%fd206, %fd204, %fd191, %fd205;
	mov.f64 	%fd207, 0d3FC99999999A06C1;
	fma.rn.f64 	%fd208, %fd206, %fd191, %fd207;
	mov.f64 	%fd209, 0d3FD5555555555535;
	fma.rn.f64 	%fd210, %fd208, %fd191, %fd209;
	mul.f64 	%fd211, %fd191, %fd210;
	fma.rn.f64 	%fd212, %fd211, %fd190, %fd190;
	add.f64 	%fd213, %fd212, %fd212;
	mov.f64 	%fd214, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd215, %fd177, %fd214, %fd213;
	mov.f64 	%fd216, 0dC009000000000000;
	sub.f64 	%fd217, %fd216, %fd215;
	mov.f64 	%fd218, 0dBC08DDF93324D327;
	mov.f64 	%fd219, 0dBBB135D2E746E627;
	fma.rn.f64 	%fd220, %fd219, %fd217, %fd218;
	mov.f64 	%fd221, 0d3C37B83EEF0B7C9F;
	fma.rn.f64 	%fd222, %fd220, %fd217, %fd221;
	mov.f64 	%fd223, 0d3C69BA72CD589B91;
	fma.rn.f64 	%fd224, %fd222, %fd217, %fd223;
	mov.f64 	%fd225, 0dBCA33689090A6B96;
	fma.rn.f64 	%fd226, %fd224, %fd217, %fd225;
	mov.f64 	%fd227, 0d3C782E11898132E0;
	fma.rn.f64 	%fd228, %fd226, %fd217, %fd227;
	mov.f64 	%fd229, 0d3CFDE4ACFD9E26BA;
	fma.rn.f64 	%fd230, %fd228, %fd217, %fd229;
	mov.f64 	%fd231, 0dBD26D33EED66C487;
	fma.rn.f64 	%fd232, %fd230, %fd217, %fd231;
	mov.f64 	%fd233, 0dBD36F2167040D8E2;
	fma.rn.f64 	%fd234, %fd232, %fd217, %fd233;
	mov.f64 	%fd235, 0d3D872A22C2D77E20;
	fma.rn.f64 	%fd236, %fd234, %fd217, %fd235;
	mov.f64 	%fd237, 0dBDAC8859C4E5C0AF;
	fma.rn.f64 	%fd238, %fd236, %fd217, %fd237;
	mov.f64 	%fd239, 0dBDCDC583D118A561;
	fma.rn.f64 	%fd240, %fd238, %fd217, %fd239;
	mov.f64 	%fd241, 0d3E120F47CCF46B3C;
	fma.rn.f64 	%fd242, %fd240, %fd217, %fd241;
	mov.f64 	%fd243, 0dBE31A9E38DC84D60;
	fma.rn.f64 	%fd244, %fd242, %fd217, %fd243;
	mov.f64 	%fd245, 0dBE5F36CD6D3D46A9;
	fma.rn.f64 	%fd246, %fd244, %fd217, %fd245;
	mov.f64 	%fd247, 0d3E9C6B4F5D03B787;
	fma.rn.f64 	%fd248, %fd246, %fd217, %fd247;
	mov.f64 	%fd249, 0dBEB6E8A5434AE8A2;
	fma.rn.f64 	%fd250, %fd248, %fd217, %fd249;
	mov.f64 	%fd251, 0dBEED1D1F7B8736F6;
	fma.rn.f64 	%fd252, %fd250, %fd217, %fd251;
	mov.f64 	%fd253, 0d3F2879C2A212F024;
	fma.rn.f64 	%fd254, %fd252, %fd217, %fd253;
	mov.f64 	%fd255, 0dBF4845769484FCA8;
	fma.rn.f64 	%fd256, %fd254, %fd217, %fd255;
	mov.f64 	%fd257, 0dBF78B6C33114F909;
	fma.rn.f64 	%fd258, %fd256, %fd217, %fd257;
	mov.f64 	%fd259, 0d3FCEBD80D9B13E28;
	fma.rn.f64 	%fd260, %fd258, %fd217, %fd259;
	mov.f64 	%fd261, 0d3FFA755E7C99AE86;
	fma.rn.f64 	%fd262, %fd260, %fd217, %fd261;
	fma.rn.f64 	%fd266, %fd262, %fd2, %fd262;
	bra.uni 	BB38_17;

BB38_2:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	setp.gt.s32	%p5, %r2, 1072693247;
	selp.f64	%fd4, %fd3, %fd1, %p5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r58}, %fd4;
	}
	mov.b32 	 %f1, %r58;
	setp.ltu.f32	%p6, %f1, 0f2B2BFF2F;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r59, %temp}, %fd4;
	}
	@%p6 bra 	BB38_4;
	bra.uni 	BB38_3;

BB38_4:
	setp.gt.f64	%p7, %fd4, 0d0000000000000000;
	setp.lt.s32	%p8, %r58, 2146435072;
	and.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB38_9;
	bra.uni 	BB38_5;

BB38_9:
	mov.u32 	%r60, -1023;
	setp.gt.s32	%p13, %r58, 1048575;
	@%p13 bra 	BB38_11;

	mul.f64 	%fd105, %fd4, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r58}, %fd105;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r59, %temp}, %fd105;
	}
	mov.u32 	%r60, -1077;

BB38_11:
	shr.u32 	%r31, %r58, 20;
	add.s32 	%r61, %r60, %r31;
	and.b32  	%r32, %r58, -2146435073;
	or.b32  	%r33, %r32, 1072693248;
	mov.b64 	%fd263, {%r59, %r33};
	setp.lt.s32	%p14, %r33, 1073127583;
	@%p14 bra 	BB38_13;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd263;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd263;
	}
	add.s32 	%r36, %r35, -1048576;
	mov.b64 	%fd263, {%r34, %r36};
	add.s32 	%r61, %r61, 1;

BB38_13:
	add.f64 	%fd107, %fd263, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd106,%fd107;
	// inline asm
	neg.f64 	%fd108, %fd107;
	mov.f64 	%fd109, 0d3FF0000000000000;
	fma.rn.f64 	%fd110, %fd108, %fd106, %fd109;
	fma.rn.f64 	%fd111, %fd110, %fd110, %fd110;
	fma.rn.f64 	%fd112, %fd111, %fd106, %fd106;
	add.f64 	%fd113, %fd263, 0dBFF0000000000000;
	mul.f64 	%fd114, %fd113, %fd112;
	fma.rn.f64 	%fd115, %fd113, %fd112, %fd114;
	mul.f64 	%fd116, %fd115, %fd115;
	mov.f64 	%fd117, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd118, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd119, %fd118, %fd116, %fd117;
	mov.f64 	%fd120, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd121, %fd119, %fd116, %fd120;
	mov.f64 	%fd122, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd123, %fd121, %fd116, %fd122;
	mov.f64 	%fd124, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd125, %fd123, %fd116, %fd124;
	mov.f64 	%fd126, 0d3F624924923BE72D;
	fma.rn.f64 	%fd127, %fd125, %fd116, %fd126;
	mov.f64 	%fd128, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd129, %fd127, %fd116, %fd128;
	mov.f64 	%fd130, 0d3FB5555555555554;
	fma.rn.f64 	%fd131, %fd129, %fd116, %fd130;
	sub.f64 	%fd132, %fd113, %fd115;
	add.f64 	%fd133, %fd132, %fd132;
	neg.f64 	%fd134, %fd115;
	fma.rn.f64 	%fd135, %fd134, %fd113, %fd133;
	mul.f64 	%fd136, %fd112, %fd135;
	mul.f64 	%fd137, %fd116, %fd131;
	fma.rn.f64 	%fd138, %fd137, %fd115, %fd136;
	xor.b32  	%r37, %r61, -2147483648;
	mov.u32 	%r38, 1127219200;
	mov.b64 	%fd139, {%r37, %r38};
	mov.u32 	%r39, -2147483648;
	mov.b64 	%fd140, {%r39, %r38};
	sub.f64 	%fd141, %fd139, %fd140;
	mov.f64 	%fd142, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd143, %fd141, %fd142, %fd115;
	neg.f64 	%fd144, %fd141;
	fma.rn.f64 	%fd145, %fd144, %fd142, %fd143;
	sub.f64 	%fd146, %fd145, %fd115;
	sub.f64 	%fd147, %fd138, %fd146;
	mov.f64 	%fd148, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd149, %fd141, %fd148, %fd147;
	add.f64 	%fd264, %fd143, %fd149;
	bra.uni 	BB38_14;

BB38_3:
	shr.u32 	%r22, %r58, 20;
	and.b32  	%r23, %r22, 2046;
	add.s32 	%r24, %r23, 2147482626;
	mov.u32 	%r25, 1127219200;
	mov.b64 	%fd23, {%r24, %r25};
	mov.u32 	%r26, -2147483648;
	mov.b64 	%fd24, {%r26, %r25};
	sub.f64 	%fd25, %fd23, %fd24;
	and.b32  	%r27, %r58, -2145386497;
	add.s32 	%r28, %r27, 1071644672;
	mov.b64 	%fd26, {%r59, %r28};
	add.f64 	%fd27, %fd26, 0dBFF0000000000000;
	add.f64 	%fd20, %fd26, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd19,%fd20;
	// inline asm
	neg.f64 	%fd28, %fd20;
	mov.f64 	%fd29, 0d3FF0000000000000;
	fma.rn.f64 	%fd30, %fd28, %fd19, %fd29;
	fma.rn.f64 	%fd31, %fd30, %fd30, %fd30;
	fma.rn.f64 	%fd32, %fd31, %fd19, %fd19;
	mul.f64 	%fd33, %fd27, %fd32;
	mov.f64 	%fd34, 0dC000000000000000;
	fma.rn.f64 	%fd35, %fd34, %fd33, %fd27;
	neg.f64 	%fd36, %fd33;
	fma.rn.f64 	%fd37, %fd36, %fd27, %fd35;
	fma.rn.f64 	%fd38, %fd37, %fd32, %fd33;
	mul.f64 	%fd39, %fd38, %fd38;
	mov.f64 	%fd40, 0d3FA55CF59CDC5D89;
	mov.f64 	%fd41, 0d3FB5C5C218C775C9;
	fma.rn.f64 	%fd42, %fd41, %fd39, %fd40;
	mov.f64 	%fd43, 0d3FAEFD18CF6EBB9C;
	fma.rn.f64 	%fd44, %fd42, %fd39, %fd43;
	mov.f64 	%fd45, 0d3FB10682EDCB8D1B;
	fma.rn.f64 	%fd46, %fd44, %fd39, %fd45;
	mov.f64 	%fd47, 0d3FB3B1DD3AC7FC96;
	fma.rn.f64 	%fd48, %fd46, %fd39, %fd47;
	mov.f64 	%fd49, 0d3FB745CB459B54A6;
	fma.rn.f64 	%fd50, %fd48, %fd39, %fd49;
	mov.f64 	%fd51, 0d3FBC71C741A0669F;
	fma.rn.f64 	%fd52, %fd50, %fd39, %fd51;
	mov.f64 	%fd53, 0d3FC249249209112E;
	fma.rn.f64 	%fd54, %fd52, %fd39, %fd53;
	mov.f64 	%fd55, 0d3FC99999999A06C1;
	fma.rn.f64 	%fd56, %fd54, %fd39, %fd55;
	mov.f64 	%fd57, 0d3FD5555555555535;
	fma.rn.f64 	%fd58, %fd56, %fd39, %fd57;
	mul.f64 	%fd59, %fd39, %fd58;
	fma.rn.f64 	%fd60, %fd59, %fd38, %fd38;
	add.f64 	%fd61, %fd60, %fd60;
	mov.f64 	%fd62, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd63, %fd25, %fd62, %fd61;
	neg.f64 	%fd22, %fd63;
	// inline asm
	rsqrt.approx.ftz.f64 %fd21, %fd22;
	// inline asm
	mul.rn.f64 	%fd64, %fd21, %fd21;
	neg.f64 	%fd65, %fd64;
	fma.rn.f64 	%fd66, %fd22, %fd65, %fd29;
	mov.f64 	%fd67, 0d3FE0000000000000;
	mov.f64 	%fd68, 0d3FD8000000000000;
	fma.rn.f64 	%fd69, %fd68, %fd66, %fd67;
	mul.rn.f64 	%fd70, %fd66, %fd21;
	fma.rn.f64 	%fd71, %fd69, %fd70, %fd21;
	mov.f64 	%fd72, 0d4000A0E7333839AA;
	mov.f64 	%fd73, 0d3FEBE9222591AFAB;
	fma.rn.f64 	%fd74, %fd73, %fd71, %fd72;
	mov.f64 	%fd75, 0d4008768CF7E57D5C;
	fma.rn.f64 	%fd76, %fd74, %fd71, %fd75;
	mov.f64 	%fd77, 0d400B77E7E28DA583;
	fma.rn.f64 	%fd78, %fd76, %fd71, %fd77;
	mov.f64 	%fd79, 0d3FF34F26A4F99CF9;
	fma.rn.f64 	%fd80, %fd78, %fd71, %fd79;
	mov.f64 	%fd81, 0d3FC1F674ADB019ED;
	fma.rn.f64 	%fd82, %fd80, %fd71, %fd81;
	mov.f64 	%fd83, 0d3F75DDAE9506431D;
	fma.rn.f64 	%fd84, %fd82, %fd71, %fd83;
	mov.f64 	%fd85, 0d3F0ADA49AA32489C;
	fma.rn.f64 	%fd86, %fd84, %fd71, %fd85;
	add.f64 	%fd87, %fd71, 0d4001E90FF51C2197;
	mov.f64 	%fd88, 0d40111EA3A7CF3820;
	fma.rn.f64 	%fd89, %fd87, %fd71, %fd88;
	mov.f64 	%fd90, 0d4011A0E4A4749594;
	fma.rn.f64 	%fd91, %fd89, %fd71, %fd90;
	mov.f64 	%fd92, 0d400D4E977D38C14D;
	fma.rn.f64 	%fd93, %fd91, %fd71, %fd92;
	mov.f64 	%fd94, 0d3FF37FD567EC0D5F;
	fma.rn.f64 	%fd95, %fd93, %fd71, %fd94;
	mov.f64 	%fd96, 0d3FC1FB9D7F676033;
	fma.rn.f64 	%fd97, %fd95, %fd71, %fd96;
	mov.f64 	%fd98, 0d3F75DDCDF98946E4;
	fma.rn.f64 	%fd99, %fd97, %fd71, %fd98;
	mov.f64 	%fd100, 0d3F0ADA42D79D8DBB;
	fma.rn.f64 	%fd101, %fd99, %fd71, %fd100;
	mul.f64 	%fd102, %fd71, %fd101;
	div.rn.f64 	%fd265, %fd86, %fd102;
	bra.uni 	BB38_15;

BB38_5:
	abs.f64 	%fd103, %fd4;
	setp.gtu.f64	%p10, %fd103, 0d7FF0000000000000;
	@%p10 bra 	BB38_8;
	bra.uni 	BB38_6;

BB38_8:
	add.f64 	%fd264, %fd4, %fd4;
	bra.uni 	BB38_14;

BB38_6:
	setp.eq.f64	%p11, %fd4, 0d0000000000000000;
	mov.f64 	%fd264, 0dFFF0000000000000;
	@%p11 bra 	BB38_14;

	setp.eq.f64	%p12, %fd4, 0d7FF0000000000000;
	selp.f64	%fd264, %fd4, 0dFFF8000000000000, %p12;

BB38_14:
	neg.f64 	%fd150, %fd264;
	rsqrt.approx.f64 	%fd151, %fd150;
	mov.f64 	%fd152, 0d3FFA2013964E259C;
	mov.f64 	%fd153, 0d3FE8E2101C71B0BF;
	fma.rn.f64 	%fd154, %fd153, %fd151, %fd152;
	mov.f64 	%fd155, 0d3FDABFE90921BE68;
	fma.rn.f64 	%fd156, %fd154, %fd151, %fd155;
	mov.f64 	%fd157, 0d3F97E41314DE00D4;
	fma.rn.f64 	%fd158, %fd156, %fd151, %fd157;
	mov.f64 	%fd159, 0d3F311BD487102E94;
	fma.rn.f64 	%fd160, %fd158, %fd151, %fd159;
	add.f64 	%fd161, %fd151, 0d3FF59895C30BAA54;
	mov.f64 	%fd162, 0d3FFAE8E5956A143F;
	fma.rn.f64 	%fd163, %fd161, %fd151, %fd162;
	mov.f64 	%fd164, 0d3FDACCE85FF7383D;
	fma.rn.f64 	%fd165, %fd163, %fd151, %fd164;
	mov.f64 	%fd166, 0d3F97E43B6CAC34FE;
	fma.rn.f64 	%fd167, %fd165, %fd151, %fd166;
	mov.f64 	%fd168, 0d3F311BD08289EB12;
	fma.rn.f64 	%fd169, %fd167, %fd151, %fd168;
	mul.f64 	%fd170, %fd151, %fd169;
	div.rn.f64 	%fd265, %fd160, %fd170;

BB38_15:
	neg.f64 	%fd171, %fd265;
	selp.f64	%fd266, %fd171, %fd265, %p5;

BB38_17:
	mov.u32 	%r57, %tid.x;
	mov.u32 	%r56, %ctaid.x;
	mov.u32 	%r55, %ntid.x;
	mad.lo.s32 	%r54, %r55, %r56, %r57;
	shl.b32 	%r53, %r54, 3;
	ld.param.u32 	%r52, [vec_erfcinv_param_1];
	cvta.to.global.u32 	%r49, %r52;
	add.s32 	%r51, %r49, %r53;
	st.global.f64 	[%r51], %fd266;

BB38_18:
	ret;
}

	// .globl	vec_erfcx
.visible .entry vec_erfcx(
	.param .u32 vec_erfcx_param_0,
	.param .u32 vec_erfcx_param_1,
	.param .u32 vec_erfcx_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<39>;
	.reg .f64 	%fd<141>;


	ld.param.u32 	%r8, [vec_erfcx_param_0];
	ld.param.u32 	%r7, [vec_erfcx_param_2];
	mov.u32 	%r9, %tid.x;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r9;
	setp.ge.u32	%p1, %r1, %r8;
	@%p1 bra 	BB39_10;

	cvta.to.global.u32 	%r12, %r7;
	shl.b32 	%r13, %r1, 3;
	add.s32 	%r14, %r12, %r13;
	ld.global.f64 	%fd1, [%r14];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	mov.b32 	 %f2, %r2;
	abs.f32 	%f3, %f2;
	setp.lt.f32	%p2, %f3, 0f40400000;
	@%p2 bra 	BB39_3;
	bra.uni 	BB39_2;

BB39_3:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r15, %temp}, %fd1;
	}
	and.b32  	%r16, %r2, 2147483647;
	mov.b64 	%fd31, {%r15, %r16};
	add.f64 	%fd32, %fd31, 0dC010000000000000;
	add.f64 	%fd28, %fd31, 0d4010000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd27,%fd28;
	// inline asm
	neg.f64 	%fd33, %fd28;
	mov.f64 	%fd34, 0d3FF0000000000000;
	fma.rn.f64 	%fd35, %fd33, %fd27, %fd34;
	fma.rn.f64 	%fd36, %fd35, %fd35, %fd35;
	fma.rn.f64 	%fd37, %fd36, %fd27, %fd27;
	mul.f64 	%fd38, %fd32, %fd37;
	add.rn.f64 	%fd39, %fd38, %fd34;
	mov.f64 	%fd40, 0dC010000000000000;
	fma.rn.f64 	%fd41, %fd40, %fd39, %fd31;
	neg.f64 	%fd42, %fd38;
	fma.rn.f64 	%fd43, %fd42, %fd31, %fd41;
	fma.rn.f64 	%fd44, %fd37, %fd43, %fd38;
	mov.f64 	%fd45, 0dBE44E1C6FD03D328;
	mov.f64 	%fd46, 0dBDF8774AD4E0BFD7;
	fma.rn.f64 	%fd47, %fd46, %fd44, %fd45;
	mov.f64 	%fd48, 0dBE4330149F7A56B6;
	fma.rn.f64 	%fd49, %fd47, %fd44, %fd48;
	mov.f64 	%fd50, 0d3E7BEDDED8376273;
	fma.rn.f64 	%fd51, %fd49, %fd44, %fd50;
	mov.f64 	%fd52, 0d3E6F9254C3ABF22B;
	fma.rn.f64 	%fd53, %fd51, %fd44, %fd52;
	mov.f64 	%fd54, 0dBEAB9068C2148CF0;
	fma.rn.f64 	%fd55, %fd53, %fd44, %fd54;
	mov.f64 	%fd56, 0d3E94C6454DB34009;
	fma.rn.f64 	%fd57, %fd55, %fd44, %fd56;
	mov.f64 	%fd58, 0d3ED7F1C378F2311D;
	fma.rn.f64 	%fd59, %fd57, %fd44, %fd58;
	mov.f64 	%fd60, 0dBEE78E051C6D5C58;
	fma.rn.f64 	%fd61, %fd59, %fd44, %fd60;
	mov.f64 	%fd62, 0dBEF995B4EAD14A90;
	fma.rn.f64 	%fd63, %fd61, %fd44, %fd62;
	mov.f64 	%fd64, 0d3F23BE27CF0A29B2;
	fma.rn.f64 	%fd65, %fd63, %fd44, %fd64;
	mov.f64 	%fd66, 0dBF2A1DEF3E81672E;
	fma.rn.f64 	%fd67, %fd65, %fd44, %fd66;
	mov.f64 	%fd68, 0dBF48D4ABE68C1713;
	fma.rn.f64 	%fd69, %fd67, %fd44, %fd68;
	mov.f64 	%fd70, 0d3F749C67210DD6B4;
	fma.rn.f64 	%fd71, %fd69, %fd44, %fd70;
	mov.f64 	%fd72, 0dBF9096238568E357;
	fma.rn.f64 	%fd73, %fd71, %fd44, %fd72;
	mov.f64 	%fd74, 0d3FA3079EDF8C2DC9;
	fma.rn.f64 	%fd75, %fd73, %fd44, %fd74;
	mov.f64 	%fd76, 0dBFB0FB06DFF601FC;
	fma.rn.f64 	%fd77, %fd75, %fd44, %fd76;
	mov.f64 	%fd78, 0d3FB7FEE004DFBCDC;
	fma.rn.f64 	%fd79, %fd77, %fd44, %fd78;
	mov.f64 	%fd80, 0dBFB9DDB23C3DB8C6;
	fma.rn.f64 	%fd81, %fd79, %fd44, %fd80;
	mov.f64 	%fd82, 0d3FB16ECEFCFA5FDA;
	fma.rn.f64 	%fd83, %fd81, %fd44, %fd82;
	mov.f64 	%fd84, 0d3F8F7F5DF66FB6D6;
	fma.rn.f64 	%fd85, %fd83, %fd44, %fd84;
	mov.f64 	%fd86, 0dBFC1DF1AD154A29D;
	fma.rn.f64 	%fd87, %fd85, %fd44, %fd86;
	mov.f64 	%fd88, 0d3FF3BA5916E9FD7F;
	fma.rn.f64 	%fd89, %fd87, %fd44, %fd88;
	mov.f64 	%fd90, 0d4000000000000000;
	fma.rn.f64 	%fd30, %fd90, %fd31, %fd34;
	// inline asm
	rcp.approx.ftz.f64 %fd29,%fd30;
	// inline asm
	neg.f64 	%fd91, %fd30;
	fma.rn.f64 	%fd92, %fd91, %fd29, %fd34;
	fma.rn.f64 	%fd93, %fd92, %fd92, %fd92;
	fma.rn.f64 	%fd94, %fd93, %fd29, %fd29;
	mul.f64 	%fd95, %fd89, %fd94;
	mul.f64 	%fd96, %fd95, 0dC000000000000000;
	fma.rn.f64 	%fd97, %fd31, %fd96, %fd89;
	neg.f64 	%fd98, %fd95;
	add.rn.f64 	%fd99, %fd97, %fd98;
	fma.rn.f64 	%fd140, %fd99, %fd94, %fd95;
	bra.uni 	BB39_4;

BB39_2:
	rcp.rn.f64 	%fd13, %fd1;
	mul.f64 	%fd14, %fd13, %fd13;
	mov.f64 	%fd15, 0d401A400000000000;
	mov.f64 	%fd16, 0dC03D880000000000;
	fma.rn.f64 	%fd17, %fd16, %fd14, %fd15;
	mov.f64 	%fd18, 0dBFFE000000000000;
	fma.rn.f64 	%fd19, %fd17, %fd14, %fd18;
	mov.f64 	%fd20, 0d3FE8000000000000;
	fma.rn.f64 	%fd21, %fd19, %fd14, %fd20;
	mov.f64 	%fd22, 0dBFE0000000000000;
	fma.rn.f64 	%fd23, %fd21, %fd14, %fd22;
	mov.f64 	%fd24, 0d3FF0000000000000;
	fma.rn.f64 	%fd25, %fd23, %fd14, %fd24;
	mul.f64 	%fd26, %fd13, 0d3FE20DD750429B6D;
	mul.f64 	%fd140, %fd26, %fd25;

BB39_4:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r32}, %fd1;
	}
	setp.gt.s32	%p3, %r32, -1;
	@%p3 bra 	BB39_9;

	mul.f64 	%fd5, %fd1, %fd1;
	neg.f64 	%fd100, %fd5;
	fma.rn.f64 	%fd6, %fd1, %fd1, %fd100;
	mov.f64 	%fd101, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd102, %fd5, %fd101;
	mov.f64 	%fd103, 0d4338000000000000;
	add.rn.f64 	%fd104, %fd102, %fd103;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r3, %temp}, %fd104;
	}
	mov.f64 	%fd105, 0dC338000000000000;
	add.rn.f64 	%fd106, %fd104, %fd105;
	mov.f64 	%fd107, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd108, %fd106, %fd107, %fd5;
	mov.f64 	%fd109, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd110, %fd106, %fd109, %fd108;
	mov.f64 	%fd111, 0d3E928AF3FCA213EA;
	mov.f64 	%fd112, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd113, %fd112, %fd110, %fd111;
	mov.f64 	%fd114, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd115, %fd113, %fd110, %fd114;
	mov.f64 	%fd116, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd117, %fd115, %fd110, %fd116;
	mov.f64 	%fd118, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd119, %fd117, %fd110, %fd118;
	mov.f64 	%fd120, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd121, %fd119, %fd110, %fd120;
	mov.f64 	%fd122, 0d3F81111111122322;
	fma.rn.f64 	%fd123, %fd121, %fd110, %fd122;
	mov.f64 	%fd124, 0d3FA55555555502A1;
	fma.rn.f64 	%fd125, %fd123, %fd110, %fd124;
	mov.f64 	%fd126, 0d3FC5555555555511;
	fma.rn.f64 	%fd127, %fd125, %fd110, %fd126;
	mov.f64 	%fd128, 0d3FE000000000000B;
	fma.rn.f64 	%fd129, %fd127, %fd110, %fd128;
	mov.f64 	%fd130, 0d3FF0000000000000;
	fma.rn.f64 	%fd131, %fd129, %fd110, %fd130;
	fma.rn.f64 	%fd132, %fd131, %fd110, %fd130;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r4, %temp}, %fd132;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r5}, %fd132;
	}
	shl.b32 	%r17, %r3, 20;
	add.s32 	%r18, %r5, %r17;
	mov.b64 	%fd139, {%r4, %r18};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd5;
	}
	mov.b32 	 %f4, %r19;
	abs.f32 	%f1, %f4;
	setp.lt.f32	%p4, %f1, 0f4086232B;
	@%p4 bra 	BB39_8;

	setp.lt.f64	%p5, %fd5, 0d0000000000000000;
	add.f64 	%fd133, %fd5, 0d7FF0000000000000;
	selp.f64	%fd139, 0d0000000000000000, %fd133, %p5;
	setp.geu.f32	%p6, %f1, 0f40874800;
	@%p6 bra 	BB39_8;

	shr.u32 	%r20, %r3, 31;
	add.s32 	%r21, %r3, %r20;
	shr.s32 	%r22, %r21, 1;
	shl.b32 	%r23, %r22, 20;
	add.s32 	%r24, %r23, %r5;
	mov.b64 	%fd134, {%r4, %r24};
	sub.s32 	%r25, %r3, %r22;
	shl.b32 	%r26, %r25, 20;
	add.s32 	%r27, %r26, 1072693248;
	mov.u32 	%r28, 0;
	mov.b64 	%fd135, {%r28, %r27};
	mul.f64 	%fd139, %fd134, %fd135;

BB39_8:
	add.f64 	%fd136, %fd139, %fd139;
	fma.rn.f64 	%fd137, %fd136, %fd6, %fd136;
	sub.f64 	%fd138, %fd137, %fd140;
	setp.eq.f64	%p7, %fd136, 0d7FF0000000000000;
	selp.f64	%fd140, %fd136, %fd138, %p7;

BB39_9:
	mov.u32 	%r38, %tid.x;
	mov.u32 	%r37, %ctaid.x;
	mov.u32 	%r36, %ntid.x;
	mad.lo.s32 	%r35, %r36, %r37, %r38;
	shl.b32 	%r34, %r35, 3;
	ld.param.u32 	%r33, [vec_erfcx_param_1];
	cvta.to.global.u32 	%r29, %r33;
	add.s32 	%r31, %r29, %r34;
	st.global.f64 	[%r31], %fd140;

BB39_10:
	ret;
}

	// .globl	vec_erf
.visible .entry vec_erf(
	.param .u32 vec_erf_param_0,
	.param .u32 vec_erf_param_1,
	.param .u32 vec_erf_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<36>;
	.reg .f64 	%fd<112>;


	ld.param.u32 	%r7, [vec_erf_param_0];
	ld.param.u32 	%r6, [vec_erf_param_2];
	mov.u32 	%r8, %tid.x;
	mov.u32 	%r9, %ntid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r1, %r9, %r10, %r8;
	setp.ge.u32	%p1, %r1, %r7;
	@%p1 bra 	BB40_9;

	cvta.to.global.u32 	%r11, %r6;
	shl.b32 	%r12, %r1, 3;
	add.s32 	%r13, %r11, %r12;
	ld.global.f64 	%fd1, [%r13];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	and.b32  	%r3, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r4, %temp}, %fd1;
	}
	setp.lt.u32	%p2, %r3, 1072693248;
	@%p2 bra 	BB40_7;
	bra.uni 	BB40_2;

BB40_7:
	mul.f64 	%fd87, %fd1, %fd1;
	mov.f64 	%fd88, 0d3E4D5F4BB7A316F6;
	mov.f64 	%fd89, 0dBE0A83AA3B08FBC2;
	fma.rn.f64 	%fd90, %fd89, %fd87, %fd88;
	mov.f64 	%fd91, 0dBE85BDCE301B3CDF;
	fma.rn.f64 	%fd92, %fd90, %fd87, %fd91;
	mov.f64 	%fd93, 0d3EBB978FADB81BC9;
	fma.rn.f64 	%fd94, %fd92, %fd87, %fd93;
	mov.f64 	%fd95, 0dBEEF4C99D6AE5FB8;
	fma.rn.f64 	%fd96, %fd94, %fd87, %fd95;
	mov.f64 	%fd97, 0d3F1F9A2AF549012E;
	fma.rn.f64 	%fd98, %fd96, %fd87, %fd97;
	mov.f64 	%fd99, 0dBF4C02DAFC636A47;
	fma.rn.f64 	%fd100, %fd98, %fd87, %fd99;
	mov.f64 	%fd101, 0d3F7565BCCF619AC0;
	fma.rn.f64 	%fd102, %fd100, %fd87, %fd101;
	mov.f64 	%fd103, 0dBF9B82CE311E321A;
	fma.rn.f64 	%fd104, %fd102, %fd87, %fd103;
	mov.f64 	%fd105, 0d3FBCE2F21A04075C;
	fma.rn.f64 	%fd106, %fd104, %fd87, %fd105;
	mov.f64 	%fd107, 0dBFD812746B0379B4;
	fma.rn.f64 	%fd108, %fd106, %fd87, %fd107;
	mov.f64 	%fd109, 0d3FF20DD750429B6D;
	fma.rn.f64 	%fd110, %fd108, %fd87, %fd109;
	mul.f64 	%fd111, %fd1, %fd110;
	bra.uni 	BB40_8;

BB40_2:
	setp.lt.u32	%p3, %r3, 2146435072;
	@%p3 bra 	BB40_6;
	bra.uni 	BB40_3;

BB40_6:
	mov.b64 	%fd8, {%r4, %r3};
	mov.f64 	%fd9, 0dBCF1384CE38C616A;
	mov.f64 	%fd10, 0d3C8B9C2B870030E8;
	fma.rn.f64 	%fd11, %fd10, %fd8, %fd9;
	mov.f64 	%fd12, 0d3D4458AE9746C2FD;
	fma.rn.f64 	%fd13, %fd11, %fd8, %fd12;
	mov.f64 	%fd14, 0dBD8E4A44D4F1AB56;
	fma.rn.f64 	%fd15, %fd13, %fd8, %fd14;
	mov.f64 	%fd16, 0d3DCFDF15265C58EE;
	fma.rn.f64 	%fd17, %fd15, %fd8, %fd16;
	mov.f64 	%fd18, 0dBE0933832F358D51;
	fma.rn.f64 	%fd19, %fd17, %fd8, %fd18;
	mov.f64 	%fd20, 0d3E3F136D3F719446;
	fma.rn.f64 	%fd21, %fd19, %fd8, %fd20;
	mov.f64 	%fd22, 0dBE6E94C2FE151B3B;
	fma.rn.f64 	%fd23, %fd21, %fd8, %fd22;
	mov.f64 	%fd24, 0d3E985A70310EE0A8;
	fma.rn.f64 	%fd25, %fd23, %fd8, %fd24;
	mov.f64 	%fd26, 0dBEBF944DA1520B74;
	fma.rn.f64 	%fd27, %fd25, %fd8, %fd26;
	mov.f64 	%fd28, 0d3EE09F503825C543;
	fma.rn.f64 	%fd29, %fd27, %fd8, %fd28;
	mov.f64 	%fd30, 0dBEFBEEFE9F949E59;
	fma.rn.f64 	%fd31, %fd29, %fd8, %fd30;
	mov.f64 	%fd32, 0d3F11D785C6E28857;
	fma.rn.f64 	%fd33, %fd31, %fd8, %fd32;
	mov.f64 	%fd34, 0dBF1D866B223048C7;
	fma.rn.f64 	%fd35, %fd33, %fd8, %fd34;
	mov.f64 	%fd36, 0d3EF258F0847E8908;
	fma.rn.f64 	%fd37, %fd35, %fd8, %fd36;
	mov.f64 	%fd38, 0d3F429CFC58DBB776;
	fma.rn.f64 	%fd39, %fd37, %fd8, %fd38;
	mov.f64 	%fd40, 0dBF5BE16D3F71F3C5;
	fma.rn.f64 	%fd41, %fd39, %fd8, %fd40;
	mov.f64 	%fd42, 0d3F2E8BDA60326B1A;
	fma.rn.f64 	%fd43, %fd41, %fd8, %fd42;
	mov.f64 	%fd44, 0d3F938FB20B0988A6;
	fma.rn.f64 	%fd45, %fd43, %fd8, %fd44;
	mov.f64 	%fd46, 0dBFBA4E3A80F64E33;
	fma.rn.f64 	%fd47, %fd45, %fd8, %fd46;
	mov.f64 	%fd48, 0dBFE45F3E88093928;
	fma.rn.f64 	%fd49, %fd47, %fd8, %fd48;
	mov.f64 	%fd50, 0dBFF20DD599CAEEA0;
	fma.rn.f64 	%fd51, %fd49, %fd8, %fd50;
	mov.f64 	%fd52, 0dBE883BE1E31CE133;
	fma.rn.f64 	%fd53, %fd51, %fd8, %fd52;
	mov.f64 	%fd54, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd55, %fd53, %fd54;
	mov.f64 	%fd56, 0d4338000000000000;
	add.rn.f64 	%fd57, %fd55, %fd56;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r18, %temp}, %fd57;
	}
	mov.f64 	%fd58, 0dC338000000000000;
	add.rn.f64 	%fd59, %fd57, %fd58;
	mov.f64 	%fd60, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd61, %fd59, %fd60, %fd53;
	mov.f64 	%fd62, 0d3E928AF3FCA213EA;
	mov.f64 	%fd63, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd64, %fd63, %fd61, %fd62;
	mov.f64 	%fd65, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd66, %fd64, %fd61, %fd65;
	mov.f64 	%fd67, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd68, %fd66, %fd61, %fd67;
	mov.f64 	%fd69, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd70, %fd68, %fd61, %fd69;
	mov.f64 	%fd71, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd72, %fd70, %fd61, %fd71;
	mov.f64 	%fd73, 0d3F81111111122322;
	fma.rn.f64 	%fd74, %fd72, %fd61, %fd73;
	mov.f64 	%fd75, 0d3FA55555555502A1;
	fma.rn.f64 	%fd76, %fd74, %fd61, %fd75;
	mov.f64 	%fd77, 0d3FC5555555555511;
	fma.rn.f64 	%fd78, %fd76, %fd61, %fd77;
	mov.f64 	%fd79, 0d3FE000000000000B;
	fma.rn.f64 	%fd80, %fd78, %fd61, %fd79;
	mov.f64 	%fd81, 0d3FF0000000000000;
	fma.rn.f64 	%fd82, %fd80, %fd61, %fd81;
	fma.rn.f64 	%fd83, %fd82, %fd61, %fd81;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd83;
	}
	shl.b32 	%r20, %r18, 20;
	add.s32 	%r21, %r19, %r20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd83;
	}
	mov.b64 	%fd84, {%r22, %r21};
	sub.f64 	%fd85, %fd81, %fd84;
	setp.gt.u32	%p7, %r3, 1075294207;
	selp.f64	%fd86, 0d3FF0000000000000, %fd85, %p7;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r23, %temp}, %fd86;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r24}, %fd86;
	}
	and.b32  	%r25, %r2, -2147483648;
	or.b32  	%r26, %r24, %r25;
	mov.b64 	%fd111, {%r23, %r26};
	bra.uni 	BB40_8;

BB40_3:
	setp.eq.s32	%p4, %r3, 2146435072;
	setp.eq.s32	%p5, %r4, 0;
	and.pred  	%p6, %p4, %p5;
	@%p6 bra 	BB40_5;
	bra.uni 	BB40_4;

BB40_5:
	mov.f64 	%fd7, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd7;
	}
	and.b32  	%r15, %r2, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd7;
	}
	or.b32  	%r17, %r16, %r15;
	mov.b64 	%fd111, {%r14, %r17};
	bra.uni 	BB40_8;

BB40_4:
	add.f64 	%fd111, %fd1, %fd1;

BB40_8:
	mov.u32 	%r35, %tid.x;
	mov.u32 	%r34, %ctaid.x;
	mov.u32 	%r33, %ntid.x;
	mad.lo.s32 	%r32, %r33, %r34, %r35;
	shl.b32 	%r31, %r32, 3;
	ld.param.u32 	%r30, [vec_erf_param_1];
	cvta.to.global.u32 	%r27, %r30;
	add.s32 	%r29, %r27, %r31;
	st.global.f64 	[%r29], %fd111;

BB40_9:
	ret;
}

	// .globl	vec_erfinv
.visible .entry vec_erfinv(
	.param .u32 vec_erfinv_param_0,
	.param .u32 vec_erfinv_param_1,
	.param .u32 vec_erfinv_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<32>;
	.reg .f64 	%fd<173>;


	ld.param.u32 	%r4, [vec_erfinv_param_0];
	ld.param.u32 	%r3, [vec_erfinv_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB41_10;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd1, [%r10];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r11}, %fd1;
	}
	mov.b32 	 %f1, %r11;
	abs.f32 	%f2, %f1;
	setp.lt.f32	%p2, %f2, 0f3FF00000;
	@%p2 bra 	BB41_4;
	bra.uni 	BB41_2;

BB41_4:
	neg.f64 	%fd12, %fd1;
	mov.f64 	%fd13, 0d3FF0000000000000;
	fma.rn.f64 	%fd14, %fd1, %fd12, %fd13;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd14;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd14;
	}
	shr.u32 	%r14, %r12, 20;
	and.b32  	%r15, %r14, 2046;
	add.s32 	%r16, %r15, 2147482626;
	mov.u32 	%r17, 1127219200;
	mov.b64 	%fd15, {%r16, %r17};
	mov.u32 	%r18, -2147483648;
	mov.b64 	%fd16, {%r18, %r17};
	sub.f64 	%fd17, %fd15, %fd16;
	and.b32  	%r19, %r12, -2145386497;
	add.s32 	%r20, %r19, 1071644672;
	mov.b64 	%fd18, {%r13, %r20};
	add.f64 	%fd19, %fd18, 0dBFF0000000000000;
	add.f64 	%fd11, %fd18, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd10,%fd11;
	// inline asm
	neg.f64 	%fd20, %fd11;
	fma.rn.f64 	%fd21, %fd20, %fd10, %fd13;
	fma.rn.f64 	%fd22, %fd21, %fd21, %fd21;
	fma.rn.f64 	%fd23, %fd22, %fd10, %fd10;
	mul.f64 	%fd24, %fd19, %fd23;
	mov.f64 	%fd25, 0dC000000000000000;
	fma.rn.f64 	%fd26, %fd25, %fd24, %fd19;
	neg.f64 	%fd27, %fd24;
	fma.rn.f64 	%fd28, %fd27, %fd19, %fd26;
	fma.rn.f64 	%fd29, %fd28, %fd23, %fd24;
	mul.f64 	%fd30, %fd29, %fd29;
	mov.f64 	%fd31, 0d3FA55CF59CDC5D89;
	mov.f64 	%fd32, 0d3FB5C5C218C775C9;
	fma.rn.f64 	%fd33, %fd32, %fd30, %fd31;
	mov.f64 	%fd34, 0d3FAEFD18CF6EBB9C;
	fma.rn.f64 	%fd35, %fd33, %fd30, %fd34;
	mov.f64 	%fd36, 0d3FB10682EDCB8D1B;
	fma.rn.f64 	%fd37, %fd35, %fd30, %fd36;
	mov.f64 	%fd38, 0d3FB3B1DD3AC7FC96;
	fma.rn.f64 	%fd39, %fd37, %fd30, %fd38;
	mov.f64 	%fd40, 0d3FB745CB459B54A6;
	fma.rn.f64 	%fd41, %fd39, %fd30, %fd40;
	mov.f64 	%fd42, 0d3FBC71C741A0669F;
	fma.rn.f64 	%fd43, %fd41, %fd30, %fd42;
	mov.f64 	%fd44, 0d3FC249249209112E;
	fma.rn.f64 	%fd45, %fd43, %fd30, %fd44;
	mov.f64 	%fd46, 0d3FC99999999A06C1;
	fma.rn.f64 	%fd47, %fd45, %fd30, %fd46;
	mov.f64 	%fd48, 0d3FD5555555555535;
	fma.rn.f64 	%fd49, %fd47, %fd30, %fd48;
	mul.f64 	%fd50, %fd30, %fd49;
	fma.rn.f64 	%fd51, %fd50, %fd29, %fd29;
	add.f64 	%fd52, %fd51, %fd51;
	mov.f64 	%fd53, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd4, %fd17, %fd53, %fd52;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd4;
	}
	setp.lt.u32	%p5, %r21, -1072103424;
	@%p5 bra 	BB41_8;
	bra.uni 	BB41_5;

BB41_8:
	mov.f64 	%fd125, 0dC009000000000000;
	sub.f64 	%fd126, %fd125, %fd4;
	mov.f64 	%fd127, 0dBC08DDF93324D327;
	mov.f64 	%fd128, 0dBBB135D2E746E627;
	fma.rn.f64 	%fd129, %fd128, %fd126, %fd127;
	mov.f64 	%fd130, 0d3C37B83EEF0B7C9F;
	fma.rn.f64 	%fd131, %fd129, %fd126, %fd130;
	mov.f64 	%fd132, 0d3C69BA72CD589B91;
	fma.rn.f64 	%fd133, %fd131, %fd126, %fd132;
	mov.f64 	%fd134, 0dBCA33689090A6B96;
	fma.rn.f64 	%fd135, %fd133, %fd126, %fd134;
	mov.f64 	%fd136, 0d3C782E11898132E0;
	fma.rn.f64 	%fd137, %fd135, %fd126, %fd136;
	mov.f64 	%fd138, 0d3CFDE4ACFD9E26BA;
	fma.rn.f64 	%fd139, %fd137, %fd126, %fd138;
	mov.f64 	%fd140, 0dBD26D33EED66C487;
	fma.rn.f64 	%fd141, %fd139, %fd126, %fd140;
	mov.f64 	%fd142, 0dBD36F2167040D8E2;
	fma.rn.f64 	%fd143, %fd141, %fd126, %fd142;
	mov.f64 	%fd144, 0d3D872A22C2D77E20;
	fma.rn.f64 	%fd145, %fd143, %fd126, %fd144;
	mov.f64 	%fd146, 0dBDAC8859C4E5C0AF;
	fma.rn.f64 	%fd147, %fd145, %fd126, %fd146;
	mov.f64 	%fd148, 0dBDCDC583D118A561;
	fma.rn.f64 	%fd149, %fd147, %fd126, %fd148;
	mov.f64 	%fd150, 0d3E120F47CCF46B3C;
	fma.rn.f64 	%fd151, %fd149, %fd126, %fd150;
	mov.f64 	%fd152, 0dBE31A9E38DC84D60;
	fma.rn.f64 	%fd153, %fd151, %fd126, %fd152;
	mov.f64 	%fd154, 0dBE5F36CD6D3D46A9;
	fma.rn.f64 	%fd155, %fd153, %fd126, %fd154;
	mov.f64 	%fd156, 0d3E9C6B4F5D03B787;
	fma.rn.f64 	%fd157, %fd155, %fd126, %fd156;
	mov.f64 	%fd158, 0dBEB6E8A5434AE8A2;
	fma.rn.f64 	%fd159, %fd157, %fd126, %fd158;
	mov.f64 	%fd160, 0dBEED1D1F7B8736F6;
	fma.rn.f64 	%fd161, %fd159, %fd126, %fd160;
	mov.f64 	%fd162, 0d3F2879C2A212F024;
	fma.rn.f64 	%fd163, %fd161, %fd126, %fd162;
	mov.f64 	%fd164, 0dBF4845769484FCA8;
	fma.rn.f64 	%fd165, %fd163, %fd126, %fd164;
	mov.f64 	%fd166, 0dBF78B6C33114F909;
	fma.rn.f64 	%fd167, %fd165, %fd126, %fd166;
	mov.f64 	%fd168, 0d3FCEBD80D9B13E28;
	fma.rn.f64 	%fd169, %fd167, %fd126, %fd168;
	mov.f64 	%fd170, 0d3FFA755E7C99AE86;
	fma.rn.f64 	%fd8, %fd169, %fd126, %fd170;
	mov.f64 	%fd172, %fd8;
	bra.uni 	BB41_9;

BB41_2:
	abs.f64 	%fd2, %fd1;
	setp.gtu.f64	%p3, %fd2, 0d7FF0000000000000;
	mov.f64 	%fd172, %fd1;
	@%p3 bra 	BB41_9;

	setp.eq.f64	%p4, %fd2, 0d3FF0000000000000;
	selp.f64	%fd3, 0d7FF0000000000000, 0dFFF8000000000000, %p4;
	mov.f64 	%fd172, %fd3;
	bra.uni 	BB41_9;

BB41_5:
	neg.f64 	%fd54, %fd4;
	sqrt.rn.f64 	%fd5, %fd54;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd5;
	}
	setp.lt.s32	%p6, %r22, 1074790400;
	@%p6 bra 	BB41_7;
	bra.uni 	BB41_6;

BB41_7:
	add.f64 	%fd88, %fd5, 0dC00A000000000000;
	mov.f64 	%fd89, 0d3E785CBE52878635;
	mov.f64 	%fd90, 0d3E23040F87DBD932;
	fma.rn.f64 	%fd91, %fd90, %fd88, %fd89;
	mov.f64 	%fd92, 0dBE92777453DD3955;
	fma.rn.f64 	%fd93, %fd91, %fd88, %fd92;
	mov.f64 	%fd94, 0d3E5395ABCD554C6C;
	fma.rn.f64 	%fd95, %fd93, %fd88, %fd94;
	mov.f64 	%fd96, 0d3EB936388A3790AD;
	fma.rn.f64 	%fd97, %fd95, %fd88, %fd96;
	mov.f64 	%fd98, 0dBED0D5DB812B5083;
	fma.rn.f64 	%fd99, %fd97, %fd88, %fd98;
	mov.f64 	%fd100, 0d3EC8860CD5D652F6;
	fma.rn.f64 	%fd101, %fd99, %fd88, %fd100;
	mov.f64 	%fd102, 0d3EEA29A0CACDFB23;
	fma.rn.f64 	%fd103, %fd101, %fd88, %fd102;
	mov.f64 	%fd104, 0dBF08CEF1F80281F2;
	fma.rn.f64 	%fd105, %fd103, %fd88, %fd104;
	mov.f64 	%fd106, 0d3F11E684D0B9188A;
	fma.rn.f64 	%fd107, %fd105, %fd88, %fd106;
	mov.f64 	%fd108, 0d3EF932CD54C8A222;
	fma.rn.f64 	%fd109, %fd107, %fd88, %fd108;
	mov.f64 	%fd110, 0dBF37448A89EF8AA3;
	fma.rn.f64 	%fd111, %fd109, %fd88, %fd110;
	mov.f64 	%fd112, 0d3F4F3CC55AD40C25;
	fma.rn.f64 	%fd113, %fd111, %fd88, %fd112;
	mov.f64 	%fd114, 0dBF5BA924132F38B1;
	fma.rn.f64 	%fd115, %fd113, %fd88, %fd114;
	mov.f64 	%fd116, 0d3F6468EECA533CF8;
	fma.rn.f64 	%fd117, %fd115, %fd88, %fd116;
	mov.f64 	%fd118, 0dBF6EBADABB891BBD;
	fma.rn.f64 	%fd119, %fd117, %fd88, %fd118;
	mov.f64 	%fd120, 0d3F75FFCFE5B76AFC;
	fma.rn.f64 	%fd121, %fd119, %fd88, %fd120;
	mov.f64 	%fd122, 0d3FF0158A6D641D39;
	fma.rn.f64 	%fd123, %fd121, %fd88, %fd122;
	mov.f64 	%fd124, 0d4008ABCC380D5A48;
	fma.rn.f64 	%fd7, %fd123, %fd88, %fd124;
	mov.f64 	%fd172, %fd7;
	bra.uni 	BB41_9;

BB41_6:
	add.f64 	%fd55, %fd5, 0dC014000000000000;
	mov.f64 	%fd56, 0dBDF18FEEC0E38727;
	mov.f64 	%fd57, 0dBDBDCEC3A7785389;
	fma.rn.f64 	%fd58, %fd57, %fd55, %fd56;
	mov.f64 	%fd59, 0d3E19E6BF2DDA45E3;
	fma.rn.f64 	%fd60, %fd58, %fd55, %fd59;
	mov.f64 	%fd61, 0dBE30468FB24E2F5F;
	fma.rn.f64 	%fd62, %fd60, %fd55, %fd61;
	mov.f64 	%fd63, 0d3E405AC6A8FBA182;
	fma.rn.f64 	%fd64, %fd62, %fd55, %fd63;
	mov.f64 	%fd65, 0dBE50102E495FB9C0;
	fma.rn.f64 	%fd66, %fd64, %fd55, %fd65;
	mov.f64 	%fd67, 0d3E5F4C20E1334AF8;
	fma.rn.f64 	%fd68, %fd66, %fd55, %fd67;
	mov.f64 	%fd69, 0dBE722D220FDF9C3E;
	fma.rn.f64 	%fd70, %fd68, %fd55, %fd69;
	mov.f64 	%fd71, 0d3E8EBC8BB824CB54;
	fma.rn.f64 	%fd72, %fd70, %fd55, %fd71;
	mov.f64 	%fd73, 0dBEB0A8D40EA372CC;
	fma.rn.f64 	%fd74, %fd72, %fd55, %fd73;
	mov.f64 	%fd75, 0d3ED2FBD29D093D2B;
	fma.rn.f64 	%fd76, %fd74, %fd55, %fd75;
	mov.f64 	%fd77, 0dBEF4A3497E1E0FAC;
	fma.rn.f64 	%fd78, %fd76, %fd55, %fd77;
	mov.f64 	%fd79, 0d3F13EBF4EB00938F;
	fma.rn.f64 	%fd80, %fd78, %fd55, %fd79;
	mov.f64 	%fd81, 0dBF2C2F36A8FC5D53;
	fma.rn.f64 	%fd82, %fd80, %fd55, %fd81;
	mov.f64 	%fd83, 0dBF222EA5DF04047C;
	fma.rn.f64 	%fd84, %fd82, %fd55, %fd83;
	mov.f64 	%fd85, 0d3FF02A30D1FBA0DC;
	fma.rn.f64 	%fd86, %fd84, %fd55, %fd85;
	mov.f64 	%fd87, 0d4013664DDD1AD7FB;
	fma.rn.f64 	%fd6, %fd86, %fd55, %fd87;
	mov.f64 	%fd172, %fd6;

BB41_9:
	mov.f64 	%fd9, %fd172;
	mov.u32 	%r31, %tid.x;
	mov.u32 	%r30, %ctaid.x;
	mov.u32 	%r29, %ntid.x;
	mad.lo.s32 	%r28, %r29, %r30, %r31;
	shl.b32 	%r27, %r28, 3;
	ld.param.u32 	%r26, [vec_erfinv_param_1];
	cvta.to.global.u32 	%r23, %r26;
	add.s32 	%r25, %r23, %r27;
	mul.f64 	%fd171, %fd1, %fd9;
	st.global.f64 	[%r25], %fd171;

BB41_10:
	ret;
}

	// .globl	vec_exp10
.visible .entry vec_exp10(
	.param .u32 vec_exp10_param_0,
	.param .u32 vec_exp10_param_1,
	.param .u32 vec_exp10_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<29>;
	.reg .f64 	%fd<47>;


	ld.param.u32 	%r8, [vec_exp10_param_0];
	ld.param.u32 	%r6, [vec_exp10_param_1];
	ld.param.u32 	%r7, [vec_exp10_param_2];
	mov.u32 	%r9, %tid.x;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r9;
	setp.ge.u32	%p1, %r1, %r8;
	@%p1 bra 	BB42_5;

	cvta.to.global.u32 	%r12, %r7;
	shl.b32 	%r13, %r1, 3;
	add.s32 	%r14, %r12, %r13;
	ld.global.f64 	%fd1, [%r14];
	mov.f64 	%fd6, 0d400A934F0979A371;
	mul.rn.f64 	%fd7, %fd1, %fd6;
	mov.f64 	%fd8, 0d4338000000000000;
	add.rn.f64 	%fd9, %fd7, %fd8;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r2, %temp}, %fd9;
	}
	mov.f64 	%fd10, 0dC338000000000000;
	add.rn.f64 	%fd11, %fd9, %fd10;
	mov.f64 	%fd12, 0dBFD34413509F79FF;
	fma.rn.f64 	%fd13, %fd11, %fd12, %fd1;
	mov.f64 	%fd14, 0d3C49DC1DA994FD21;
	fma.rn.f64 	%fd15, %fd11, %fd14, %fd13;
	mul.f64 	%fd16, %fd15, 0dBCAF48AD494EA3E9;
	mov.f64 	%fd17, 0d40026BB1BBB55516;
	fma.rn.f64 	%fd18, %fd15, %fd17, %fd16;
	mov.f64 	%fd19, 0d3E928AF3FCA213EA;
	mov.f64 	%fd20, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd21, %fd20, %fd18, %fd19;
	mov.f64 	%fd22, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd23, %fd21, %fd18, %fd22;
	mov.f64 	%fd24, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd25, %fd23, %fd18, %fd24;
	mov.f64 	%fd26, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd27, %fd25, %fd18, %fd26;
	mov.f64 	%fd28, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd29, %fd27, %fd18, %fd28;
	mov.f64 	%fd30, 0d3F81111111122322;
	fma.rn.f64 	%fd31, %fd29, %fd18, %fd30;
	mov.f64 	%fd32, 0d3FA55555555502A1;
	fma.rn.f64 	%fd33, %fd31, %fd18, %fd32;
	mov.f64 	%fd34, 0d3FC5555555555511;
	fma.rn.f64 	%fd35, %fd33, %fd18, %fd34;
	mov.f64 	%fd36, 0d3FE000000000000B;
	fma.rn.f64 	%fd37, %fd35, %fd18, %fd36;
	mov.f64 	%fd38, 0d3FF0000000000000;
	fma.rn.f64 	%fd39, %fd37, %fd18, %fd38;
	fma.rn.f64 	%fd40, %fd39, %fd18, %fd38;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r3, %temp}, %fd40;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd40;
	}
	shl.b32 	%r15, %r2, 20;
	add.s32 	%r16, %r4, %r15;
	mov.b64 	%fd46, {%r3, %r16};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r5}, %fd1;
	}
	mov.b32 	 %f2, %r5;
	abs.f32 	%f1, %f2;
	setp.lt.f32	%p2, %f1, 0f40733A71;
	@%p2 bra 	BB42_4;

	setp.lt.s32	%p3, %r5, 0;
	selp.f64	%fd41, 0d0000000000000000, 0d7FF0000000000000, %p3;
	abs.f64 	%fd42, %fd1;
	setp.gtu.f64	%p4, %fd42, 0d7FF0000000000000;
	add.f64 	%fd43, %fd1, %fd1;
	selp.f64	%fd46, %fd43, %fd41, %p4;
	setp.geu.f32	%p5, %f1, 0f407439B8;
	@%p5 bra 	BB42_4;

	shr.u32 	%r17, %r2, 31;
	add.s32 	%r18, %r2, %r17;
	shr.s32 	%r19, %r18, 1;
	shl.b32 	%r20, %r19, 20;
	add.s32 	%r21, %r20, %r4;
	mov.b64 	%fd44, {%r3, %r21};
	sub.s32 	%r22, %r2, %r19;
	shl.b32 	%r23, %r22, 20;
	add.s32 	%r24, %r23, 1072693248;
	mov.u32 	%r25, 0;
	mov.b64 	%fd45, {%r25, %r24};
	mul.f64 	%fd46, %fd44, %fd45;

BB42_4:
	cvta.to.global.u32 	%r26, %r6;
	add.s32 	%r28, %r26, %r13;
	st.global.f64 	[%r28], %fd46;

BB42_5:
	ret;
}

	// .globl	vec_exp2
.visible .entry vec_exp2(
	.param .u32 vec_exp2_param_0,
	.param .u32 vec_exp2_param_1,
	.param .u32 vec_exp2_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<29>;
	.reg .f64 	%fd<42>;


	ld.param.u32 	%r8, [vec_exp2_param_0];
	ld.param.u32 	%r6, [vec_exp2_param_1];
	ld.param.u32 	%r7, [vec_exp2_param_2];
	mov.u32 	%r9, %tid.x;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r9;
	setp.ge.u32	%p1, %r1, %r8;
	@%p1 bra 	BB43_5;

	cvta.to.global.u32 	%r12, %r7;
	shl.b32 	%r13, %r1, 3;
	add.s32 	%r14, %r12, %r13;
	ld.global.f64 	%fd1, [%r14];
	mov.f64 	%fd6, 0d4338000000000000;
	add.rn.f64 	%fd7, %fd1, %fd6;
	mov.f64 	%fd8, 0dC338000000000000;
	add.rn.f64 	%fd9, %fd7, %fd8;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r2, %temp}, %fd7;
	}
	sub.f64 	%fd10, %fd1, %fd9;
	mul.f64 	%fd11, %fd10, 0d3C7ABC9E3B39803F;
	mov.f64 	%fd12, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd13, %fd10, %fd12, %fd11;
	mov.f64 	%fd14, 0d3E928AF3FCA213EA;
	mov.f64 	%fd15, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd16, %fd15, %fd13, %fd14;
	mov.f64 	%fd17, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd18, %fd16, %fd13, %fd17;
	mov.f64 	%fd19, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd20, %fd18, %fd13, %fd19;
	mov.f64 	%fd21, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd22, %fd20, %fd13, %fd21;
	mov.f64 	%fd23, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd24, %fd22, %fd13, %fd23;
	mov.f64 	%fd25, 0d3F81111111122322;
	fma.rn.f64 	%fd26, %fd24, %fd13, %fd25;
	mov.f64 	%fd27, 0d3FA55555555502A1;
	fma.rn.f64 	%fd28, %fd26, %fd13, %fd27;
	mov.f64 	%fd29, 0d3FC5555555555511;
	fma.rn.f64 	%fd30, %fd28, %fd13, %fd29;
	mov.f64 	%fd31, 0d3FE000000000000B;
	fma.rn.f64 	%fd32, %fd30, %fd13, %fd31;
	mov.f64 	%fd33, 0d3FF0000000000000;
	fma.rn.f64 	%fd34, %fd32, %fd13, %fd33;
	fma.rn.f64 	%fd35, %fd34, %fd13, %fd33;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r3, %temp}, %fd35;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd35;
	}
	shl.b32 	%r15, %r2, 20;
	add.s32 	%r16, %r4, %r15;
	mov.b64 	%fd41, {%r3, %r16};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r5}, %fd1;
	}
	mov.b32 	 %f2, %r5;
	abs.f32 	%f1, %f2;
	setp.lt.f32	%p2, %f1, 0f408FF000;
	@%p2 bra 	BB43_4;

	setp.lt.s32	%p3, %r5, 0;
	selp.f64	%fd36, 0d0000000000000000, 0d7FF0000000000000, %p3;
	abs.f64 	%fd37, %fd1;
	setp.gtu.f64	%p4, %fd37, 0d7FF0000000000000;
	add.f64 	%fd38, %fd1, %fd1;
	selp.f64	%fd41, %fd38, %fd36, %p4;
	setp.geu.f32	%p5, %f1, 0f4090CC00;
	@%p5 bra 	BB43_4;

	shr.u32 	%r17, %r2, 31;
	add.s32 	%r18, %r2, %r17;
	shr.s32 	%r19, %r18, 1;
	shl.b32 	%r20, %r19, 20;
	add.s32 	%r21, %r20, %r4;
	mov.b64 	%fd39, {%r3, %r21};
	sub.s32 	%r22, %r2, %r19;
	shl.b32 	%r23, %r22, 20;
	add.s32 	%r24, %r23, 1072693248;
	mov.u32 	%r25, 0;
	mov.b64 	%fd40, {%r25, %r24};
	mul.f64 	%fd41, %fd39, %fd40;

BB43_4:
	cvta.to.global.u32 	%r26, %r6;
	add.s32 	%r28, %r26, %r13;
	st.global.f64 	[%r28], %fd41;

BB43_5:
	ret;
}

	// .globl	vec_exp
.visible .entry vec_exp(
	.param .u32 vec_exp_param_0,
	.param .u32 vec_exp_param_1,
	.param .u32 vec_exp_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<29>;
	.reg .f64 	%fd<42>;


	ld.param.u32 	%r7, [vec_exp_param_0];
	ld.param.u32 	%r5, [vec_exp_param_1];
	ld.param.u32 	%r6, [vec_exp_param_2];
	mov.u32 	%r8, %tid.x;
	mov.u32 	%r9, %ntid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r1, %r9, %r10, %r8;
	setp.ge.u32	%p1, %r1, %r7;
	@%p1 bra 	BB44_5;

	cvta.to.global.u32 	%r11, %r6;
	shl.b32 	%r12, %r1, 3;
	add.s32 	%r13, %r11, %r12;
	ld.global.f64 	%fd1, [%r13];
	mov.f64 	%fd6, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd7, %fd1, %fd6;
	mov.f64 	%fd8, 0d4338000000000000;
	add.rn.f64 	%fd9, %fd7, %fd8;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r2, %temp}, %fd9;
	}
	mov.f64 	%fd10, 0dC338000000000000;
	add.rn.f64 	%fd11, %fd9, %fd10;
	mov.f64 	%fd12, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd13, %fd11, %fd12, %fd1;
	mov.f64 	%fd14, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd15, %fd11, %fd14, %fd13;
	mov.f64 	%fd16, 0d3E928AF3FCA213EA;
	mov.f64 	%fd17, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd18, %fd17, %fd15, %fd16;
	mov.f64 	%fd19, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd20, %fd18, %fd15, %fd19;
	mov.f64 	%fd21, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd22, %fd20, %fd15, %fd21;
	mov.f64 	%fd23, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd24, %fd22, %fd15, %fd23;
	mov.f64 	%fd25, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd26, %fd24, %fd15, %fd25;
	mov.f64 	%fd27, 0d3F81111111122322;
	fma.rn.f64 	%fd28, %fd26, %fd15, %fd27;
	mov.f64 	%fd29, 0d3FA55555555502A1;
	fma.rn.f64 	%fd30, %fd28, %fd15, %fd29;
	mov.f64 	%fd31, 0d3FC5555555555511;
	fma.rn.f64 	%fd32, %fd30, %fd15, %fd31;
	mov.f64 	%fd33, 0d3FE000000000000B;
	fma.rn.f64 	%fd34, %fd32, %fd15, %fd33;
	mov.f64 	%fd35, 0d3FF0000000000000;
	fma.rn.f64 	%fd36, %fd34, %fd15, %fd35;
	fma.rn.f64 	%fd37, %fd36, %fd15, %fd35;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r3, %temp}, %fd37;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd37;
	}
	shl.b32 	%r14, %r2, 20;
	add.s32 	%r15, %r4, %r14;
	mov.b64 	%fd41, {%r3, %r15};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd1;
	}
	mov.b32 	 %f2, %r16;
	abs.f32 	%f1, %f2;
	setp.lt.f32	%p2, %f1, 0f4086232B;
	@%p2 bra 	BB44_4;

	setp.lt.f64	%p3, %fd1, 0d0000000000000000;
	add.f64 	%fd38, %fd1, 0d7FF0000000000000;
	selp.f64	%fd41, 0d0000000000000000, %fd38, %p3;
	setp.geu.f32	%p4, %f1, 0f40874800;
	@%p4 bra 	BB44_4;

	shr.u32 	%r17, %r2, 31;
	add.s32 	%r18, %r2, %r17;
	shr.s32 	%r19, %r18, 1;
	shl.b32 	%r20, %r19, 20;
	add.s32 	%r21, %r20, %r4;
	mov.b64 	%fd39, {%r3, %r21};
	sub.s32 	%r22, %r2, %r19;
	shl.b32 	%r23, %r22, 20;
	add.s32 	%r24, %r23, 1072693248;
	mov.u32 	%r25, 0;
	mov.b64 	%fd40, {%r25, %r24};
	mul.f64 	%fd41, %fd39, %fd40;

BB44_4:
	cvta.to.global.u32 	%r26, %r5;
	add.s32 	%r28, %r26, %r12;
	st.global.f64 	[%r28], %fd41;

BB44_5:
	ret;
}

	// .globl	vec_expm1
.visible .entry vec_expm1(
	.param .u32 vec_expm1_param_0,
	.param .u32 vec_expm1_param_1,
	.param .u32 vec_expm1_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<24>;
	.reg .f64 	%fd<49>;


	ld.param.u32 	%r5, [vec_expm1_param_0];
	ld.param.u32 	%r3, [vec_expm1_param_1];
	ld.param.u32 	%r4, [vec_expm1_param_2];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB45_5;

	cvta.to.global.u32 	%r9, %r4;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	ld.global.f64 	%fd1, [%r11];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	setp.lt.u32	%p2, %r2, 1082535491;
	setp.lt.s32	%p3, %r2, -1068859392;
	or.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB45_3;
	bra.uni 	BB45_2;

BB45_3:
	mov.f64 	%fd8, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd9, %fd1, %fd8;
	mov.f64 	%fd10, 0d4338000000000000;
	add.rn.f64 	%fd11, %fd9, %fd10;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r12, %temp}, %fd11;
	}
	mov.f64 	%fd12, 0dC338000000000000;
	add.rn.f64 	%fd13, %fd11, %fd12;
	mov.f64 	%fd14, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd15, %fd13, %fd14, %fd1;
	mov.f64 	%fd16, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd17, %fd13, %fd16, %fd15;
	add.s32 	%r13, %r2, %r2;
	setp.lt.u32	%p7, %r13, 2142496327;
	selp.b32	%r14, 0, %r12, %p7;
	selp.f64	%fd18, %fd1, %fd17, %p7;
	mov.f64 	%fd19, 0d3E5AF86D8EBD13CD;
	mov.f64 	%fd20, 0d3E21F4076ACD15B6;
	fma.rn.f64 	%fd21, %fd20, %fd18, %fd19;
	mov.f64 	%fd22, 0d3E927E5092BA033D;
	fma.rn.f64 	%fd23, %fd21, %fd18, %fd22;
	mov.f64 	%fd24, 0d3EC71DDE6C5F9DA1;
	fma.rn.f64 	%fd25, %fd23, %fd18, %fd24;
	mov.f64 	%fd26, 0d3EFA01A018D034E6;
	fma.rn.f64 	%fd27, %fd25, %fd18, %fd26;
	mov.f64 	%fd28, 0d3F2A01A01B3B6940;
	fma.rn.f64 	%fd29, %fd27, %fd18, %fd28;
	mov.f64 	%fd30, 0d3F56C16C16C1B5DD;
	fma.rn.f64 	%fd31, %fd29, %fd18, %fd30;
	mov.f64 	%fd32, 0d3F8111111110F74D;
	fma.rn.f64 	%fd33, %fd31, %fd18, %fd32;
	mov.f64 	%fd34, 0d3FA555555555554D;
	fma.rn.f64 	%fd35, %fd33, %fd18, %fd34;
	mov.f64 	%fd36, 0d3FC5555555555557;
	fma.rn.f64 	%fd37, %fd35, %fd18, %fd36;
	mov.f64 	%fd38, 0d3FE0000000000000;
	fma.rn.f64 	%fd39, %fd37, %fd18, %fd38;
	mul.f64 	%fd40, %fd18, %fd39;
	fma.rn.f64 	%fd41, %fd40, %fd18, %fd18;
	setp.eq.s32	%p8, %r14, 1024;
	selp.b32	%r15, -1, 0, %p8;
	add.s32 	%r16, %r15, %r14;
	shl.b32 	%r17, %r16, 20;
	add.s32 	%r18, %r17, 1072693248;
	mov.u32 	%r19, 0;
	mov.b64 	%fd42, {%r19, %r18};
	mov.u32 	%r20, 1072693248;
	mov.b64 	%fd43, {%r19, %r20};
	sub.f64 	%fd44, %fd42, %fd43;
	fma.rn.f64 	%fd45, %fd41, %fd42, %fd44;
	add.f64 	%fd46, %fd45, %fd45;
	selp.f64	%fd47, %fd46, %fd45, %p8;
	setp.eq.s32	%p9, %r13, 0;
	selp.f64	%fd48, %fd18, %fd47, %p9;
	bra.uni 	BB45_4;

BB45_2:
	setp.lt.s32	%p5, %r2, 0;
	selp.f64	%fd5, 0dBFF0000000000000, 0d7FF0000000000000, %p5;
	abs.f64 	%fd6, %fd1;
	setp.gtu.f64	%p6, %fd6, 0d7FF0000000000000;
	add.f64 	%fd7, %fd1, %fd1;
	selp.f64	%fd48, %fd7, %fd5, %p6;

BB45_4:
	cvta.to.global.u32 	%r21, %r3;
	add.s32 	%r23, %r21, %r10;
	st.global.f64 	[%r23], %fd48;

BB45_5:
	ret;
}

	// .globl	vec_fabs
.visible .entry vec_fabs(
	.param .u32 vec_fabs_param_0,
	.param .u32 vec_fabs_param_1,
	.param .u32 vec_fabs_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<3>;


	ld.param.u32 	%r4, [vec_fabs_param_0];
	ld.param.u32 	%r2, [vec_fabs_param_1];
	ld.param.u32 	%r3, [vec_fabs_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB46_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd1, [%r10];
	abs.f64 	%fd2, %fd1;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd2;

BB46_2:
	ret;
}

	// .globl	vec_floor
.visible .entry vec_floor(
	.param .u32 vec_floor_param_0,
	.param .u32 vec_floor_param_1,
	.param .u32 vec_floor_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<3>;


	ld.param.u32 	%r4, [vec_floor_param_0];
	ld.param.u32 	%r2, [vec_floor_param_1];
	ld.param.u32 	%r3, [vec_floor_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB47_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd1, [%r10];
	cvt.rmi.f64.f64	%fd2, %fd1;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd2;

BB47_2:
	ret;
}

	// .globl	vec_j0
.visible .entry vec_j0(
	.param .u32 vec_j0_param_0,
	.param .u32 vec_j0_param_1,
	.param .u32 vec_j0_param_2
)
{
	.local .align 4 .b8 	__local_depot48[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<11>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<216>;


	mov.u32 	%r40, __local_depot48;
	cvta.local.u32 	%SP, %r40;
	ld.param.u32 	%r13, [vec_j0_param_0];
	ld.param.u32 	%r11, [vec_j0_param_1];
	ld.param.u32 	%r12, [vec_j0_param_2];
	add.u32 	%r14, %SP, 0;
	cvta.to.local.u32 	%r1, %r14;
	add.u32 	%r15, %SP, 4;
	cvta.to.local.u32 	%r2, %r15;
	mov.u32 	%r16, %ntid.x;
	mov.u32 	%r17, %ctaid.x;
	mov.u32 	%r18, %tid.x;
	mad.lo.s32 	%r3, %r16, %r17, %r18;
	setp.ge.u32	%p1, %r3, %r13;
	@%p1 bra 	BB48_20;

	cvta.to.global.u32 	%r19, %r12;
	shl.b32 	%r20, %r3, 3;
	add.s32 	%r21, %r19, %r20;
	ld.global.f64 	%fd25, [%r21];
	abs.f64 	%fd1, %fd25;
	setp.gtu.f64	%p2, %fd1, 0d400FB319F277BBE5;
	@%p2 bra 	BB48_3;
	bra.uni 	BB48_2;

BB48_3:
	setp.gtu.f64	%p3, %fd1, 0d401C58FD1A62F5EC;
	@%p3 bra 	BB48_5;
	bra.uni 	BB48_4;

BB48_5:
	setp.gtu.f64	%p4, %fd1, 0d402471FCB6A7A8C0;
	@%p4 bra 	BB48_7;
	bra.uni 	BB48_6;

BB48_7:
	abs.f64 	%fd132, %fd1;
	mov.f64 	%fd215, 0d0000000000000000;
	setp.eq.f64	%p5, %fd132, 0d7FF0000000000000;
	@%p5 bra 	BB48_19;

	// inline asm
	rcp.approx.ftz.f64 %fd133,%fd1;
	// inline asm
	neg.f64 	%fd135, %fd1;
	mov.f64 	%fd136, 0d3FF0000000000000;
	fma.rn.f64 	%fd137, %fd135, %fd133, %fd136;
	fma.rn.f64 	%fd138, %fd137, %fd137, %fd137;
	fma.rn.f64 	%fd139, %fd138, %fd133, %fd133;
	mul.f64 	%fd140, %fd139, %fd139;
	mov.f64 	%fd141, 0d409927467A655012;
	mov.f64 	%fd142, 0dC0D115CB8C11A9DC;
	fma.rn.f64 	%fd143, %fd142, %fd140, %fd141;
	mov.f64 	%fd144, 0dC05751787E247BD4;
	fma.rn.f64 	%fd145, %fd143, %fd140, %fd144;
	mov.f64 	%fd146, 0d401704C4E5FC36B2;
	fma.rn.f64 	%fd147, %fd145, %fd140, %fd146;
	mov.f64 	%fd148, 0dBFE15B747A2FD531;
	fma.rn.f64 	%fd149, %fd147, %fd140, %fd148;
	mov.f64 	%fd150, 0d3FBA7FEACF6CB79B;
	fma.rn.f64 	%fd151, %fd149, %fd140, %fd150;
	mov.f64 	%fd152, 0dBFAFFFFFEDDCF548;
	fma.rn.f64 	%fd153, %fd151, %fd140, %fd152;
	mov.f64 	%fd154, 0d3FEFFFFFFFFFC9E5;
	fma.rn.f64 	%fd155, %fd153, %fd140, %fd154;
	mov.f64 	%fd156, 0d410ECD4523B12B84;
	mov.f64 	%fd157, 0dC14602FE1C34685E;
	fma.rn.f64 	%fd158, %fd157, %fd140, %fd156;
	mov.f64 	%fd159, 0dC0C7A2FC1972F05A;
	fma.rn.f64 	%fd160, %fd158, %fd140, %fd159;
	mov.f64 	%fd161, 0d407EBA131F7E5BEB;
	fma.rn.f64 	%fd162, %fd160, %fd140, %fd161;
	mov.f64 	%fd163, 0dC0373B92E6E7CC7D;
	fma.rn.f64 	%fd164, %fd162, %fd140, %fd163;
	mov.f64 	%fd165, 0d3FFA31BEE63A2F08;
	fma.rn.f64 	%fd166, %fd164, %fd140, %fd165;
	mov.f64 	%fd167, 0dBFCAD320104D5D05;
	fma.rn.f64 	%fd168, %fd166, %fd140, %fd167;
	mov.f64 	%fd169, 0d3FB0AAAA9C76D07E;
	fma.rn.f64 	%fd170, %fd168, %fd140, %fd169;
	mov.f64 	%fd171, 0dBFBFFFFFFFFDACEC;
	fma.rn.f64 	%fd172, %fd170, %fd140, %fd171;
	fma.rn.f64 	%fd5, %fd172, %fd139, %fd1;
	rsqrt.approx.f64 	%fd173, %fd1;
	mul.f64 	%fd174, %fd173, 0d3FE9884533D43651;
	mul.f64 	%fd6, %fd155, %fd174;
	mul.f64 	%fd175, %fd5, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r38, %fd175;
	st.local.u32 	[%r2], %r38;
	cvt.rn.f64.s32	%fd176, %r38;
	neg.f64 	%fd177, %fd176;
	mov.f64 	%fd178, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd179, %fd177, %fd178, %fd5;
	mov.f64 	%fd180, 0d3C91A62633145C00;
	fma.rn.f64 	%fd181, %fd177, %fd180, %fd179;
	mov.f64 	%fd182, 0d397B839A252049C0;
	fma.rn.f64 	%fd211, %fd177, %fd182, %fd181;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd5;
	}
	and.b32  	%r23, %r22, 2145386496;
	setp.lt.u32	%p6, %r23, 1105199104;
	@%p6 bra 	BB48_10;

	add.u32 	%r37, %SP, 4;
	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd5;
	.param .b32 param1;
	st.param.b32	[param1+0], %r37;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd211, [retval0+0];
	
	//{
	}// Callseq End 1
	ld.local.u32 	%r38, [%r2];

BB48_10:
	and.b32  	%r25, %r38, 3;
	cvt.rn.f64.s32	%fd183, %r25;
	add.f64 	%fd184, %fd211, 0dBFE921FB54442D18;
	fma.rn.f64 	%fd212, %fd183, 0d3FF921FB54442D18, %fd184;
	abs.f64 	%fd185, %fd212;
	setp.neu.f64	%p7, %fd185, 0d7FF0000000000000;
	@%p7 bra 	BB48_12;

	mov.f64 	%fd186, 0d0000000000000000;
	mul.rn.f64 	%fd212, %fd212, %fd186;

BB48_12:
	mov.f64 	%fd210, 0d3FF921FB54442D18;
	mul.f64 	%fd187, %fd212, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r39, %fd187;
	st.local.u32 	[%r1], %r39;
	cvt.rn.f64.s32	%fd188, %r39;
	neg.f64 	%fd189, %fd188;
	fma.rn.f64 	%fd191, %fd189, %fd210, %fd212;
	fma.rn.f64 	%fd193, %fd189, %fd180, %fd191;
	fma.rn.f64 	%fd213, %fd189, %fd182, %fd193;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r26}, %fd212;
	}
	and.b32  	%r27, %r26, 2145386496;
	setp.lt.u32	%p8, %r27, 1105199104;
	@%p8 bra 	BB48_14;

	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd212;
	.param .b32 param1;
	st.param.b32	[param1+0], %r14;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd213, [retval0+0];
	
	//{
	}// Callseq End 2
	ld.local.u32 	%r39, [%r1];

BB48_14:
	add.s32 	%r10, %r39, 1;
	and.b32  	%r29, %r10, 1;
	setp.eq.s32	%p9, %r29, 0;
	selp.f64	%fd195, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p9;
	shl.b32 	%r30, %r29, 6;
	mov.u32 	%r31, __cudart_sin_cos_coeffs;
	add.s32 	%r32, %r30, %r31;
	ld.const.f64 	%fd196, [%r32+8];
	mul.rn.f64 	%fd16, %fd213, %fd213;
	fma.rn.f64 	%fd197, %fd195, %fd16, %fd196;
	ld.const.f64 	%fd198, [%r32+16];
	fma.rn.f64 	%fd199, %fd197, %fd16, %fd198;
	ld.const.f64 	%fd200, [%r32+24];
	fma.rn.f64 	%fd201, %fd199, %fd16, %fd200;
	ld.const.f64 	%fd202, [%r32+32];
	fma.rn.f64 	%fd203, %fd201, %fd16, %fd202;
	ld.const.f64 	%fd204, [%r32+40];
	fma.rn.f64 	%fd205, %fd203, %fd16, %fd204;
	ld.const.f64 	%fd206, [%r32+48];
	fma.rn.f64 	%fd17, %fd205, %fd16, %fd206;
	fma.rn.f64 	%fd214, %fd17, %fd213, %fd213;
	@%p9 bra 	BB48_16;

	fma.rn.f64 	%fd214, %fd17, %fd16, %fd136;

BB48_16:
	and.b32  	%r33, %r10, 2;
	setp.eq.s32	%p10, %r33, 0;
	@%p10 bra 	BB48_18;

	mov.f64 	%fd208, 0d0000000000000000;
	mov.f64 	%fd209, 0dBFF0000000000000;
	fma.rn.f64 	%fd214, %fd214, %fd209, %fd208;

BB48_18:
	mul.f64 	%fd215, %fd6, %fd214;
	bra.uni 	BB48_19;

BB48_2:
	add.f64 	%fd26, %fd1, 0dC0033D152E971B40;
	add.f64 	%fd27, %fd26, 0d3CA0F539D7DA258E;
	mov.f64 	%fd28, 0dBCFCF8F9A8C294BC;
	mov.f64 	%fd29, 0dBCC0D18564C48C61;
	fma.rn.f64 	%fd30, %fd29, %fd27, %fd28;
	mov.f64 	%fd31, 0d3D3FAB983CAE498B;
	fma.rn.f64 	%fd32, %fd30, %fd27, %fd31;
	mov.f64 	%fd33, 0d3D7CD7C018579B88;
	fma.rn.f64 	%fd34, %fd32, %fd27, %fd33;
	mov.f64 	%fd35, 0dBDBBDD2342D64FDD;
	fma.rn.f64 	%fd36, %fd34, %fd27, %fd35;
	mov.f64 	%fd37, 0dBDF5C2D9416B1E2B;
	fma.rn.f64 	%fd38, %fd36, %fd27, %fd37;
	mov.f64 	%fd39, 0d3E32951D73174DD5;
	fma.rn.f64 	%fd40, %fd38, %fd27, %fd39;
	mov.f64 	%fd41, 0d3E67FF99802CAEB5;
	fma.rn.f64 	%fd42, %fd40, %fd27, %fd41;
	mov.f64 	%fd43, 0dBEA1CCE305C4C9F7;
	fma.rn.f64 	%fd44, %fd42, %fd27, %fd43;
	mov.f64 	%fd45, 0dBED232C77E29E1BB;
	fma.rn.f64 	%fd46, %fd44, %fd27, %fd45;
	mov.f64 	%fd47, 0d3F06ED3B9F0EF757;
	fma.rn.f64 	%fd48, %fd46, %fd27, %fd47;
	mov.f64 	%fd49, 0d3F315382BA096A62;
	fma.rn.f64 	%fd50, %fd48, %fd27, %fd49;
	mov.f64 	%fd51, 0dBF61F992590D1AE4;
	fma.rn.f64 	%fd52, %fd50, %fd27, %fd51;
	mov.f64 	%fd53, 0dBF81BB1CBE1A465F;
	fma.rn.f64 	%fd54, %fd52, %fd27, %fd53;
	mov.f64 	%fd55, 0d3FACFAE864368D84;
	fma.rn.f64 	%fd56, %fd54, %fd27, %fd55;
	mov.f64 	%fd57, 0d3FBBA1DEEA0294A3;
	fma.rn.f64 	%fd58, %fd56, %fd27, %fd57;
	mov.f64 	%fd59, 0dBFE09CDB36551280;
	fma.rn.f64 	%fd60, %fd58, %fd27, %fd59;
	mul.f64 	%fd215, %fd27, %fd60;
	bra.uni 	BB48_19;

BB48_4:
	add.f64 	%fd61, %fd1, 0dC016148F5B2C2E45;
	add.f64 	%fd62, %fd61, 0dBC975054CD60A517;
	mov.f64 	%fd63, 0d3CF83FD1F333EB61;
	mov.f64 	%fd64, 0d3CBCB0A8F126B343;
	fma.rn.f64 	%fd65, %fd64, %fd62, %fd63;
	mov.f64 	%fd66, 0dBD4100E33E3FB413;
	fma.rn.f64 	%fd67, %fd65, %fd62, %fd66;
	mov.f64 	%fd68, 0dBD7846076D004627;
	fma.rn.f64 	%fd69, %fd67, %fd62, %fd68;
	mov.f64 	%fd70, 0d3DBE2F1D4F90720D;
	fma.rn.f64 	%fd71, %fd69, %fd62, %fd70;
	mov.f64 	%fd72, 0d3DF1D03B1E4A119B;
	fma.rn.f64 	%fd73, %fd71, %fd62, %fd72;
	mov.f64 	%fd74, 0dBE341D72B1B3BCE9;
	fma.rn.f64 	%fd75, %fd73, %fd62, %fd74;
	mov.f64 	%fd76, 0dBE62DA37CE2A9EF8;
	fma.rn.f64 	%fd77, %fd75, %fd62, %fd76;
	mov.f64 	%fd78, 0d3EA32E6D9974F763;
	fma.rn.f64 	%fd79, %fd77, %fd62, %fd78;
	mov.f64 	%fd80, 0d3ECAD77D744A1879;
	fma.rn.f64 	%fd81, %fd79, %fd62, %fd80;
	mov.f64 	%fd82, 0dBF0863F481A37337;
	fma.rn.f64 	%fd83, %fd81, %fd62, %fd82;
	mov.f64 	%fd84, 0dBF26F641F418F0F4;
	fma.rn.f64 	%fd85, %fd83, %fd62, %fd84;
	mov.f64 	%fd86, 0d3F627E31FE9A969E;
	fma.rn.f64 	%fd87, %fd85, %fd62, %fd86;
	mov.f64 	%fd88, 0d3F72F7FFE9025628;
	fma.rn.f64 	%fd89, %fd87, %fd62, %fd88;
	mov.f64 	%fd90, 0dBFAB2150CB41E8BF;
	fma.rn.f64 	%fd91, %fd89, %fd62, %fd90;
	mov.f64 	%fd92, 0dBF9F8F72E7A848DE;
	fma.rn.f64 	%fd93, %fd91, %fd62, %fd92;
	mov.f64 	%fd94, 0d3FD5C6E60A097823;
	fma.rn.f64 	%fd95, %fd93, %fd62, %fd94;
	mul.f64 	%fd215, %fd62, %fd95;
	bra.uni 	BB48_19;

BB48_6:
	add.f64 	%fd96, %fd1, 0dC0214EB56CCCDECA;
	add.f64 	%fd97, %fd96, 0d3CB51970714C7C25;
	mov.f64 	%fd98, 0dBCF4B3A71AAAC629;
	mov.f64 	%fd99, 0dBCBDB7FFCF659E24;
	fma.rn.f64 	%fd100, %fd99, %fd97, %fd98;
	mov.f64 	%fd101, 0d3D417EC150ECDCE7;
	fma.rn.f64 	%fd102, %fd100, %fd97, %fd101;
	mov.f64 	%fd103, 0d3D7438F5EA1D10B2;
	fma.rn.f64 	%fd104, %fd102, %fd97, %fd103;
	mov.f64 	%fd105, 0dBDBEDAE7EC2C9E87;
	fma.rn.f64 	%fd106, %fd104, %fd97, %fd105;
	mov.f64 	%fd107, 0dBDECADD2C4B91F58;
	fma.rn.f64 	%fd108, %fd106, %fd97, %fd107;
	mov.f64 	%fd109, 0d3E34582C8EE12204;
	fma.rn.f64 	%fd110, %fd108, %fd97, %fd109;
	mov.f64 	%fd111, 0d3E5CEDA451DD20F8;
	fma.rn.f64 	%fd112, %fd110, %fd97, %fd111;
	mov.f64 	%fd113, 0dBEA30E8CC3165E2F;
	fma.rn.f64 	%fd114, %fd112, %fd97, %fd113;
	mov.f64 	%fd115, 0dBEC3324842BB1A2E;
	fma.rn.f64 	%fd116, %fd114, %fd97, %fd115;
	mov.f64 	%fd117, 0d3F07800BC54FBDDB;
	fma.rn.f64 	%fd118, %fd116, %fd97, %fd117;
	mov.f64 	%fd119, 0d3F1D79605276949A;
	fma.rn.f64 	%fd120, %fd118, %fd97, %fd119;
	mov.f64 	%fd121, 0dBF60E0D60385A629;
	fma.rn.f64 	%fd122, %fd120, %fd97, %fd121;
	mov.f64 	%fd123, 0dBF648E63600D82F3;
	fma.rn.f64 	%fd124, %fd122, %fd97, %fd123;
	mov.f64 	%fd125, 0d3FA68B984EC6493A;
	fma.rn.f64 	%fd126, %fd124, %fd97, %fd125;
	mov.f64 	%fd127, 0d3F900F7FCF183E0B;
	fma.rn.f64 	%fd128, %fd126, %fd97, %fd127;
	mov.f64 	%fd129, 0dBFD15F7977A772D4;
	fma.rn.f64 	%fd130, %fd128, %fd97, %fd129;
	mul.f64 	%fd215, %fd97, %fd130;

BB48_19:
	cvta.to.global.u32 	%r34, %r11;
	add.s32 	%r36, %r34, %r20;
	st.global.f64 	[%r36], %fd215;

BB48_20:
	ret;
}

	// .globl	vec_j1
.visible .entry vec_j1(
	.param .u32 vec_j1_param_0,
	.param .u32 vec_j1_param_1,
	.param .u32 vec_j1_param_2
)
{
	.local .align 4 .b8 	__local_depot49[8];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<13>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<217>;


	mov.u32 	%r41, __local_depot49;
	cvta.local.u32 	%SP, %r41;
	ld.param.u32 	%r12, [vec_j1_param_0];
	ld.param.u32 	%r10, [vec_j1_param_1];
	ld.param.u32 	%r11, [vec_j1_param_2];
	add.u32 	%r13, %SP, 0;
	cvta.to.local.u32 	%r1, %r13;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %tid.x;
	mad.lo.s32 	%r2, %r14, %r15, %r16;
	setp.ge.u32	%p1, %r2, %r12;
	@%p1 bra 	BB49_20;

	cvta.to.global.u32 	%r17, %r11;
	shl.b32 	%r18, %r2, 3;
	add.s32 	%r19, %r17, %r18;
	ld.global.f64 	%fd1, [%r19];
	abs.f64 	%fd2, %fd1;
	setp.gtu.f64	%p2, %fd2, 0d400353AABAD7B784;
	@%p2 bra 	BB49_3;
	bra.uni 	BB49_2;

BB49_3:
	setp.gtu.f64	%p3, %fd2, 0d4015B1D0574614EA;
	@%p3 bra 	BB49_5;
	bra.uni 	BB49_4;

BB49_5:
	setp.gtu.f64	%p4, %fd2, 0d40213065E54C1AA9;
	@%p4 bra 	BB49_7;
	bra.uni 	BB49_6;

BB49_7:
	abs.f64 	%fd124, %fd2;
	mov.f64 	%fd216, 0d0000000000000000;
	setp.eq.f64	%p5, %fd124, 0d7FF0000000000000;
	@%p5 bra 	BB49_19;

	add.u32 	%r20, %SP, 4;
	cvta.to.local.u32 	%r21, %r20;
	// inline asm
	rcp.approx.ftz.f64 %fd125,%fd2;
	// inline asm
	neg.f64 	%fd127, %fd2;
	mov.f64 	%fd128, 0d3FF0000000000000;
	fma.rn.f64 	%fd129, %fd127, %fd125, %fd128;
	fma.rn.f64 	%fd130, %fd129, %fd129, %fd129;
	fma.rn.f64 	%fd131, %fd130, %fd125, %fd125;
	mul.f64 	%fd132, %fd131, %fd131;
	mov.f64 	%fd133, 0dC099C06322A3F8BE;
	mov.f64 	%fd134, 0d40CD02EA3F2F6751;
	fma.rn.f64 	%fd135, %fd134, %fd132, %fd133;
	mov.f64 	%fd136, 0d405B89354DA77324;
	fma.rn.f64 	%fd137, %fd135, %fd132, %fd136;
	mov.f64 	%fd138, 0dC01E352294653188;
	fma.rn.f64 	%fd139, %fd137, %fd132, %fd138;
	mov.f64 	%fd140, 0d3FE9BC7DB16BD7A7;
	fma.rn.f64 	%fd141, %fd139, %fd132, %fd140;
	mov.f64 	%fd142, 0dBFC8BFE1C3A4F741;
	fma.rn.f64 	%fd143, %fd141, %fd132, %fd142;
	mov.f64 	%fd144, 0d3FC7FFFFF0D00BE2;
	fma.rn.f64 	%fd145, %fd143, %fd132, %fd144;
	mov.f64 	%fd146, 0d3FF00000000068CC;
	fma.rn.f64 	%fd147, %fd145, %fd132, %fd146;
	mov.f64 	%fd148, 0d415A30AC6857BEE0;
	mov.f64 	%fd149, 0dC18DA26B212FDC9A;
	fma.rn.f64 	%fd150, %fd149, %fd132, %fd148;
	mov.f64 	%fd151, 0dC11764222AD7C910;
	fma.rn.f64 	%fd152, %fd150, %fd132, %fd151;
	mov.f64 	%fd153, 0d40CEB02E0C306857;
	fma.rn.f64 	%fd154, %fd152, %fd132, %fd153;
	mov.f64 	%fd155, 0dC08351859FA2B23B;
	fma.rn.f64 	%fd156, %fd154, %fd132, %fd155;
	mov.f64 	%fd157, 0d403E65A07AF51F42;
	fma.rn.f64 	%fd158, %fd156, %fd132, %fd157;
	mov.f64 	%fd159, 0dC002F2B817F77A57;
	fma.rn.f64 	%fd160, %fd158, %fd132, %fd159;
	mov.f64 	%fd161, 0d3FD7BCC34DA069FD;
	fma.rn.f64 	%fd162, %fd160, %fd132, %fd161;
	mov.f64 	%fd163, 0dBFC4FFFFF8A44463;
	fma.rn.f64 	%fd164, %fd162, %fd132, %fd163;
	mov.f64 	%fd165, 0d3FD7FFFFFFFF5CD7;
	fma.rn.f64 	%fd166, %fd164, %fd132, %fd165;
	fma.rn.f64 	%fd6, %fd166, %fd131, %fd2;
	rsqrt.approx.f64 	%fd167, %fd2;
	mul.f64 	%fd168, %fd167, 0d3FE9884533D43651;
	mul.f64 	%fd7, %fd147, %fd168;
	mul.f64 	%fd169, %fd6, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r39, %fd169;
	st.local.u32 	[%r21], %r39;
	cvt.rn.f64.s32	%fd170, %r39;
	neg.f64 	%fd171, %fd170;
	mov.f64 	%fd172, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd173, %fd171, %fd172, %fd6;
	mov.f64 	%fd174, 0d3C91A62633145C00;
	fma.rn.f64 	%fd175, %fd171, %fd174, %fd173;
	mov.f64 	%fd176, 0d397B839A252049C0;
	fma.rn.f64 	%fd212, %fd171, %fd176, %fd175;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd6;
	}
	and.b32  	%r23, %r22, 2145386496;
	setp.lt.u32	%p6, %r23, 1105199104;
	@%p6 bra 	BB49_10;

	// Callseq Start 3
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd6;
	.param .b32 param1;
	st.param.b32	[param1+0], %r20;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd212, [retval0+0];
	
	//{
	}// Callseq End 3
	ld.local.u32 	%r39, [%r21];

BB49_10:
	and.b32  	%r26, %r39, 3;
	cvt.rn.f64.s32	%fd177, %r26;
	add.f64 	%fd178, %fd212, 0dC002D97C7F3321D2;
	fma.rn.f64 	%fd213, %fd177, 0d3FF921FB54442D18, %fd178;
	abs.f64 	%fd179, %fd213;
	setp.neu.f64	%p7, %fd179, 0d7FF0000000000000;
	@%p7 bra 	BB49_12;

	mov.f64 	%fd180, 0d0000000000000000;
	mul.rn.f64 	%fd213, %fd213, %fd180;

BB49_12:
	mov.f64 	%fd210, 0d397B839A252049C0;
	mov.f64 	%fd209, 0d3C91A62633145C00;
	mov.f64 	%fd208, 0d3FF921FB54442D18;
	mul.f64 	%fd181, %fd213, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r40, %fd181;
	st.local.u32 	[%r1], %r40;
	cvt.rn.f64.s32	%fd182, %r40;
	neg.f64 	%fd183, %fd182;
	fma.rn.f64 	%fd185, %fd183, %fd208, %fd213;
	fma.rn.f64 	%fd187, %fd183, %fd209, %fd185;
	fma.rn.f64 	%fd214, %fd183, %fd210, %fd187;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r27}, %fd213;
	}
	and.b32  	%r28, %r27, 2145386496;
	setp.lt.u32	%p8, %r28, 1105199104;
	@%p8 bra 	BB49_14;

	add.u32 	%r38, %SP, 0;
	// Callseq Start 4
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd213;
	.param .b32 param1;
	st.param.b32	[param1+0], %r38;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd214, [retval0+0];
	
	//{
	}// Callseq End 4
	ld.local.u32 	%r40, [%r1];

BB49_14:
	add.s32 	%r9, %r40, 1;
	and.b32  	%r30, %r9, 1;
	setp.eq.s32	%p9, %r30, 0;
	selp.f64	%fd189, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p9;
	shl.b32 	%r31, %r30, 6;
	mov.u32 	%r32, __cudart_sin_cos_coeffs;
	add.s32 	%r33, %r31, %r32;
	ld.const.f64 	%fd190, [%r33+8];
	mul.rn.f64 	%fd17, %fd214, %fd214;
	fma.rn.f64 	%fd191, %fd189, %fd17, %fd190;
	ld.const.f64 	%fd192, [%r33+16];
	fma.rn.f64 	%fd193, %fd191, %fd17, %fd192;
	ld.const.f64 	%fd194, [%r33+24];
	fma.rn.f64 	%fd195, %fd193, %fd17, %fd194;
	ld.const.f64 	%fd196, [%r33+32];
	fma.rn.f64 	%fd197, %fd195, %fd17, %fd196;
	ld.const.f64 	%fd198, [%r33+40];
	fma.rn.f64 	%fd199, %fd197, %fd17, %fd198;
	ld.const.f64 	%fd200, [%r33+48];
	fma.rn.f64 	%fd18, %fd199, %fd17, %fd200;
	fma.rn.f64 	%fd215, %fd18, %fd214, %fd214;
	@%p9 bra 	BB49_16;

	mov.f64 	%fd211, 0d3FF0000000000000;
	fma.rn.f64 	%fd215, %fd18, %fd17, %fd211;

BB49_16:
	and.b32  	%r34, %r9, 2;
	setp.eq.s32	%p10, %r34, 0;
	@%p10 bra 	BB49_18;

	mov.f64 	%fd202, 0d0000000000000000;
	mov.f64 	%fd203, 0dBFF0000000000000;
	fma.rn.f64 	%fd215, %fd215, %fd203, %fd202;

BB49_18:
	mul.f64 	%fd216, %fd7, %fd215;
	bra.uni 	BB49_19;

BB49_2:
	mov.f64 	%fd26, 0dBD4DD167A0DC3F55;
	mov.f64 	%fd27, 0d3D020E4ADCDE2AD3;
	fma.rn.f64 	%fd28, %fd27, %fd2, %fd26;
	mov.f64 	%fd29, 0d3D5503F5A491E487;
	fma.rn.f64 	%fd30, %fd28, %fd2, %fd29;
	mov.f64 	%fd31, 0d3DC1F29940C2403A;
	fma.rn.f64 	%fd32, %fd30, %fd2, %fd31;
	mov.f64 	%fd33, 0d3D84CF9302EACDEF;
	fma.rn.f64 	%fd34, %fd32, %fd2, %fd33;
	mov.f64 	%fd35, 0dBE384A53DBBCA436;
	fma.rn.f64 	%fd36, %fd34, %fd2, %fd35;
	mov.f64 	%fd37, 0d3D9779BEE4F63BCC;
	fma.rn.f64 	%fd38, %fd36, %fd2, %fd37;
	mov.f64 	%fd39, 0d3EA6C160E414F3F0;
	fma.rn.f64 	%fd40, %fd38, %fd2, %fd39;
	mov.f64 	%fd41, 0d3D8F3D2F12430699;
	fma.rn.f64 	%fd42, %fd40, %fd2, %fd41;
	mov.f64 	%fd43, 0dBF0C71C72C0CED04;
	fma.rn.f64 	%fd44, %fd42, %fd2, %fd43;
	mov.f64 	%fd45, 0d3D659BCA506F1128;
	fma.rn.f64 	%fd46, %fd44, %fd2, %fd45;
	mov.f64 	%fd47, 0d3F65555555506982;
	fma.rn.f64 	%fd48, %fd46, %fd2, %fd47;
	mov.f64 	%fd49, 0d3D15BA0B425F1BFB;
	fma.rn.f64 	%fd50, %fd48, %fd2, %fd49;
	mov.f64 	%fd51, 0dBFB0000000000065;
	fma.rn.f64 	%fd52, %fd50, %fd2, %fd51;
	mov.f64 	%fd53, 0d3C8729A7253FB679;
	fma.rn.f64 	%fd54, %fd52, %fd2, %fd53;
	mov.f64 	%fd55, 0d3FE0000000000000;
	fma.rn.f64 	%fd56, %fd54, %fd2, %fd55;
	mul.f64 	%fd216, %fd2, %fd56;
	bra.uni 	BB49_19;

BB49_4:
	add.f64 	%fd57, %fd2, 0dC00EA75575AF6F09;
	add.f64 	%fd58, %fd57, 0d3CA60155A9D1B256;
	mov.f64 	%fd59, 0d3D41011A1DF02DAD;
	mov.f64 	%fd60, 0dBCF8D3CDBB60175E;
	fma.rn.f64 	%fd61, %fd60, %fd58, %fd59;
	mov.f64 	%fd62, 0d3D76013AC1E5E222;
	fma.rn.f64 	%fd63, %fd61, %fd58, %fd62;
	mov.f64 	%fd64, 0dBDBEC315D96D5F03;
	fma.rn.f64 	%fd65, %fd63, %fd58, %fd64;
	mov.f64 	%fd66, 0dBDF03BE1B4B57207;
	fma.rn.f64 	%fd67, %fd65, %fd58, %fd66;
	mov.f64 	%fd68, 0d3E345695F8B660F7;
	fma.rn.f64 	%fd69, %fd67, %fd58, %fd68;
	mov.f64 	%fd70, 0d3E617069FCFCFFF4;
	fma.rn.f64 	%fd71, %fd69, %fd58, %fd70;
	mov.f64 	%fd72, 0dBEA33825C36745EB;
	fma.rn.f64 	%fd73, %fd71, %fd58, %fd72;
	mov.f64 	%fd74, 0dBEC9799D4F90931B;
	fma.rn.f64 	%fd75, %fd73, %fd58, %fd74;
	mov.f64 	%fd76, 0d3F083A06E2F7DF13;
	fma.rn.f64 	%fd77, %fd75, %fd58, %fd76;
	mov.f64 	%fd78, 0d3F26E4C2D53A7CF6;
	fma.rn.f64 	%fd79, %fd77, %fd58, %fd78;
	mov.f64 	%fd80, 0dBF624B3409957B1C;
	fma.rn.f64 	%fd81, %fd79, %fd58, %fd80;
	mov.f64 	%fd82, 0dBF7537544C3325DF;
	fma.rn.f64 	%fd83, %fd81, %fd58, %fd82;
	mov.f64 	%fd84, 0d3FAB589D1DA138E2;
	fma.rn.f64 	%fd85, %fd83, %fd58, %fd84;
	mov.f64 	%fd86, 0d3FAAE8A39F51AD13;
	fma.rn.f64 	%fd87, %fd85, %fd58, %fd86;
	mov.f64 	%fd88, 0dBFD9C6CF582CBF7F;
	fma.rn.f64 	%fd89, %fd87, %fd58, %fd88;
	mul.f64 	%fd216, %fd58, %fd89;
	bra.uni 	BB49_19;

BB49_6:
	add.f64 	%fd90, %fd2, 0dC01C0FF5F3B47250;
	add.f64 	%fd91, %fd90, 0d3C9B226D9D243827;
	mov.f64 	%fd92, 0dBD40E8363DB649A9;
	mov.f64 	%fd93, 0d3CF3EB867515FAD6;
	fma.rn.f64 	%fd94, %fd93, %fd91, %fd92;
	mov.f64 	%fd95, 0dBD73B7DD4A6608FB;
	fma.rn.f64 	%fd96, %fd94, %fd91, %fd95;
	mov.f64 	%fd97, 0d3DBEC5E01482C750;
	fma.rn.f64 	%fd98, %fd96, %fd91, %fd97;
	mov.f64 	%fd99, 0d3DEC62BB9E882103;
	fma.rn.f64 	%fd100, %fd98, %fd91, %fd99;
	mov.f64 	%fd101, 0dBE34462EED732A23;
	fma.rn.f64 	%fd102, %fd100, %fd91, %fd101;
	mov.f64 	%fd103, 0dBE5D48DCAD7DC59B;
	fma.rn.f64 	%fd104, %fd102, %fd91, %fd103;
	mov.f64 	%fd105, 0d3EA3026DF29167E9;
	fma.rn.f64 	%fd106, %fd104, %fd91, %fd105;
	mov.f64 	%fd107, 0d3EC4255B0119666C;
	fma.rn.f64 	%fd108, %fd106, %fd91, %fd107;
	mov.f64 	%fd109, 0dBF0796A751B32693;
	fma.rn.f64 	%fd110, %fd108, %fd91, %fd109;
	mov.f64 	%fd111, 0dBF207358BBDBA284;
	fma.rn.f64 	%fd112, %fd110, %fd91, %fd111;
	mov.f64 	%fd113, 0d3F613FBC7D6927B1;
	fma.rn.f64 	%fd114, %fd112, %fd91, %fd113;
	mov.f64 	%fd115, 0d3F69A4B292E3DD75;
	fma.rn.f64 	%fd116, %fd114, %fd91, %fd115;
	mov.f64 	%fd117, 0dBFA80C83BDEEE4FB;
	fma.rn.f64 	%fd118, %fd116, %fd91, %fd117;
	mov.f64 	%fd119, 0dBF95E70DC60362BF;
	fma.rn.f64 	%fd120, %fd118, %fd91, %fd119;
	mov.f64 	%fd121, 0d3FD33518B3874E8A;
	fma.rn.f64 	%fd122, %fd120, %fd91, %fd121;
	mul.f64 	%fd216, %fd91, %fd122;

BB49_19:
	cvta.to.global.u32 	%r35, %r10;
	neg.f64 	%fd204, %fd216;
	setp.lt.f64	%p11, %fd1, 0d0000000000000000;
	selp.f64	%fd205, %fd204, %fd216, %p11;
	mul.f64 	%fd206, %fd1, 0d3FE0000000000000;
	setp.lt.f64	%p12, %fd2, 0d39B4484BFEEBC2A0;
	selp.f64	%fd207, %fd206, %fd205, %p12;
	add.s32 	%r37, %r35, %r18;
	st.global.f64 	[%r37], %fd207;

BB49_20:
	ret;
}

	// .globl	vec_lgamma
.visible .entry vec_lgamma(
	.param .u32 vec_lgamma_param_0,
	.param .u32 vec_lgamma_param_1,
	.param .u32 vec_lgamma_param_2
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<49>;
	.reg .f64 	%fd<105>;
	.reg .b64 	%rd<2>;


	ld.param.u32 	%r16, [vec_lgamma_param_0];
	ld.param.u32 	%r14, [vec_lgamma_param_1];
	ld.param.u32 	%r15, [vec_lgamma_param_2];
	mov.u32 	%r17, %ntid.x;
	mov.u32 	%r18, %ctaid.x;
	mov.u32 	%r19, %tid.x;
	mad.lo.s32 	%r1, %r17, %r18, %r19;
	setp.ge.u32	%p1, %r1, %r16;
	@%p1 bra 	BB50_23;

	cvta.to.global.u32 	%r20, %r15;
	shl.b32 	%r21, %r1, 3;
	add.s32 	%r22, %r20, %r21;
	ld.global.f64 	%fd1, [%r22];
	abs.f64 	%fd101, %fd1;
	setp.gtu.f64	%p2, %fd101, 0d7FF0000000000000;
	@%p2 bra 	BB50_21;
	bra.uni 	BB50_2;

BB50_21:
	add.f64 	%fd104, %fd1, %fd1;
	bra.uni 	BB50_22;

BB50_2:
	// Callseq Start 5
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd101;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_lgamma_pos, 
	(
	param0
	);
	ld.param.f64	%fd3, [retval0+0];
	
	//{
	}// Callseq End 5
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd1;
	}
	setp.gt.s32	%p3, %r23, -1;
	mov.f64 	%fd104, %fd3;
	@%p3 bra 	BB50_22;

	cvt.rzi.f64.f64	%fd24, %fd101;
	setp.eq.f64	%p4, %fd101, %fd24;
	mov.f64 	%fd23, 0d7FF0000000000000;
	mov.f64 	%fd104, %fd23;
	@%p4 bra 	BB50_22;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd101;
	}
	setp.lt.s32	%p5, %r2, 1006632960;
	@%p5 bra 	BB50_10;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r24, %temp}, %fd101;
	}
	add.s32 	%r25, %r2, 1048576;
	mov.b64 	%fd25, {%r24, %r25};
	cvt.rni.f64.f64	%fd26, %fd25;
	cvt.rzi.s64.f64	%rd1, %fd26;
	cvt.u32.u64	%r3, %rd1;
	neg.f64 	%fd27, %fd26;
	mov.f64 	%fd28, 0d3FE0000000000000;
	fma.rn.f64 	%fd29, %fd27, %fd28, %fd101;
	mul.f64 	%fd30, %fd29, 0d3CA1A62633145C07;
	mov.f64 	%fd31, 0d400921FB54442D18;
	fma.rn.f64 	%fd32, %fd29, %fd31, %fd30;
	and.b32  	%r26, %r3, 1;
	mul.rn.f64 	%fd4, %fd32, %fd32;
	setp.eq.s32	%p6, %r26, 0;
	selp.f64	%fd33, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p6;
	shl.b32 	%r27, %r26, 6;
	mov.u32 	%r28, __cudart_sin_cos_coeffs;
	add.s32 	%r29, %r27, %r28;
	ld.const.f64 	%fd34, [%r29+8];
	fma.rn.f64 	%fd35, %fd33, %fd4, %fd34;
	ld.const.f64 	%fd36, [%r29+16];
	fma.rn.f64 	%fd37, %fd35, %fd4, %fd36;
	ld.const.f64 	%fd38, [%r29+24];
	fma.rn.f64 	%fd39, %fd37, %fd4, %fd38;
	ld.const.f64 	%fd40, [%r29+32];
	fma.rn.f64 	%fd41, %fd39, %fd4, %fd40;
	ld.const.f64 	%fd42, [%r29+40];
	fma.rn.f64 	%fd43, %fd41, %fd4, %fd42;
	ld.const.f64 	%fd44, [%r29+48];
	fma.rn.f64 	%fd5, %fd43, %fd4, %fd44;
	fma.rn.f64 	%fd100, %fd5, %fd32, %fd32;
	@%p6 bra 	BB50_7;

	mov.f64 	%fd45, 0d3FF0000000000000;
	fma.rn.f64 	%fd100, %fd5, %fd4, %fd45;

BB50_7:
	and.b32  	%r30, %r3, 2;
	setp.eq.s32	%p7, %r30, 0;
	@%p7 bra 	BB50_9;

	mov.f64 	%fd46, 0d0000000000000000;
	mov.f64 	%fd47, 0dBFF0000000000000;
	fma.rn.f64 	%fd100, %fd100, %fd47, %fd46;

BB50_9:
	abs.f64 	%fd48, %fd100;
	mul.f64 	%fd49, %fd101, %fd48;
	div.rn.f64 	%fd101, %fd31, %fd49;

BB50_10:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r45}, %fd101;
	}
	setp.lt.s32	%p8, %r45, 2146435072;
	setp.gt.f64	%p9, %fd101, 0d0000000000000000;
	and.pred  	%p10, %p9, %p8;
	@%p10 bra 	BB50_15;
	bra.uni 	BB50_11;

BB50_15:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r46, %temp}, %fd101;
	}
	mov.u32 	%r47, -1023;
	setp.gt.s32	%p14, %r45, 1048575;
	@%p14 bra 	BB50_17;

	mul.f64 	%fd53, %fd101, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r45}, %fd53;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r46, %temp}, %fd53;
	}
	mov.u32 	%r47, -1077;

BB50_17:
	shr.u32 	%r33, %r45, 20;
	add.s32 	%r48, %r47, %r33;
	and.b32  	%r34, %r45, -2146435073;
	or.b32  	%r35, %r34, 1072693248;
	mov.b64 	%fd102, {%r46, %r35};
	setp.lt.s32	%p15, %r35, 1073127583;
	@%p15 bra 	BB50_19;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd102;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r37}, %fd102;
	}
	add.s32 	%r38, %r37, -1048576;
	mov.b64 	%fd102, {%r36, %r38};
	add.s32 	%r48, %r48, 1;

BB50_19:
	add.f64 	%fd55, %fd102, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd54,%fd55;
	// inline asm
	neg.f64 	%fd56, %fd55;
	mov.f64 	%fd57, 0d3FF0000000000000;
	fma.rn.f64 	%fd58, %fd56, %fd54, %fd57;
	fma.rn.f64 	%fd59, %fd58, %fd58, %fd58;
	fma.rn.f64 	%fd60, %fd59, %fd54, %fd54;
	add.f64 	%fd61, %fd102, 0dBFF0000000000000;
	mul.f64 	%fd62, %fd61, %fd60;
	fma.rn.f64 	%fd63, %fd61, %fd60, %fd62;
	mul.f64 	%fd64, %fd63, %fd63;
	mov.f64 	%fd65, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd66, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd67, %fd66, %fd64, %fd65;
	mov.f64 	%fd68, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd69, %fd67, %fd64, %fd68;
	mov.f64 	%fd70, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd71, %fd69, %fd64, %fd70;
	mov.f64 	%fd72, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd73, %fd71, %fd64, %fd72;
	mov.f64 	%fd74, 0d3F624924923BE72D;
	fma.rn.f64 	%fd75, %fd73, %fd64, %fd74;
	mov.f64 	%fd76, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd77, %fd75, %fd64, %fd76;
	mov.f64 	%fd78, 0d3FB5555555555554;
	fma.rn.f64 	%fd79, %fd77, %fd64, %fd78;
	sub.f64 	%fd80, %fd61, %fd63;
	add.f64 	%fd81, %fd80, %fd80;
	neg.f64 	%fd82, %fd63;
	fma.rn.f64 	%fd83, %fd82, %fd61, %fd81;
	mul.f64 	%fd84, %fd60, %fd83;
	mul.f64 	%fd85, %fd64, %fd79;
	fma.rn.f64 	%fd86, %fd85, %fd63, %fd84;
	xor.b32  	%r39, %r48, -2147483648;
	mov.u32 	%r40, 1127219200;
	mov.b64 	%fd87, {%r39, %r40};
	mov.u32 	%r41, -2147483648;
	mov.b64 	%fd88, {%r41, %r40};
	sub.f64 	%fd89, %fd87, %fd88;
	mov.f64 	%fd90, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd91, %fd89, %fd90, %fd63;
	neg.f64 	%fd92, %fd89;
	fma.rn.f64 	%fd93, %fd92, %fd90, %fd91;
	sub.f64 	%fd94, %fd93, %fd63;
	sub.f64 	%fd95, %fd86, %fd94;
	mov.f64 	%fd96, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd97, %fd89, %fd96, %fd95;
	add.f64 	%fd103, %fd91, %fd97;
	bra.uni 	BB50_20;

BB50_11:
	abs.f64 	%fd51, %fd101;
	setp.gtu.f64	%p11, %fd51, 0d7FF0000000000000;
	@%p11 bra 	BB50_14;
	bra.uni 	BB50_12;

BB50_14:
	add.f64 	%fd103, %fd101, %fd101;
	bra.uni 	BB50_20;

BB50_12:
	setp.eq.f64	%p12, %fd101, 0d0000000000000000;
	mov.f64 	%fd103, 0dFFF0000000000000;
	@%p12 bra 	BB50_20;

	setp.eq.f64	%p13, %fd101, 0d7FF0000000000000;
	selp.f64	%fd103, %fd101, 0dFFF8000000000000, %p13;

BB50_20:
	sub.f64 	%fd98, %fd103, %fd3;
	neg.f64 	%fd99, %fd103;
	selp.f64	%fd104, %fd99, %fd98, %p5;

BB50_22:
	cvta.to.global.u32 	%r42, %r14;
	add.s32 	%r44, %r42, %r21;
	st.global.f64 	[%r44], %fd104;

BB50_23:
	ret;
}

	// .globl	vec_log10
.visible .entry vec_log10(
	.param .u32 vec_log10_param_0,
	.param .u32 vec_log10_param_1,
	.param .u32 vec_log10_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<39>;
	.reg .f64 	%fd<61>;


	ld.param.u32 	%r14, [vec_log10_param_0];
	ld.param.u32 	%r12, [vec_log10_param_1];
	ld.param.u32 	%r13, [vec_log10_param_2];
	mov.u32 	%r15, %tid.x;
	mov.u32 	%r16, %ntid.x;
	mov.u32 	%r17, %ctaid.x;
	mad.lo.s32 	%r1, %r16, %r17, %r15;
	setp.ge.u32	%p1, %r1, %r14;
	@%p1 bra 	BB51_12;

	cvta.to.global.u32 	%r18, %r13;
	shl.b32 	%r19, %r1, 3;
	add.s32 	%r20, %r18, %r19;
	ld.global.f64 	%fd1, [%r20];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd1;
	}
	setp.gt.f64	%p2, %fd1, 0d0000000000000000;
	setp.lt.s32	%p3, %r35, 2146435072;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB51_6;
	bra.uni 	BB51_2;

BB51_6:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd1;
	}
	mov.u32 	%r37, -1023;
	setp.gt.s32	%p8, %r35, 1048575;
	@%p8 bra 	BB51_8;

	mul.f64 	%fd11, %fd1, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd11;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd11;
	}
	mov.u32 	%r37, -1077;

BB51_8:
	shr.u32 	%r23, %r35, 20;
	add.s32 	%r38, %r37, %r23;
	and.b32  	%r24, %r35, -2146435073;
	or.b32  	%r25, %r24, 1072693248;
	mov.b64 	%fd59, {%r36, %r25};
	setp.lt.s32	%p9, %r25, 1073127583;
	@%p9 bra 	BB51_10;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r26, %temp}, %fd59;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r27}, %fd59;
	}
	add.s32 	%r28, %r27, -1048576;
	mov.b64 	%fd59, {%r26, %r28};
	add.s32 	%r38, %r38, 1;

BB51_10:
	add.f64 	%fd13, %fd59, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd12,%fd13;
	// inline asm
	neg.f64 	%fd14, %fd13;
	mov.f64 	%fd15, 0d3FF0000000000000;
	fma.rn.f64 	%fd16, %fd14, %fd12, %fd15;
	fma.rn.f64 	%fd17, %fd16, %fd16, %fd16;
	fma.rn.f64 	%fd18, %fd17, %fd12, %fd12;
	add.f64 	%fd19, %fd59, 0dBFF0000000000000;
	mul.f64 	%fd20, %fd19, %fd18;
	fma.rn.f64 	%fd21, %fd19, %fd18, %fd20;
	mul.f64 	%fd22, %fd21, %fd21;
	mov.f64 	%fd23, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd24, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd25, %fd24, %fd22, %fd23;
	mov.f64 	%fd26, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd27, %fd25, %fd22, %fd26;
	mov.f64 	%fd28, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd29, %fd27, %fd22, %fd28;
	mov.f64 	%fd30, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd31, %fd29, %fd22, %fd30;
	mov.f64 	%fd32, 0d3F624924923BE72D;
	fma.rn.f64 	%fd33, %fd31, %fd22, %fd32;
	mov.f64 	%fd34, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd35, %fd33, %fd22, %fd34;
	mov.f64 	%fd36, 0d3FB5555555555554;
	fma.rn.f64 	%fd37, %fd35, %fd22, %fd36;
	sub.f64 	%fd38, %fd19, %fd21;
	add.f64 	%fd39, %fd38, %fd38;
	neg.f64 	%fd40, %fd21;
	fma.rn.f64 	%fd41, %fd40, %fd19, %fd39;
	mul.f64 	%fd42, %fd18, %fd41;
	mul.f64 	%fd43, %fd22, %fd37;
	fma.rn.f64 	%fd44, %fd43, %fd21, %fd42;
	xor.b32  	%r29, %r38, -2147483648;
	mov.u32 	%r30, 1127219200;
	mov.b64 	%fd45, {%r29, %r30};
	mov.u32 	%r31, -2147483648;
	mov.b64 	%fd46, {%r31, %r30};
	sub.f64 	%fd47, %fd45, %fd46;
	mov.f64 	%fd48, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd49, %fd47, %fd48, %fd21;
	neg.f64 	%fd50, %fd47;
	fma.rn.f64 	%fd51, %fd50, %fd48, %fd49;
	sub.f64 	%fd52, %fd51, %fd21;
	sub.f64 	%fd53, %fd44, %fd52;
	mov.f64 	%fd54, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd55, %fd47, %fd54, %fd53;
	add.f64 	%fd60, %fd49, %fd55;
	bra.uni 	BB51_11;

BB51_2:
	abs.f64 	%fd9, %fd1;
	setp.gtu.f64	%p5, %fd9, 0d7FF0000000000000;
	@%p5 bra 	BB51_5;
	bra.uni 	BB51_3;

BB51_5:
	add.f64 	%fd60, %fd1, %fd1;
	bra.uni 	BB51_11;

BB51_3:
	setp.eq.f64	%p6, %fd1, 0d0000000000000000;
	mov.f64 	%fd60, 0dFFF0000000000000;
	@%p6 bra 	BB51_11;

	setp.eq.f64	%p7, %fd1, 0d7FF0000000000000;
	selp.f64	%fd60, %fd1, 0dFFF8000000000000, %p7;

BB51_11:
	cvta.to.global.u32 	%r32, %r12;
	mul.f64 	%fd56, %fd60, 0d3C695355BAAAFAD3;
	mov.f64 	%fd57, 0d3FDBCB7B1526E50E;
	fma.rn.f64 	%fd58, %fd60, %fd57, %fd56;
	add.s32 	%r34, %r32, %r19;
	st.global.f64 	[%r34], %fd58;

BB51_12:
	ret;
}

	// .globl	vec_log1p
.visible .entry vec_log1p(
	.param .u32 vec_log1p_param_0,
	.param .u32 vec_log1p_param_1,
	.param .u32 vec_log1p_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<40>;
	.reg .f64 	%fd<83>;


	ld.param.u32 	%r14, [vec_log1p_param_0];
	ld.param.u32 	%r12, [vec_log1p_param_1];
	ld.param.u32 	%r13, [vec_log1p_param_2];
	mov.u32 	%r15, %tid.x;
	mov.u32 	%r16, %ntid.x;
	mov.u32 	%r17, %ctaid.x;
	mad.lo.s32 	%r1, %r16, %r17, %r15;
	setp.ge.u32	%p1, %r1, %r14;
	@%p1 bra 	BB52_14;

	cvta.to.global.u32 	%r18, %r13;
	shl.b32 	%r19, %r1, 3;
	add.s32 	%r20, %r18, %r19;
	ld.global.f64 	%fd1, [%r20];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd1;
	}
	setp.lt.u32	%p2, %r21, 1071994197;
	setp.lt.s32	%p3, %r21, -1076258407;
	or.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB52_12;
	bra.uni 	BB52_2;

BB52_12:
	add.f64 	%fd58, %fd1, 0d4000000000000000;
	div.rn.f64 	%fd59, %fd1, %fd58;
	mul.f64 	%fd60, %fd1, %fd59;
	neg.f64 	%fd61, %fd60;
	sub.f64 	%fd62, %fd1, %fd60;
	mul.f64 	%fd63, %fd62, %fd62;
	mov.f64 	%fd64, 0d3ED087FFCEB2DC44;
	mov.f64 	%fd65, 0d3EB372FB2FBE14B5;
	fma.rn.f64 	%fd66, %fd65, %fd63, %fd64;
	mov.f64 	%fd67, 0d3EF3B9FF890F468C;
	fma.rn.f64 	%fd68, %fd66, %fd63, %fd67;
	mov.f64 	%fd69, 0d3F17457EFD51BAF8;
	fma.rn.f64 	%fd70, %fd68, %fd63, %fd69;
	mov.f64 	%fd71, 0d3F3C71C8DE3CE825;
	fma.rn.f64 	%fd72, %fd70, %fd63, %fd71;
	mov.f64 	%fd73, 0d3F6249248FA4661F;
	fma.rn.f64 	%fd74, %fd72, %fd63, %fd73;
	mov.f64 	%fd75, 0d3F899999999D70C4;
	fma.rn.f64 	%fd76, %fd74, %fd63, %fd75;
	mov.f64 	%fd77, 0d3FB5555555555462;
	fma.rn.f64 	%fd78, %fd76, %fd63, %fd77;
	mul.f64 	%fd79, %fd63, %fd78;
	fma.rn.f64 	%fd80, %fd79, %fd62, %fd61;
	add.f64 	%fd82, %fd1, %fd80;
	bra.uni 	BB52_13;

BB52_2:
	add.f64 	%fd2, %fd1, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r36}, %fd2;
	}
	setp.gt.f64	%p5, %fd2, 0d0000000000000000;
	setp.lt.s32	%p6, %r36, 2146435072;
	and.pred  	%p7, %p5, %p6;
	@%p7 bra 	BB52_7;
	bra.uni 	BB52_3;

BB52_7:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r37, %temp}, %fd2;
	}
	mov.u32 	%r38, -1023;
	setp.gt.s32	%p11, %r36, 1048575;
	@%p11 bra 	BB52_9;

	mul.f64 	%fd13, %fd2, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r36}, %fd13;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r37, %temp}, %fd13;
	}
	mov.u32 	%r38, -1077;

BB52_9:
	shr.u32 	%r24, %r36, 20;
	add.s32 	%r39, %r38, %r24;
	and.b32  	%r25, %r36, -2146435073;
	or.b32  	%r26, %r25, 1072693248;
	mov.b64 	%fd81, {%r37, %r26};
	setp.lt.s32	%p12, %r26, 1073127583;
	@%p12 bra 	BB52_11;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r27, %temp}, %fd81;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd81;
	}
	add.s32 	%r29, %r28, -1048576;
	mov.b64 	%fd81, {%r27, %r29};
	add.s32 	%r39, %r39, 1;

BB52_11:
	add.f64 	%fd15, %fd81, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd14,%fd15;
	// inline asm
	neg.f64 	%fd16, %fd15;
	mov.f64 	%fd17, 0d3FF0000000000000;
	fma.rn.f64 	%fd18, %fd16, %fd14, %fd17;
	fma.rn.f64 	%fd19, %fd18, %fd18, %fd18;
	fma.rn.f64 	%fd20, %fd19, %fd14, %fd14;
	add.f64 	%fd21, %fd81, 0dBFF0000000000000;
	mul.f64 	%fd22, %fd21, %fd20;
	fma.rn.f64 	%fd23, %fd21, %fd20, %fd22;
	mul.f64 	%fd24, %fd23, %fd23;
	mov.f64 	%fd25, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd26, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd27, %fd26, %fd24, %fd25;
	mov.f64 	%fd28, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd29, %fd27, %fd24, %fd28;
	mov.f64 	%fd30, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd31, %fd29, %fd24, %fd30;
	mov.f64 	%fd32, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd33, %fd31, %fd24, %fd32;
	mov.f64 	%fd34, 0d3F624924923BE72D;
	fma.rn.f64 	%fd35, %fd33, %fd24, %fd34;
	mov.f64 	%fd36, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd37, %fd35, %fd24, %fd36;
	mov.f64 	%fd38, 0d3FB5555555555554;
	fma.rn.f64 	%fd39, %fd37, %fd24, %fd38;
	sub.f64 	%fd40, %fd21, %fd23;
	add.f64 	%fd41, %fd40, %fd40;
	neg.f64 	%fd42, %fd23;
	fma.rn.f64 	%fd43, %fd42, %fd21, %fd41;
	mul.f64 	%fd44, %fd20, %fd43;
	mul.f64 	%fd45, %fd24, %fd39;
	fma.rn.f64 	%fd46, %fd45, %fd23, %fd44;
	xor.b32  	%r30, %r39, -2147483648;
	mov.u32 	%r31, 1127219200;
	mov.b64 	%fd47, {%r30, %r31};
	mov.u32 	%r32, -2147483648;
	mov.b64 	%fd48, {%r32, %r31};
	sub.f64 	%fd49, %fd47, %fd48;
	mov.f64 	%fd50, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd51, %fd49, %fd50, %fd23;
	neg.f64 	%fd52, %fd49;
	fma.rn.f64 	%fd53, %fd52, %fd50, %fd51;
	sub.f64 	%fd54, %fd53, %fd23;
	sub.f64 	%fd55, %fd46, %fd54;
	mov.f64 	%fd56, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd57, %fd49, %fd56, %fd55;
	add.f64 	%fd82, %fd51, %fd57;
	bra.uni 	BB52_13;

BB52_3:
	abs.f64 	%fd11, %fd2;
	setp.gtu.f64	%p8, %fd11, 0d7FF0000000000000;
	@%p8 bra 	BB52_6;
	bra.uni 	BB52_4;

BB52_6:
	add.f64 	%fd82, %fd2, %fd2;
	bra.uni 	BB52_13;

BB52_4:
	setp.eq.f64	%p9, %fd2, 0d0000000000000000;
	mov.f64 	%fd82, 0dFFF0000000000000;
	@%p9 bra 	BB52_13;

	setp.eq.f64	%p10, %fd2, 0d7FF0000000000000;
	selp.f64	%fd82, %fd2, 0dFFF8000000000000, %p10;

BB52_13:
	cvta.to.global.u32 	%r33, %r12;
	add.s32 	%r35, %r33, %r19;
	st.global.f64 	[%r35], %fd82;

BB52_14:
	ret;
}

	// .globl	vec_log2
.visible .entry vec_log2(
	.param .u32 vec_log2_param_0,
	.param .u32 vec_log2_param_1,
	.param .u32 vec_log2_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<39>;
	.reg .f64 	%fd<61>;


	ld.param.u32 	%r14, [vec_log2_param_0];
	ld.param.u32 	%r12, [vec_log2_param_1];
	ld.param.u32 	%r13, [vec_log2_param_2];
	mov.u32 	%r15, %tid.x;
	mov.u32 	%r16, %ntid.x;
	mov.u32 	%r17, %ctaid.x;
	mad.lo.s32 	%r1, %r16, %r17, %r15;
	setp.ge.u32	%p1, %r1, %r14;
	@%p1 bra 	BB53_12;

	cvta.to.global.u32 	%r18, %r13;
	shl.b32 	%r19, %r1, 3;
	add.s32 	%r20, %r18, %r19;
	ld.global.f64 	%fd1, [%r20];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd1;
	}
	setp.gt.f64	%p2, %fd1, 0d0000000000000000;
	setp.lt.s32	%p3, %r35, 2146435072;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB53_6;
	bra.uni 	BB53_2;

BB53_6:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd1;
	}
	mov.u32 	%r37, -1023;
	setp.gt.s32	%p8, %r35, 1048575;
	@%p8 bra 	BB53_8;

	mul.f64 	%fd11, %fd1, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd11;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd11;
	}
	mov.u32 	%r37, -1077;

BB53_8:
	shr.u32 	%r23, %r35, 20;
	add.s32 	%r38, %r37, %r23;
	and.b32  	%r24, %r35, -2146435073;
	or.b32  	%r25, %r24, 1072693248;
	mov.b64 	%fd59, {%r36, %r25};
	setp.lt.s32	%p9, %r25, 1073127583;
	@%p9 bra 	BB53_10;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r26, %temp}, %fd59;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r27}, %fd59;
	}
	add.s32 	%r28, %r27, -1048576;
	mov.b64 	%fd59, {%r26, %r28};
	add.s32 	%r38, %r38, 1;

BB53_10:
	add.f64 	%fd13, %fd59, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd12,%fd13;
	// inline asm
	neg.f64 	%fd14, %fd13;
	mov.f64 	%fd15, 0d3FF0000000000000;
	fma.rn.f64 	%fd16, %fd14, %fd12, %fd15;
	fma.rn.f64 	%fd17, %fd16, %fd16, %fd16;
	fma.rn.f64 	%fd18, %fd17, %fd12, %fd12;
	add.f64 	%fd19, %fd59, 0dBFF0000000000000;
	mul.f64 	%fd20, %fd19, %fd18;
	fma.rn.f64 	%fd21, %fd19, %fd18, %fd20;
	mul.f64 	%fd22, %fd21, %fd21;
	mov.f64 	%fd23, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd24, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd25, %fd24, %fd22, %fd23;
	mov.f64 	%fd26, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd27, %fd25, %fd22, %fd26;
	mov.f64 	%fd28, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd29, %fd27, %fd22, %fd28;
	mov.f64 	%fd30, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd31, %fd29, %fd22, %fd30;
	mov.f64 	%fd32, 0d3F624924923BE72D;
	fma.rn.f64 	%fd33, %fd31, %fd22, %fd32;
	mov.f64 	%fd34, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd35, %fd33, %fd22, %fd34;
	mov.f64 	%fd36, 0d3FB5555555555554;
	fma.rn.f64 	%fd37, %fd35, %fd22, %fd36;
	sub.f64 	%fd38, %fd19, %fd21;
	add.f64 	%fd39, %fd38, %fd38;
	neg.f64 	%fd40, %fd21;
	fma.rn.f64 	%fd41, %fd40, %fd19, %fd39;
	mul.f64 	%fd42, %fd18, %fd41;
	mul.f64 	%fd43, %fd22, %fd37;
	fma.rn.f64 	%fd44, %fd43, %fd21, %fd42;
	xor.b32  	%r29, %r38, -2147483648;
	mov.u32 	%r30, 1127219200;
	mov.b64 	%fd45, {%r29, %r30};
	mov.u32 	%r31, -2147483648;
	mov.b64 	%fd46, {%r31, %r30};
	sub.f64 	%fd47, %fd45, %fd46;
	mov.f64 	%fd48, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd49, %fd47, %fd48, %fd21;
	neg.f64 	%fd50, %fd47;
	fma.rn.f64 	%fd51, %fd50, %fd48, %fd49;
	sub.f64 	%fd52, %fd51, %fd21;
	sub.f64 	%fd53, %fd44, %fd52;
	mov.f64 	%fd54, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd55, %fd47, %fd54, %fd53;
	add.f64 	%fd60, %fd49, %fd55;
	bra.uni 	BB53_11;

BB53_2:
	abs.f64 	%fd9, %fd1;
	setp.gtu.f64	%p5, %fd9, 0d7FF0000000000000;
	@%p5 bra 	BB53_5;
	bra.uni 	BB53_3;

BB53_5:
	add.f64 	%fd60, %fd1, %fd1;
	bra.uni 	BB53_11;

BB53_3:
	setp.eq.f64	%p6, %fd1, 0d0000000000000000;
	mov.f64 	%fd60, 0dFFF0000000000000;
	@%p6 bra 	BB53_11;

	setp.eq.f64	%p7, %fd1, 0d7FF0000000000000;
	selp.f64	%fd60, %fd1, 0dFFF8000000000000, %p7;

BB53_11:
	cvta.to.global.u32 	%r32, %r12;
	mul.f64 	%fd56, %fd60, 0d3C7777D0FFDA0D24;
	mov.f64 	%fd57, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd58, %fd60, %fd57, %fd56;
	add.s32 	%r34, %r32, %r19;
	st.global.f64 	[%r34], %fd58;

BB53_12:
	ret;
}

	// .globl	vec_logb
.visible .entry vec_logb(
	.param .u32 vec_logb_param_0,
	.param .u32 vec_logb_param_1,
	.param .u32 vec_logb_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<21>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<5>;


	ld.param.u32 	%r5, [vec_logb_param_0];
	ld.param.u32 	%r3, [vec_logb_param_1];
	ld.param.u32 	%r4, [vec_logb_param_2];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB54_9;

	cvta.to.global.u32 	%r9, %r4;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	ld.global.f64 	%fd1, [%r11];
	abs.f64 	%fd2, %fd1;
	setp.gtu.f64	%p2, %fd2, 0d7FF0000000000000;
	@%p2 bra 	BB54_7;
	bra.uni 	BB54_2;

BB54_7:
	add.f64 	%fd8, %fd1, %fd1;
	bra.uni 	BB54_8;

BB54_2:
	setp.eq.f64	%p3, %fd2, 0d7FF0000000000000;
	mov.f64 	%fd8, %fd2;
	@%p3 bra 	BB54_8;

	setp.eq.f64	%p4, %fd2, 0d0000000000000000;
	mov.f64 	%fd7, 0dFFF0000000000000;
	mov.f64 	%fd8, %fd7;
	@%p4 bra 	BB54_8;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd2;
	}
	setp.gt.u32	%p5, %r2, 1048575;
	@%p5 bra 	BB54_6;
	bra.uni 	BB54_5;

BB54_6:
	shr.u32 	%r16, %r2, 20;
	add.s32 	%r17, %r16, -1023;
	cvt.rn.f64.s32	%fd8, %r17;
	bra.uni 	BB54_8;

BB54_5:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r12, %temp}, %fd2;
	}
	cvt.u64.u32	%rd1, %r2;
	shl.b64 	%rd2, %rd1, 32;
	cvt.u64.u32	%rd3, %r12;
	or.b64  	%rd4, %rd2, %rd3;
	clz.b64 	%r13, %rd4;
	mov.u32 	%r14, -1011;
	sub.s32 	%r15, %r14, %r13;
	cvt.rn.f64.s32	%fd8, %r15;

BB54_8:
	cvta.to.global.u32 	%r18, %r3;
	add.s32 	%r20, %r18, %r10;
	st.global.f64 	[%r20], %fd8;

BB54_9:
	ret;
}

	// .globl	vec_log
.visible .entry vec_log(
	.param .u32 vec_log_param_0,
	.param .u32 vec_log_param_1,
	.param .u32 vec_log_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<39>;
	.reg .f64 	%fd<58>;


	ld.param.u32 	%r14, [vec_log_param_0];
	ld.param.u32 	%r12, [vec_log_param_1];
	ld.param.u32 	%r13, [vec_log_param_2];
	mov.u32 	%r15, %tid.x;
	mov.u32 	%r16, %ntid.x;
	mov.u32 	%r17, %ctaid.x;
	mad.lo.s32 	%r1, %r16, %r17, %r15;
	setp.ge.u32	%p1, %r1, %r14;
	@%p1 bra 	BB55_12;

	cvta.to.global.u32 	%r18, %r13;
	shl.b32 	%r19, %r1, 3;
	add.s32 	%r20, %r18, %r19;
	ld.global.f64 	%fd1, [%r20];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd1;
	}
	setp.gt.f64	%p2, %fd1, 0d0000000000000000;
	setp.lt.s32	%p3, %r35, 2146435072;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB55_6;
	bra.uni 	BB55_2;

BB55_6:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd1;
	}
	mov.u32 	%r37, -1023;
	setp.gt.s32	%p8, %r35, 1048575;
	@%p8 bra 	BB55_8;

	mul.f64 	%fd11, %fd1, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd11;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd11;
	}
	mov.u32 	%r37, -1077;

BB55_8:
	shr.u32 	%r23, %r35, 20;
	add.s32 	%r38, %r37, %r23;
	and.b32  	%r24, %r35, -2146435073;
	or.b32  	%r25, %r24, 1072693248;
	mov.b64 	%fd56, {%r36, %r25};
	setp.lt.s32	%p9, %r25, 1073127583;
	@%p9 bra 	BB55_10;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r26, %temp}, %fd56;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r27}, %fd56;
	}
	add.s32 	%r28, %r27, -1048576;
	mov.b64 	%fd56, {%r26, %r28};
	add.s32 	%r38, %r38, 1;

BB55_10:
	add.f64 	%fd13, %fd56, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd12,%fd13;
	// inline asm
	neg.f64 	%fd14, %fd13;
	mov.f64 	%fd15, 0d3FF0000000000000;
	fma.rn.f64 	%fd16, %fd14, %fd12, %fd15;
	fma.rn.f64 	%fd17, %fd16, %fd16, %fd16;
	fma.rn.f64 	%fd18, %fd17, %fd12, %fd12;
	add.f64 	%fd19, %fd56, 0dBFF0000000000000;
	mul.f64 	%fd20, %fd19, %fd18;
	fma.rn.f64 	%fd21, %fd19, %fd18, %fd20;
	mul.f64 	%fd22, %fd21, %fd21;
	mov.f64 	%fd23, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd24, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd25, %fd24, %fd22, %fd23;
	mov.f64 	%fd26, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd27, %fd25, %fd22, %fd26;
	mov.f64 	%fd28, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd29, %fd27, %fd22, %fd28;
	mov.f64 	%fd30, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd31, %fd29, %fd22, %fd30;
	mov.f64 	%fd32, 0d3F624924923BE72D;
	fma.rn.f64 	%fd33, %fd31, %fd22, %fd32;
	mov.f64 	%fd34, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd35, %fd33, %fd22, %fd34;
	mov.f64 	%fd36, 0d3FB5555555555554;
	fma.rn.f64 	%fd37, %fd35, %fd22, %fd36;
	sub.f64 	%fd38, %fd19, %fd21;
	add.f64 	%fd39, %fd38, %fd38;
	neg.f64 	%fd40, %fd21;
	fma.rn.f64 	%fd41, %fd40, %fd19, %fd39;
	mul.f64 	%fd42, %fd18, %fd41;
	mul.f64 	%fd43, %fd22, %fd37;
	fma.rn.f64 	%fd44, %fd43, %fd21, %fd42;
	xor.b32  	%r29, %r38, -2147483648;
	mov.u32 	%r30, 1127219200;
	mov.b64 	%fd45, {%r29, %r30};
	mov.u32 	%r31, -2147483648;
	mov.b64 	%fd46, {%r31, %r30};
	sub.f64 	%fd47, %fd45, %fd46;
	mov.f64 	%fd48, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd49, %fd47, %fd48, %fd21;
	neg.f64 	%fd50, %fd47;
	fma.rn.f64 	%fd51, %fd50, %fd48, %fd49;
	sub.f64 	%fd52, %fd51, %fd21;
	sub.f64 	%fd53, %fd44, %fd52;
	mov.f64 	%fd54, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd55, %fd47, %fd54, %fd53;
	add.f64 	%fd57, %fd49, %fd55;
	bra.uni 	BB55_11;

BB55_2:
	abs.f64 	%fd9, %fd1;
	setp.gtu.f64	%p5, %fd9, 0d7FF0000000000000;
	@%p5 bra 	BB55_5;
	bra.uni 	BB55_3;

BB55_5:
	add.f64 	%fd57, %fd1, %fd1;
	bra.uni 	BB55_11;

BB55_3:
	setp.eq.f64	%p6, %fd1, 0d0000000000000000;
	mov.f64 	%fd57, 0dFFF0000000000000;
	@%p6 bra 	BB55_11;

	setp.eq.f64	%p7, %fd1, 0d7FF0000000000000;
	selp.f64	%fd57, %fd1, 0dFFF8000000000000, %p7;

BB55_11:
	cvta.to.global.u32 	%r32, %r12;
	add.s32 	%r34, %r32, %r19;
	st.global.f64 	[%r34], %fd57;

BB55_12:
	ret;
}

	// .globl	vec_normcdf
.visible .entry vec_normcdf(
	.param .u32 vec_normcdf_param_0,
	.param .u32 vec_normcdf_param_1,
	.param .u32 vec_normcdf_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<145>;


	ld.param.u32 	%r7, [vec_normcdf_param_0];
	ld.param.u32 	%r6, [vec_normcdf_param_2];
	mov.u32 	%r8, %tid.x;
	mov.u32 	%r9, %ntid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r1, %r9, %r10, %r8;
	setp.ge.u32	%p1, %r1, %r7;
	@%p1 bra 	BB56_9;

	cvta.to.global.u32 	%r11, %r6;
	shl.b32 	%r12, %r1, 3;
	add.s32 	%r13, %r11, %r12;
	ld.global.f64 	%fd143, [%r13];
	abs.f64 	%fd12, %fd143;
	setp.leu.f64	%p2, %fd12, 0d4043400000000000;
	@%p2 bra 	BB56_3;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd143;
	}
	and.b32  	%r15, %r14, -2147483648;
	mov.f64 	%fd13, 0d4043400000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd13;
	}
	and.b32  	%r17, %r16, 2147483647;
	or.b32  	%r18, %r17, %r15;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd13;
	}
	mov.b64 	%fd143, {%r19, %r18};

BB56_3:
	mov.f64 	%fd14, 0dBFE6A09E667F3BCD;
	mul.rn.f64 	%fd4, %fd143, %fd14;
	neg.f64 	%fd15, %fd4;
	fma.rn.f64 	%fd16, %fd143, %fd14, %fd15;
	mov.f64 	%fd17, 0d3C8BDD3413B26456;
	fma.rn.f64 	%fd5, %fd143, %fd17, %fd16;
	add.rn.f64 	%fd6, %fd4, %fd5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd6;
	}
	and.b32  	%r3, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r4, %temp}, %fd6;
	}
	setp.lt.u32	%p3, %r3, 2146435072;
	@%p3 bra 	BB56_5;
	bra.uni 	BB56_4;

BB56_5:
	setp.lt.s32	%p8, %r2, 0;
	mov.b64 	%fd24, {%r4, %r3};
	add.f64 	%fd25, %fd24, 0dC010000000000000;
	add.f64 	%fd21, %fd24, 0d4010000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd20,%fd21;
	// inline asm
	neg.f64 	%fd26, %fd21;
	mov.f64 	%fd27, 0d3FF0000000000000;
	fma.rn.f64 	%fd28, %fd26, %fd20, %fd27;
	fma.rn.f64 	%fd29, %fd28, %fd28, %fd28;
	fma.rn.f64 	%fd30, %fd29, %fd20, %fd20;
	mul.f64 	%fd31, %fd25, %fd30;
	add.rn.f64 	%fd32, %fd31, %fd27;
	mov.f64 	%fd33, 0dC010000000000000;
	fma.rn.f64 	%fd34, %fd33, %fd32, %fd24;
	neg.f64 	%fd35, %fd31;
	fma.rn.f64 	%fd36, %fd35, %fd24, %fd34;
	fma.rn.f64 	%fd37, %fd30, %fd36, %fd31;
	mov.f64 	%fd38, 0dBE44E1C6FD03D328;
	mov.f64 	%fd39, 0dBDF8774AD4E0BFD7;
	fma.rn.f64 	%fd40, %fd39, %fd37, %fd38;
	mov.f64 	%fd41, 0dBE4330149F7A56B6;
	fma.rn.f64 	%fd42, %fd40, %fd37, %fd41;
	mov.f64 	%fd43, 0d3E7BEDDED8376273;
	fma.rn.f64 	%fd44, %fd42, %fd37, %fd43;
	mov.f64 	%fd45, 0d3E6F9254C3ABF22B;
	fma.rn.f64 	%fd46, %fd44, %fd37, %fd45;
	mov.f64 	%fd47, 0dBEAB9068C2148CF0;
	fma.rn.f64 	%fd48, %fd46, %fd37, %fd47;
	mov.f64 	%fd49, 0d3E94C6454DB34009;
	fma.rn.f64 	%fd50, %fd48, %fd37, %fd49;
	mov.f64 	%fd51, 0d3ED7F1C378F2311D;
	fma.rn.f64 	%fd52, %fd50, %fd37, %fd51;
	mov.f64 	%fd53, 0dBEE78E051C6D5C58;
	fma.rn.f64 	%fd54, %fd52, %fd37, %fd53;
	mov.f64 	%fd55, 0dBEF995B4EAD14A90;
	fma.rn.f64 	%fd56, %fd54, %fd37, %fd55;
	mov.f64 	%fd57, 0d3F23BE27CF0A29B2;
	fma.rn.f64 	%fd58, %fd56, %fd37, %fd57;
	mov.f64 	%fd59, 0dBF2A1DEF3E81672E;
	fma.rn.f64 	%fd60, %fd58, %fd37, %fd59;
	mov.f64 	%fd61, 0dBF48D4ABE68C1713;
	fma.rn.f64 	%fd62, %fd60, %fd37, %fd61;
	mov.f64 	%fd63, 0d3F749C67210DD6B4;
	fma.rn.f64 	%fd64, %fd62, %fd37, %fd63;
	mov.f64 	%fd65, 0dBF9096238568E357;
	fma.rn.f64 	%fd66, %fd64, %fd37, %fd65;
	mov.f64 	%fd67, 0d3FA3079EDF8C2DC9;
	fma.rn.f64 	%fd68, %fd66, %fd37, %fd67;
	mov.f64 	%fd69, 0dBFB0FB06DFF601FC;
	fma.rn.f64 	%fd70, %fd68, %fd37, %fd69;
	mov.f64 	%fd71, 0d3FB7FEE004DFBCDC;
	fma.rn.f64 	%fd72, %fd70, %fd37, %fd71;
	mov.f64 	%fd73, 0dBFB9DDB23C3DB8C6;
	fma.rn.f64 	%fd74, %fd72, %fd37, %fd73;
	mov.f64 	%fd75, 0d3FB16ECEFCFA5FDA;
	fma.rn.f64 	%fd76, %fd74, %fd37, %fd75;
	mov.f64 	%fd77, 0d3F8F7F5DF66FB6D6;
	fma.rn.f64 	%fd78, %fd76, %fd37, %fd77;
	mov.f64 	%fd79, 0dBFC1DF1AD154A29D;
	fma.rn.f64 	%fd80, %fd78, %fd37, %fd79;
	mov.f64 	%fd81, 0d3FF3BA5916E9FD7F;
	fma.rn.f64 	%fd82, %fd80, %fd37, %fd81;
	mov.f64 	%fd83, 0d4000000000000000;
	fma.rn.f64 	%fd23, %fd83, %fd24, %fd27;
	// inline asm
	rcp.approx.ftz.f64 %fd22,%fd23;
	// inline asm
	neg.f64 	%fd84, %fd23;
	fma.rn.f64 	%fd85, %fd84, %fd22, %fd27;
	fma.rn.f64 	%fd86, %fd85, %fd85, %fd85;
	fma.rn.f64 	%fd87, %fd86, %fd22, %fd22;
	mul.f64 	%fd88, %fd82, %fd87;
	mul.f64 	%fd89, %fd88, 0dC000000000000000;
	fma.rn.f64 	%fd90, %fd24, %fd89, %fd82;
	neg.f64 	%fd91, %fd88;
	add.rn.f64 	%fd92, %fd90, %fd91;
	fma.rn.f64 	%fd93, %fd92, %fd87, %fd88;
	mul.f64 	%fd94, %fd24, %fd24;
	neg.f64 	%fd95, %fd94;
	mov.f64 	%fd96, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd97, %fd95, %fd96;
	mov.f64 	%fd98, 0d4338000000000000;
	add.rn.f64 	%fd99, %fd97, %fd98;
	mov.f64 	%fd100, 0dC338000000000000;
	add.rn.f64 	%fd101, %fd99, %fd100;
	mov.f64 	%fd102, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd103, %fd101, %fd102, %fd95;
	mov.f64 	%fd104, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd105, %fd101, %fd104, %fd103;
	mov.f64 	%fd106, 0d3E928AF3FCA213EA;
	mov.f64 	%fd107, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd108, %fd107, %fd105, %fd106;
	mov.f64 	%fd109, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd110, %fd108, %fd105, %fd109;
	mov.f64 	%fd111, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd112, %fd110, %fd105, %fd111;
	mov.f64 	%fd113, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd114, %fd112, %fd105, %fd113;
	mov.f64 	%fd115, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd116, %fd114, %fd105, %fd115;
	mov.f64 	%fd117, 0d3F81111111122322;
	fma.rn.f64 	%fd118, %fd116, %fd105, %fd117;
	mov.f64 	%fd119, 0d3FA55555555502A1;
	fma.rn.f64 	%fd120, %fd118, %fd105, %fd119;
	mov.f64 	%fd121, 0d3FC5555555555511;
	fma.rn.f64 	%fd122, %fd120, %fd105, %fd121;
	mov.f64 	%fd123, 0d3FE000000000000B;
	fma.rn.f64 	%fd124, %fd122, %fd105, %fd123;
	fma.rn.f64 	%fd125, %fd124, %fd105, %fd27;
	fma.rn.f64 	%fd126, %fd125, %fd105, %fd27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r20, %temp}, %fd99;
	}
	shr.u32 	%r21, %r20, 31;
	add.s32 	%r22, %r20, %r21;
	shr.s32 	%r23, %r22, 1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r24, %temp}, %fd126;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd126;
	}
	shl.b32 	%r26, %r23, 20;
	add.s32 	%r27, %r25, %r26;
	mov.b64 	%fd127, {%r24, %r27};
	sub.s32 	%r28, %r20, %r23;
	shl.b32 	%r29, %r28, 20;
	add.s32 	%r30, %r29, 1072693248;
	mov.u32 	%r31, 0;
	mov.b64 	%fd128, {%r31, %r30};
	mul.f64 	%fd129, %fd127, %fd128;
	neg.f64 	%fd130, %fd24;
	fma.rn.f64 	%fd131, %fd130, %fd24, %fd94;
	fma.rn.f64 	%fd132, %fd129, %fd131, %fd129;
	mul.f64 	%fd133, %fd93, %fd132;
	setp.gt.u32	%p9, %r3, 1077624832;
	selp.f64	%fd134, 0d0000000000000000, %fd133, %p9;
	sub.f64 	%fd135, %fd83, %fd134;
	selp.f64	%fd144, %fd135, %fd134, %p8;
	bra.uni 	BB56_6;

BB56_4:
	setp.lt.s32	%p4, %r2, 0;
	setp.eq.s32	%p5, %r4, 0;
	setp.eq.s32	%p6, %r3, 2146435072;
	and.pred  	%p7, %p6, %p5;
	selp.f64	%fd18, 0d4000000000000000, 0d0000000000000000, %p4;
	add.f64 	%fd19, %fd6, %fd6;
	selp.f64	%fd144, %fd18, %fd19, %p7;

BB56_6:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r32}, %fd143;
	}
	setp.lt.u32	%p10, %r32, -1074790399;
	@%p10 bra 	BB56_8;

	mov.f64 	%fd142, 0dBFE6A09E667F3BCD;
	mul.rn.f64 	%fd141, %fd143, %fd142;
	sub.f64 	%fd136, %fd141, %fd6;
	add.rn.f64 	%fd137, %fd136, %fd5;
	mul.f64 	%fd138, %fd6, 0dC000000000000000;
	mul.f64 	%fd139, %fd138, %fd144;
	fma.rn.f64 	%fd144, %fd139, %fd137, %fd144;

BB56_8:
	mov.u32 	%r41, %tid.x;
	mov.u32 	%r40, %ctaid.x;
	mov.u32 	%r39, %ntid.x;
	mad.lo.s32 	%r38, %r39, %r40, %r41;
	shl.b32 	%r37, %r38, 3;
	ld.param.u32 	%r36, [vec_normcdf_param_1];
	cvta.to.global.u32 	%r33, %r36;
	add.s32 	%r35, %r33, %r37;
	mul.f64 	%fd140, %fd144, 0d3FE0000000000000;
	st.global.f64 	[%r35], %fd140;

BB56_9:
	ret;
}

	// .globl	vec_normcdfinv
.visible .entry vec_normcdfinv(
	.param .u32 vec_normcdfinv_param_0,
	.param .u32 vec_normcdfinv_param_1,
	.param .u32 vec_normcdfinv_param_2
)
{
	.reg .pred 	%p<16>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<62>;
	.reg .f64 	%fd<273>;


	ld.param.u32 	%r15, [vec_normcdfinv_param_0];
	ld.param.u32 	%r14, [vec_normcdfinv_param_2];
	mov.u32 	%r16, %tid.x;
	mov.u32 	%r17, %ntid.x;
	mov.u32 	%r18, %ctaid.x;
	mad.lo.s32 	%r1, %r17, %r18, %r16;
	setp.ge.u32	%p1, %r1, %r15;
	@%p1 bra 	BB57_18;

	cvta.to.global.u32 	%r19, %r14;
	shl.b32 	%r20, %r1, 3;
	add.s32 	%r21, %r19, %r20;
	ld.global.f64 	%fd18, [%r21];
	add.f64 	%fd1, %fd18, %fd18;
	neg.f64 	%fd2, %fd1;
	mov.f64 	%fd19, 0d4000000000000000;
	add.rn.f64 	%fd3, %fd19, %fd2;
	setp.le.f64	%p2, %fd1, 0d3FFFFC0B65AA4E0E;
	setp.ge.f64	%p3, %fd1, 0d3F4FA4D2AD8F904D;
	and.pred  	%p4, %p3, %p2;
	@%p4 bra 	BB57_16;
	bra.uni 	BB57_2;

BB57_16:
	mul.rn.f64 	%fd175, %fd3, %fd1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r40}, %fd175;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r41, %temp}, %fd175;
	}
	shr.u32 	%r42, %r40, 20;
	and.b32  	%r43, %r42, 2046;
	add.s32 	%r44, %r43, 2147482626;
	mov.u32 	%r45, 1127219200;
	mov.b64 	%fd176, {%r44, %r45};
	mov.u32 	%r46, -2147483648;
	mov.b64 	%fd177, {%r46, %r45};
	sub.f64 	%fd178, %fd176, %fd177;
	and.b32  	%r47, %r40, -2145386497;
	add.s32 	%r48, %r47, 1071644672;
	mov.b64 	%fd179, {%r41, %r48};
	add.f64 	%fd180, %fd179, 0dBFF0000000000000;
	add.f64 	%fd174, %fd179, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd173,%fd174;
	// inline asm
	neg.f64 	%fd181, %fd174;
	mov.f64 	%fd182, 0d3FF0000000000000;
	fma.rn.f64 	%fd183, %fd181, %fd173, %fd182;
	fma.rn.f64 	%fd184, %fd183, %fd183, %fd183;
	fma.rn.f64 	%fd185, %fd184, %fd173, %fd173;
	mul.f64 	%fd186, %fd180, %fd185;
	mov.f64 	%fd187, 0dC000000000000000;
	fma.rn.f64 	%fd188, %fd187, %fd186, %fd180;
	neg.f64 	%fd189, %fd186;
	fma.rn.f64 	%fd190, %fd189, %fd180, %fd188;
	fma.rn.f64 	%fd191, %fd190, %fd185, %fd186;
	mul.f64 	%fd192, %fd191, %fd191;
	mov.f64 	%fd193, 0d3FA55CF59CDC5D89;
	mov.f64 	%fd194, 0d3FB5C5C218C775C9;
	fma.rn.f64 	%fd195, %fd194, %fd192, %fd193;
	mov.f64 	%fd196, 0d3FAEFD18CF6EBB9C;
	fma.rn.f64 	%fd197, %fd195, %fd192, %fd196;
	mov.f64 	%fd198, 0d3FB10682EDCB8D1B;
	fma.rn.f64 	%fd199, %fd197, %fd192, %fd198;
	mov.f64 	%fd200, 0d3FB3B1DD3AC7FC96;
	fma.rn.f64 	%fd201, %fd199, %fd192, %fd200;
	mov.f64 	%fd202, 0d3FB745CB459B54A6;
	fma.rn.f64 	%fd203, %fd201, %fd192, %fd202;
	mov.f64 	%fd204, 0d3FBC71C741A0669F;
	fma.rn.f64 	%fd205, %fd203, %fd192, %fd204;
	mov.f64 	%fd206, 0d3FC249249209112E;
	fma.rn.f64 	%fd207, %fd205, %fd192, %fd206;
	mov.f64 	%fd208, 0d3FC99999999A06C1;
	fma.rn.f64 	%fd209, %fd207, %fd192, %fd208;
	mov.f64 	%fd210, 0d3FD5555555555535;
	fma.rn.f64 	%fd211, %fd209, %fd192, %fd210;
	mul.f64 	%fd212, %fd192, %fd211;
	fma.rn.f64 	%fd213, %fd212, %fd191, %fd191;
	add.f64 	%fd214, %fd213, %fd213;
	mov.f64 	%fd215, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd216, %fd178, %fd215, %fd214;
	mov.f64 	%fd217, 0dC009000000000000;
	sub.f64 	%fd218, %fd217, %fd216;
	mov.f64 	%fd219, 0dBC08DDF93324D327;
	mov.f64 	%fd220, 0dBBB135D2E746E627;
	fma.rn.f64 	%fd221, %fd220, %fd218, %fd219;
	mov.f64 	%fd222, 0d3C37B83EEF0B7C9F;
	fma.rn.f64 	%fd223, %fd221, %fd218, %fd222;
	mov.f64 	%fd224, 0d3C69BA72CD589B91;
	fma.rn.f64 	%fd225, %fd223, %fd218, %fd224;
	mov.f64 	%fd226, 0dBCA33689090A6B96;
	fma.rn.f64 	%fd227, %fd225, %fd218, %fd226;
	mov.f64 	%fd228, 0d3C782E11898132E0;
	fma.rn.f64 	%fd229, %fd227, %fd218, %fd228;
	mov.f64 	%fd230, 0d3CFDE4ACFD9E26BA;
	fma.rn.f64 	%fd231, %fd229, %fd218, %fd230;
	mov.f64 	%fd232, 0dBD26D33EED66C487;
	fma.rn.f64 	%fd233, %fd231, %fd218, %fd232;
	mov.f64 	%fd234, 0dBD36F2167040D8E2;
	fma.rn.f64 	%fd235, %fd233, %fd218, %fd234;
	mov.f64 	%fd236, 0d3D872A22C2D77E20;
	fma.rn.f64 	%fd237, %fd235, %fd218, %fd236;
	mov.f64 	%fd238, 0dBDAC8859C4E5C0AF;
	fma.rn.f64 	%fd239, %fd237, %fd218, %fd238;
	mov.f64 	%fd240, 0dBDCDC583D118A561;
	fma.rn.f64 	%fd241, %fd239, %fd218, %fd240;
	mov.f64 	%fd242, 0d3E120F47CCF46B3C;
	fma.rn.f64 	%fd243, %fd241, %fd218, %fd242;
	mov.f64 	%fd244, 0dBE31A9E38DC84D60;
	fma.rn.f64 	%fd245, %fd243, %fd218, %fd244;
	mov.f64 	%fd246, 0dBE5F36CD6D3D46A9;
	fma.rn.f64 	%fd247, %fd245, %fd218, %fd246;
	mov.f64 	%fd248, 0d3E9C6B4F5D03B787;
	fma.rn.f64 	%fd249, %fd247, %fd218, %fd248;
	mov.f64 	%fd250, 0dBEB6E8A5434AE8A2;
	fma.rn.f64 	%fd251, %fd249, %fd218, %fd250;
	mov.f64 	%fd252, 0dBEED1D1F7B8736F6;
	fma.rn.f64 	%fd253, %fd251, %fd218, %fd252;
	mov.f64 	%fd254, 0d3F2879C2A212F024;
	fma.rn.f64 	%fd255, %fd253, %fd218, %fd254;
	mov.f64 	%fd256, 0dBF4845769484FCA8;
	fma.rn.f64 	%fd257, %fd255, %fd218, %fd256;
	mov.f64 	%fd258, 0dBF78B6C33114F909;
	fma.rn.f64 	%fd259, %fd257, %fd218, %fd258;
	mov.f64 	%fd260, 0d3FCEBD80D9B13E28;
	fma.rn.f64 	%fd261, %fd259, %fd218, %fd260;
	mov.f64 	%fd262, 0d3FFA755E7C99AE86;
	fma.rn.f64 	%fd263, %fd261, %fd218, %fd262;
	fma.rn.f64 	%fd272, %fd263, %fd2, %fd263;
	bra.uni 	BB57_17;

BB57_2:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	setp.gt.s32	%p5, %r2, 1072693247;
	selp.f64	%fd4, %fd3, %fd1, %p5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r58}, %fd4;
	}
	mov.b32 	 %f1, %r58;
	setp.ltu.f32	%p6, %f1, 0f2B2BFF2F;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r59, %temp}, %fd4;
	}
	@%p6 bra 	BB57_4;
	bra.uni 	BB57_3;

BB57_4:
	setp.gt.f64	%p7, %fd4, 0d0000000000000000;
	setp.lt.s32	%p8, %r58, 2146435072;
	and.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB57_9;
	bra.uni 	BB57_5;

BB57_9:
	mov.u32 	%r60, -1023;
	setp.gt.s32	%p13, %r58, 1048575;
	@%p13 bra 	BB57_11;

	mul.f64 	%fd106, %fd4, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r58}, %fd106;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r59, %temp}, %fd106;
	}
	mov.u32 	%r60, -1077;

BB57_11:
	shr.u32 	%r31, %r58, 20;
	add.s32 	%r61, %r60, %r31;
	and.b32  	%r32, %r58, -2146435073;
	or.b32  	%r33, %r32, 1072693248;
	mov.b64 	%fd269, {%r59, %r33};
	setp.lt.s32	%p14, %r33, 1073127583;
	@%p14 bra 	BB57_13;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd269;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd269;
	}
	add.s32 	%r36, %r35, -1048576;
	mov.b64 	%fd269, {%r34, %r36};
	add.s32 	%r61, %r61, 1;

BB57_13:
	add.f64 	%fd108, %fd269, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd107,%fd108;
	// inline asm
	neg.f64 	%fd109, %fd108;
	mov.f64 	%fd110, 0d3FF0000000000000;
	fma.rn.f64 	%fd111, %fd109, %fd107, %fd110;
	fma.rn.f64 	%fd112, %fd111, %fd111, %fd111;
	fma.rn.f64 	%fd113, %fd112, %fd107, %fd107;
	add.f64 	%fd114, %fd269, 0dBFF0000000000000;
	mul.f64 	%fd115, %fd114, %fd113;
	fma.rn.f64 	%fd116, %fd114, %fd113, %fd115;
	mul.f64 	%fd117, %fd116, %fd116;
	mov.f64 	%fd118, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd119, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd120, %fd119, %fd117, %fd118;
	mov.f64 	%fd121, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd122, %fd120, %fd117, %fd121;
	mov.f64 	%fd123, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd124, %fd122, %fd117, %fd123;
	mov.f64 	%fd125, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd126, %fd124, %fd117, %fd125;
	mov.f64 	%fd127, 0d3F624924923BE72D;
	fma.rn.f64 	%fd128, %fd126, %fd117, %fd127;
	mov.f64 	%fd129, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd130, %fd128, %fd117, %fd129;
	mov.f64 	%fd131, 0d3FB5555555555554;
	fma.rn.f64 	%fd132, %fd130, %fd117, %fd131;
	sub.f64 	%fd133, %fd114, %fd116;
	add.f64 	%fd134, %fd133, %fd133;
	neg.f64 	%fd135, %fd116;
	fma.rn.f64 	%fd136, %fd135, %fd114, %fd134;
	mul.f64 	%fd137, %fd113, %fd136;
	mul.f64 	%fd138, %fd117, %fd132;
	fma.rn.f64 	%fd139, %fd138, %fd116, %fd137;
	xor.b32  	%r37, %r61, -2147483648;
	mov.u32 	%r38, 1127219200;
	mov.b64 	%fd140, {%r37, %r38};
	mov.u32 	%r39, -2147483648;
	mov.b64 	%fd141, {%r39, %r38};
	sub.f64 	%fd142, %fd140, %fd141;
	mov.f64 	%fd143, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd144, %fd142, %fd143, %fd116;
	neg.f64 	%fd145, %fd142;
	fma.rn.f64 	%fd146, %fd145, %fd143, %fd144;
	sub.f64 	%fd147, %fd146, %fd116;
	sub.f64 	%fd148, %fd139, %fd147;
	mov.f64 	%fd149, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd150, %fd142, %fd149, %fd148;
	add.f64 	%fd270, %fd144, %fd150;
	bra.uni 	BB57_14;

BB57_3:
	shr.u32 	%r22, %r58, 20;
	and.b32  	%r23, %r22, 2046;
	add.s32 	%r24, %r23, 2147482626;
	mov.u32 	%r25, 1127219200;
	mov.b64 	%fd24, {%r24, %r25};
	mov.u32 	%r26, -2147483648;
	mov.b64 	%fd25, {%r26, %r25};
	sub.f64 	%fd26, %fd24, %fd25;
	and.b32  	%r27, %r58, -2145386497;
	add.s32 	%r28, %r27, 1071644672;
	mov.b64 	%fd27, {%r59, %r28};
	add.f64 	%fd28, %fd27, 0dBFF0000000000000;
	add.f64 	%fd21, %fd27, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd20,%fd21;
	// inline asm
	neg.f64 	%fd29, %fd21;
	mov.f64 	%fd30, 0d3FF0000000000000;
	fma.rn.f64 	%fd31, %fd29, %fd20, %fd30;
	fma.rn.f64 	%fd32, %fd31, %fd31, %fd31;
	fma.rn.f64 	%fd33, %fd32, %fd20, %fd20;
	mul.f64 	%fd34, %fd28, %fd33;
	mov.f64 	%fd35, 0dC000000000000000;
	fma.rn.f64 	%fd36, %fd35, %fd34, %fd28;
	neg.f64 	%fd37, %fd34;
	fma.rn.f64 	%fd38, %fd37, %fd28, %fd36;
	fma.rn.f64 	%fd39, %fd38, %fd33, %fd34;
	mul.f64 	%fd40, %fd39, %fd39;
	mov.f64 	%fd41, 0d3FA55CF59CDC5D89;
	mov.f64 	%fd42, 0d3FB5C5C218C775C9;
	fma.rn.f64 	%fd43, %fd42, %fd40, %fd41;
	mov.f64 	%fd44, 0d3FAEFD18CF6EBB9C;
	fma.rn.f64 	%fd45, %fd43, %fd40, %fd44;
	mov.f64 	%fd46, 0d3FB10682EDCB8D1B;
	fma.rn.f64 	%fd47, %fd45, %fd40, %fd46;
	mov.f64 	%fd48, 0d3FB3B1DD3AC7FC96;
	fma.rn.f64 	%fd49, %fd47, %fd40, %fd48;
	mov.f64 	%fd50, 0d3FB745CB459B54A6;
	fma.rn.f64 	%fd51, %fd49, %fd40, %fd50;
	mov.f64 	%fd52, 0d3FBC71C741A0669F;
	fma.rn.f64 	%fd53, %fd51, %fd40, %fd52;
	mov.f64 	%fd54, 0d3FC249249209112E;
	fma.rn.f64 	%fd55, %fd53, %fd40, %fd54;
	mov.f64 	%fd56, 0d3FC99999999A06C1;
	fma.rn.f64 	%fd57, %fd55, %fd40, %fd56;
	mov.f64 	%fd58, 0d3FD5555555555535;
	fma.rn.f64 	%fd59, %fd57, %fd40, %fd58;
	mul.f64 	%fd60, %fd40, %fd59;
	fma.rn.f64 	%fd61, %fd60, %fd39, %fd39;
	add.f64 	%fd62, %fd61, %fd61;
	mov.f64 	%fd63, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd64, %fd26, %fd63, %fd62;
	neg.f64 	%fd23, %fd64;
	// inline asm
	rsqrt.approx.ftz.f64 %fd22, %fd23;
	// inline asm
	mul.rn.f64 	%fd65, %fd22, %fd22;
	neg.f64 	%fd66, %fd65;
	fma.rn.f64 	%fd67, %fd23, %fd66, %fd30;
	mov.f64 	%fd68, 0d3FE0000000000000;
	mov.f64 	%fd69, 0d3FD8000000000000;
	fma.rn.f64 	%fd70, %fd69, %fd67, %fd68;
	mul.rn.f64 	%fd71, %fd67, %fd22;
	fma.rn.f64 	%fd72, %fd70, %fd71, %fd22;
	mov.f64 	%fd73, 0d4000A0E7333839AA;
	mov.f64 	%fd74, 0d3FEBE9222591AFAB;
	fma.rn.f64 	%fd75, %fd74, %fd72, %fd73;
	mov.f64 	%fd76, 0d4008768CF7E57D5C;
	fma.rn.f64 	%fd77, %fd75, %fd72, %fd76;
	mov.f64 	%fd78, 0d400B77E7E28DA583;
	fma.rn.f64 	%fd79, %fd77, %fd72, %fd78;
	mov.f64 	%fd80, 0d3FF34F26A4F99CF9;
	fma.rn.f64 	%fd81, %fd79, %fd72, %fd80;
	mov.f64 	%fd82, 0d3FC1F674ADB019ED;
	fma.rn.f64 	%fd83, %fd81, %fd72, %fd82;
	mov.f64 	%fd84, 0d3F75DDAE9506431D;
	fma.rn.f64 	%fd85, %fd83, %fd72, %fd84;
	mov.f64 	%fd86, 0d3F0ADA49AA32489C;
	fma.rn.f64 	%fd87, %fd85, %fd72, %fd86;
	add.f64 	%fd88, %fd72, 0d4001E90FF51C2197;
	mov.f64 	%fd89, 0d40111EA3A7CF3820;
	fma.rn.f64 	%fd90, %fd88, %fd72, %fd89;
	mov.f64 	%fd91, 0d4011A0E4A4749594;
	fma.rn.f64 	%fd92, %fd90, %fd72, %fd91;
	mov.f64 	%fd93, 0d400D4E977D38C14D;
	fma.rn.f64 	%fd94, %fd92, %fd72, %fd93;
	mov.f64 	%fd95, 0d3FF37FD567EC0D5F;
	fma.rn.f64 	%fd96, %fd94, %fd72, %fd95;
	mov.f64 	%fd97, 0d3FC1FB9D7F676033;
	fma.rn.f64 	%fd98, %fd96, %fd72, %fd97;
	mov.f64 	%fd99, 0d3F75DDCDF98946E4;
	fma.rn.f64 	%fd100, %fd98, %fd72, %fd99;
	mov.f64 	%fd101, 0d3F0ADA42D79D8DBB;
	fma.rn.f64 	%fd102, %fd100, %fd72, %fd101;
	mul.f64 	%fd103, %fd72, %fd102;
	div.rn.f64 	%fd271, %fd87, %fd103;
	bra.uni 	BB57_15;

BB57_5:
	abs.f64 	%fd104, %fd4;
	setp.gtu.f64	%p10, %fd104, 0d7FF0000000000000;
	@%p10 bra 	BB57_8;
	bra.uni 	BB57_6;

BB57_8:
	add.f64 	%fd270, %fd4, %fd4;
	bra.uni 	BB57_14;

BB57_6:
	setp.eq.f64	%p11, %fd4, 0d0000000000000000;
	mov.f64 	%fd270, 0dFFF0000000000000;
	@%p11 bra 	BB57_14;

	setp.eq.f64	%p12, %fd4, 0d7FF0000000000000;
	selp.f64	%fd270, %fd4, 0dFFF8000000000000, %p12;

BB57_14:
	neg.f64 	%fd151, %fd270;
	rsqrt.approx.f64 	%fd152, %fd151;
	mov.f64 	%fd153, 0d3FFA2013964E259C;
	mov.f64 	%fd154, 0d3FE8E2101C71B0BF;
	fma.rn.f64 	%fd155, %fd154, %fd152, %fd153;
	mov.f64 	%fd156, 0d3FDABFE90921BE68;
	fma.rn.f64 	%fd157, %fd155, %fd152, %fd156;
	mov.f64 	%fd158, 0d3F97E41314DE00D4;
	fma.rn.f64 	%fd159, %fd157, %fd152, %fd158;
	mov.f64 	%fd160, 0d3F311BD487102E94;
	fma.rn.f64 	%fd161, %fd159, %fd152, %fd160;
	add.f64 	%fd162, %fd152, 0d3FF59895C30BAA54;
	mov.f64 	%fd163, 0d3FFAE8E5956A143F;
	fma.rn.f64 	%fd164, %fd162, %fd152, %fd163;
	mov.f64 	%fd165, 0d3FDACCE85FF7383D;
	fma.rn.f64 	%fd166, %fd164, %fd152, %fd165;
	mov.f64 	%fd167, 0d3F97E43B6CAC34FE;
	fma.rn.f64 	%fd168, %fd166, %fd152, %fd167;
	mov.f64 	%fd169, 0d3F311BD08289EB12;
	fma.rn.f64 	%fd170, %fd168, %fd152, %fd169;
	mul.f64 	%fd171, %fd152, %fd170;
	div.rn.f64 	%fd271, %fd161, %fd171;

BB57_15:
	neg.f64 	%fd172, %fd271;
	selp.f64	%fd272, %fd172, %fd271, %p5;

BB57_17:
	mov.u32 	%r57, %tid.x;
	mov.u32 	%r56, %ctaid.x;
	mov.u32 	%r55, %ntid.x;
	mad.lo.s32 	%r54, %r55, %r56, %r57;
	shl.b32 	%r53, %r54, 3;
	ld.param.u32 	%r52, [vec_normcdfinv_param_1];
	cvta.to.global.u32 	%r49, %r52;
	mul.f64 	%fd264, %fd272, 0dBCA21165F626CDD5;
	mov.f64 	%fd265, 0dBFF6A09E667F3BCC;
	fma.rn.f64 	%fd266, %fd265, %fd272, %fd264;
	mov.f64 	%fd267, 0d0000000000000000;
	add.rn.f64 	%fd268, %fd266, %fd267;
	add.s32 	%r51, %r49, %r53;
	st.global.f64 	[%r51], %fd268;

BB57_18:
	ret;
}

	// .globl	vec_rcbrt
.visible .entry vec_rcbrt(
	.param .u32 vec_rcbrt_param_0,
	.param .u32 vec_rcbrt_param_1,
	.param .u32 vec_rcbrt_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<31>;


	ld.param.u32 	%r15, [vec_rcbrt_param_0];
	ld.param.u32 	%r13, [vec_rcbrt_param_1];
	ld.param.u32 	%r14, [vec_rcbrt_param_2];
	mov.u32 	%r16, %tid.x;
	mov.u32 	%r17, %ntid.x;
	mov.u32 	%r18, %ctaid.x;
	mad.lo.s32 	%r1, %r17, %r18, %r16;
	setp.ge.u32	%p1, %r1, %r15;
	@%p1 bra 	BB58_7;

	cvta.to.global.u32 	%r19, %r14;
	shl.b32 	%r20, %r1, 3;
	add.s32 	%r21, %r19, %r20;
	ld.global.f64 	%fd1, [%r21];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r40, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd1;
	}
	and.b32  	%r41, %r3, 2147483647;
	setp.neu.f64	%p2, %fd1, 0d0000000000000000;
	setp.lt.u32	%p3, %r41, 2146435072;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB58_3;
	bra.uni 	BB58_2;

BB58_3:
	shr.u32 	%r42, %r41, 20;
	mov.u32 	%r43, 0;
	setp.ne.s32	%p6, %r42, 0;
	@%p6 bra 	BB58_5;

	mov.b64 	%fd8, {%r40, %r41};
	mul.f64 	%fd9, %fd8, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r40, %temp}, %fd9;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r41}, %fd9;
	}
	shr.u32 	%r42, %r41, 20;
	mov.u32 	%r43, 18;

BB58_5:
	add.s32 	%r25, %r42, -1022;
	cvt.rn.f32.s32	%f5, %r25;
	mul.f32 	%f6, %f5, 0f3EAAAAAB;
	cvt.rni.s32.f32	%r26, %f6;
	mad.lo.s32 	%r27, %r26, -3145728, %r41;
	mov.b64 	%fd10, {%r40, %r27};
	cvt.rn.f32.f64	%f2, %fd10;
	// inline asm
	lg2.approx.ftz.f32 %f1,%f2;
	// inline asm
	mul.f32 	%f4, %f1, 0fBEAAAAAB;
	// inline asm
	ex2.approx.ftz.f32 %f3,%f4;
	// inline asm
	cvt.f64.f32	%fd11, %f3;
	mul.rn.f64 	%fd12, %fd11, %fd11;
	mul.rn.f64 	%fd13, %fd10, %fd11;
	neg.f64 	%fd14, %fd13;
	mov.f64 	%fd15, 0d3FF0000000000000;
	fma.rn.f64 	%fd16, %fd12, %fd14, %fd15;
	fma.rn.f64 	%fd17, %fd10, %fd11, %fd14;
	neg.f64 	%fd18, %fd17;
	fma.rn.f64 	%fd19, %fd12, %fd18, %fd16;
	neg.f64 	%fd20, %fd12;
	fma.rn.f64 	%fd21, %fd11, %fd11, %fd20;
	neg.f64 	%fd22, %fd21;
	fma.rn.f64 	%fd23, %fd13, %fd22, %fd19;
	mov.f64 	%fd24, 0d3FD5555555555555;
	mov.f64 	%fd25, 0d3FCC71C71C71C71C;
	fma.rn.f64 	%fd26, %fd25, %fd23, %fd24;
	mul.rn.f64 	%fd27, %fd23, %fd11;
	fma.rn.f64 	%fd28, %fd26, %fd27, %fd11;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r28, %temp}, %fd28;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r29}, %fd28;
	}
	sub.s32 	%r30, %r43, %r26;
	shl.b32 	%r31, %r30, 20;
	add.s32 	%r32, %r29, %r31;
	mov.b64 	%fd30, {%r28, %r32};
	bra.uni 	BB58_6;

BB58_2:
	xor.b32  	%r22, %r41, 2146435072;
	mov.b64 	%fd5, {%r40, %r22};
	abs.f64 	%fd6, %fd1;
	setp.gtu.f64	%p5, %fd6, 0d7FF0000000000000;
	add.f64 	%fd7, %fd1, %fd1;
	selp.f64	%fd30, %fd7, %fd5, %p5;

BB58_6:
	cvta.to.global.u32 	%r33, %r13;
	and.b32  	%r34, %r3, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd30;
	}
	or.b32  	%r36, %r35, %r34;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r37, %temp}, %fd30;
	}
	mov.b64 	%fd29, {%r37, %r36};
	add.s32 	%r39, %r33, %r20;
	st.global.f64 	[%r39], %fd29;

BB58_7:
	ret;
}

	// .globl	vec_rint
.visible .entry vec_rint(
	.param .u32 vec_rint_param_0,
	.param .u32 vec_rint_param_1,
	.param .u32 vec_rint_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<3>;


	ld.param.u32 	%r4, [vec_rint_param_0];
	ld.param.u32 	%r2, [vec_rint_param_1];
	ld.param.u32 	%r3, [vec_rint_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB59_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd1, [%r10];
	cvt.rni.f64.f64	%fd2, %fd1;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd2;

BB59_2:
	ret;
}

	// .globl	vec_round
.visible .entry vec_round(
	.param .u32 vec_round_param_0,
	.param .u32 vec_round_param_1,
	.param .u32 vec_round_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<19>;
	.reg .f64 	%fd<9>;


	ld.param.u32 	%r4, [vec_round_param_0];
	ld.param.u32 	%r2, [vec_round_param_1];
	ld.param.u32 	%r3, [vec_round_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB60_4;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd8, [%r10];
	abs.f64 	%fd2, %fd8;
	setp.ge.f64	%p2, %fd2, 0d4330000000000000;
	@%p2 bra 	BB60_3;

	add.f64 	%fd5, %fd2, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd6, %fd5;
	setp.lt.f64	%p3, %fd2, 0d3FE0000000000000;
	selp.f64	%fd7, 0d0000000000000000, %fd6, %p3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r11, %temp}, %fd7;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd7;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd8;
	}
	and.b32  	%r14, %r13, -2147483648;
	or.b32  	%r15, %r12, %r14;
	mov.b64 	%fd8, {%r11, %r15};

BB60_3:
	cvta.to.global.u32 	%r16, %r2;
	add.s32 	%r18, %r16, %r9;
	st.global.f64 	[%r18], %fd8;

BB60_4:
	ret;
}

	// .globl	vec_rsqrt
.visible .entry vec_rsqrt(
	.param .u32 vec_rsqrt_param_0,
	.param .u32 vec_rsqrt_param_1,
	.param .u32 vec_rsqrt_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<3>;


	ld.param.u32 	%r4, [vec_rsqrt_param_0];
	ld.param.u32 	%r2, [vec_rsqrt_param_1];
	ld.param.u32 	%r3, [vec_rsqrt_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB61_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd1, [%r10];
	rsqrt.approx.f64 	%fd2, %fd1;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd2;

BB61_2:
	ret;
}

	// .globl	vec_sin
.visible .entry vec_sin(
	.param .u32 vec_sin_param_0,
	.param .u32 vec_sin_param_1,
	.param .u32 vec_sin_param_2
)
{
	.local .align 4 .b8 	__local_depot62[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<29>;
	.reg .f64 	%fd<42>;


	mov.u32 	%r28, __local_depot62;
	cvta.local.u32 	%SP, %r28;
	ld.param.u32 	%r8, [vec_sin_param_0];
	ld.param.u32 	%r6, [vec_sin_param_1];
	ld.param.u32 	%r7, [vec_sin_param_2];
	add.u32 	%r9, %SP, 0;
	cvta.to.local.u32 	%r1, %r9;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r2, %r10, %r11, %r12;
	setp.ge.u32	%p1, %r2, %r8;
	@%p1 bra 	BB62_10;

	cvta.to.global.u32 	%r13, %r7;
	shl.b32 	%r14, %r2, 3;
	add.s32 	%r15, %r13, %r14;
	ld.global.f64 	%fd39, [%r15];
	abs.f64 	%fd14, %fd39;
	setp.neu.f64	%p2, %fd14, 0d7FF0000000000000;
	@%p2 bra 	BB62_3;

	mov.f64 	%fd15, 0d0000000000000000;
	mul.rn.f64 	%fd39, %fd39, %fd15;

BB62_3:
	mul.f64 	%fd16, %fd39, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r27, %fd16;
	st.local.u32 	[%r1], %r27;
	cvt.rn.f64.s32	%fd17, %r27;
	neg.f64 	%fd18, %fd17;
	mov.f64 	%fd19, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd20, %fd18, %fd19, %fd39;
	mov.f64 	%fd21, 0d3C91A62633145C00;
	fma.rn.f64 	%fd22, %fd18, %fd21, %fd20;
	mov.f64 	%fd23, 0d397B839A252049C0;
	fma.rn.f64 	%fd40, %fd18, %fd23, %fd22;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd39;
	}
	and.b32  	%r17, %r16, 2145386496;
	setp.lt.u32	%p3, %r17, 1105199104;
	@%p3 bra 	BB62_5;

	// Callseq Start 6
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd39;
	.param .b32 param1;
	st.param.b32	[param1+0], %r9;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd40, [retval0+0];
	
	//{
	}// Callseq End 6
	ld.local.u32 	%r27, [%r1];

BB62_5:
	and.b32  	%r19, %r27, 1;
	setp.eq.s32	%p4, %r19, 0;
	selp.f64	%fd24, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p4;
	shl.b32 	%r20, %r19, 6;
	mov.u32 	%r21, __cudart_sin_cos_coeffs;
	add.s32 	%r22, %r20, %r21;
	ld.const.f64 	%fd25, [%r22+8];
	mul.rn.f64 	%fd7, %fd40, %fd40;
	fma.rn.f64 	%fd26, %fd24, %fd7, %fd25;
	ld.const.f64 	%fd27, [%r22+16];
	fma.rn.f64 	%fd28, %fd26, %fd7, %fd27;
	ld.const.f64 	%fd29, [%r22+24];
	fma.rn.f64 	%fd30, %fd28, %fd7, %fd29;
	ld.const.f64 	%fd31, [%r22+32];
	fma.rn.f64 	%fd32, %fd30, %fd7, %fd31;
	ld.const.f64 	%fd33, [%r22+40];
	fma.rn.f64 	%fd34, %fd32, %fd7, %fd33;
	ld.const.f64 	%fd35, [%r22+48];
	fma.rn.f64 	%fd8, %fd34, %fd7, %fd35;
	fma.rn.f64 	%fd41, %fd8, %fd40, %fd40;
	@%p4 bra 	BB62_7;

	mov.f64 	%fd36, 0d3FF0000000000000;
	fma.rn.f64 	%fd41, %fd8, %fd7, %fd36;

BB62_7:
	and.b32  	%r23, %r27, 2;
	setp.eq.s32	%p5, %r23, 0;
	@%p5 bra 	BB62_9;

	mov.f64 	%fd37, 0d0000000000000000;
	mov.f64 	%fd38, 0dBFF0000000000000;
	fma.rn.f64 	%fd41, %fd41, %fd38, %fd37;

BB62_9:
	cvta.to.global.u32 	%r24, %r6;
	add.s32 	%r26, %r24, %r14;
	st.global.f64 	[%r26], %fd41;

BB62_10:
	ret;
}

	// .globl	vec_sinh
.visible .entry vec_sinh(
	.param .u32 vec_sinh_param_0,
	.param .u32 vec_sinh_param_1,
	.param .u32 vec_sinh_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<32>;
	.reg .f64 	%fd<69>;


	ld.param.u32 	%r5, [vec_sinh_param_0];
	ld.param.u32 	%r3, [vec_sinh_param_1];
	ld.param.u32 	%r4, [vec_sinh_param_2];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB63_5;

	cvta.to.global.u32 	%r9, %r4;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	ld.global.f64 	%fd5, [%r11];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd5;
	}
	and.b32  	%r12, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd5;
	}
	mov.b64 	%fd1, {%r13, %r12};
	setp.lt.u32	%p2, %r12, 1072693248;
	@%p2 bra 	BB63_3;
	bra.uni 	BB63_2;

BB63_3:
	mul.f64 	%fd52, %fd1, %fd1;
	mov.f64 	%fd53, 0d3DE611A561D87DEF;
	mov.f64 	%fd54, 0d3D6B4C75AB274C53;
	fma.rn.f64 	%fd55, %fd54, %fd52, %fd53;
	mov.f64 	%fd56, 0d3E5AE64671B18F5C;
	fma.rn.f64 	%fd57, %fd55, %fd52, %fd56;
	mov.f64 	%fd58, 0d3EC71DE3A465B1E4;
	fma.rn.f64 	%fd59, %fd57, %fd52, %fd58;
	mov.f64 	%fd60, 0d3F2A01A01A02899D;
	fma.rn.f64 	%fd61, %fd59, %fd52, %fd60;
	mov.f64 	%fd62, 0d3F811111111110A6;
	fma.rn.f64 	%fd63, %fd61, %fd52, %fd62;
	mov.f64 	%fd64, 0d3FC5555555555556;
	fma.rn.f64 	%fd65, %fd63, %fd52, %fd64;
	mul.f64 	%fd66, %fd52, %fd65;
	fma.rn.f64 	%fd68, %fd66, %fd1, %fd1;
	bra.uni 	BB63_4;

BB63_2:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd1;
	}
	mov.f64 	%fd6, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd7, %fd1, %fd6;
	mov.f64 	%fd8, 0d4338000000000000;
	add.rn.f64 	%fd9, %fd7, %fd8;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r15, %temp}, %fd9;
	}
	add.s32 	%r16, %r15, -1;
	mov.f64 	%fd10, 0dC338000000000000;
	add.rn.f64 	%fd11, %fd9, %fd10;
	mov.f64 	%fd12, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd13, %fd11, %fd12, %fd1;
	mov.f64 	%fd14, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd15, %fd11, %fd14, %fd13;
	add.s32 	%r17, %r14, %r14;
	setp.lt.u32	%p3, %r17, 2142496327;
	selp.b32	%r18, 0, %r16, %p3;
	selp.f64	%fd16, %fd1, %fd15, %p3;
	mov.f64 	%fd17, 0d3E5AF86D8EBD13CD;
	mov.f64 	%fd18, 0d3E21F4076ACD15B6;
	fma.rn.f64 	%fd19, %fd18, %fd16, %fd17;
	mov.f64 	%fd20, 0d3E927E5092BA033D;
	fma.rn.f64 	%fd21, %fd19, %fd16, %fd20;
	mov.f64 	%fd22, 0d3EC71DDE6C5F9DA1;
	fma.rn.f64 	%fd23, %fd21, %fd16, %fd22;
	mov.f64 	%fd24, 0d3EFA01A018D034E6;
	fma.rn.f64 	%fd25, %fd23, %fd16, %fd24;
	mov.f64 	%fd26, 0d3F2A01A01B3B6940;
	fma.rn.f64 	%fd27, %fd25, %fd16, %fd26;
	mov.f64 	%fd28, 0d3F56C16C16C1B5DD;
	fma.rn.f64 	%fd29, %fd27, %fd16, %fd28;
	mov.f64 	%fd30, 0d3F8111111110F74D;
	fma.rn.f64 	%fd31, %fd29, %fd16, %fd30;
	mov.f64 	%fd32, 0d3FA555555555554D;
	fma.rn.f64 	%fd33, %fd31, %fd16, %fd32;
	mov.f64 	%fd34, 0d3FC5555555555557;
	fma.rn.f64 	%fd35, %fd33, %fd16, %fd34;
	mov.f64 	%fd36, 0d3FE0000000000000;
	fma.rn.f64 	%fd37, %fd35, %fd16, %fd36;
	mul.f64 	%fd38, %fd16, %fd37;
	fma.rn.f64 	%fd39, %fd38, %fd16, %fd16;
	setp.eq.s32	%p4, %r18, 1024;
	selp.b32	%r19, -1, 0, %p4;
	add.s32 	%r20, %r19, %r18;
	shl.b32 	%r21, %r20, 20;
	add.s32 	%r22, %r21, 1072693248;
	mov.u32 	%r23, 0;
	mov.b64 	%fd40, {%r23, %r22};
	mov.u32 	%r24, 1071644672;
	mov.b64 	%fd41, {%r23, %r24};
	sub.f64 	%fd42, %fd40, %fd41;
	fma.rn.f64 	%fd43, %fd39, %fd40, %fd42;
	add.f64 	%fd44, %fd43, %fd43;
	selp.f64	%fd45, %fd44, %fd43, %p4;
	setp.eq.s32	%p5, %r17, 0;
	selp.f64	%fd46, %fd16, %fd45, %p5;
	mov.f64 	%fd47, 0d3FF0000000000000;
	mov.f64 	%fd48, 0d4000000000000000;
	fma.rn.f64 	%fd49, %fd48, %fd46, %fd47;
	div.rn.f64 	%fd50, %fd46, %fd49;
	add.f64 	%fd51, %fd50, %fd46;
	setp.ltu.f64	%p6, %fd1, 0d408633CE8FB9F87E;
	selp.f64	%fd68, %fd51, 0d7FF0000000000000, %p6;

BB63_4:
	cvta.to.global.u32 	%r25, %r3;
	and.b32  	%r26, %r2, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r27}, %fd68;
	}
	or.b32  	%r28, %r27, %r26;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r29, %temp}, %fd68;
	}
	mov.b64 	%fd67, {%r29, %r28};
	add.s32 	%r31, %r25, %r10;
	st.global.f64 	[%r31], %fd67;

BB63_5:
	ret;
}

	// .globl	vec_sinpi
.visible .entry vec_sinpi(
	.param .u32 vec_sinpi_param_0,
	.param .u32 vec_sinpi_param_1,
	.param .u32 vec_sinpi_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<37>;
	.reg .b64 	%rd<2>;


	ld.param.u32 	%r5, [vec_sinpi_param_0];
	ld.param.u32 	%r3, [vec_sinpi_param_1];
	ld.param.u32 	%r4, [vec_sinpi_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB64_8;

	cvta.to.global.u32 	%r9, %r4;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	ld.global.f64 	%fd1, [%r11];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r12, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd1;
	}
	add.s32 	%r14, %r13, 1048576;
	mov.b64 	%fd11, {%r12, %r14};
	cvt.rni.f64.f64	%fd12, %fd11;
	cvt.rzi.s64.f64	%rd1, %fd12;
	cvt.u32.u64	%r2, %rd1;
	neg.f64 	%fd13, %fd12;
	mov.f64 	%fd14, 0d3FE0000000000000;
	fma.rn.f64 	%fd15, %fd13, %fd14, %fd1;
	mul.f64 	%fd16, %fd15, 0d3CA1A62633145C07;
	mov.f64 	%fd17, 0d400921FB54442D18;
	fma.rn.f64 	%fd18, %fd15, %fd17, %fd16;
	and.b32  	%r15, %r2, 1;
	mul.rn.f64 	%fd2, %fd18, %fd18;
	setp.eq.s32	%p2, %r15, 0;
	selp.f64	%fd19, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p2;
	shl.b32 	%r16, %r15, 6;
	mov.u32 	%r17, __cudart_sin_cos_coeffs;
	add.s32 	%r18, %r16, %r17;
	ld.const.f64 	%fd20, [%r18+8];
	fma.rn.f64 	%fd21, %fd19, %fd2, %fd20;
	ld.const.f64 	%fd22, [%r18+16];
	fma.rn.f64 	%fd23, %fd21, %fd2, %fd22;
	ld.const.f64 	%fd24, [%r18+24];
	fma.rn.f64 	%fd25, %fd23, %fd2, %fd24;
	ld.const.f64 	%fd26, [%r18+32];
	fma.rn.f64 	%fd27, %fd25, %fd2, %fd26;
	ld.const.f64 	%fd28, [%r18+40];
	fma.rn.f64 	%fd29, %fd27, %fd2, %fd28;
	ld.const.f64 	%fd30, [%r18+48];
	fma.rn.f64 	%fd3, %fd29, %fd2, %fd30;
	fma.rn.f64 	%fd36, %fd3, %fd18, %fd18;
	@%p2 bra 	BB64_3;

	mov.f64 	%fd31, 0d3FF0000000000000;
	fma.rn.f64 	%fd36, %fd3, %fd2, %fd31;

BB64_3:
	and.b32  	%r19, %r2, 2;
	setp.eq.s32	%p3, %r19, 0;
	@%p3 bra 	BB64_5;

	mov.f64 	%fd32, 0d0000000000000000;
	mov.f64 	%fd33, 0dBFF0000000000000;
	fma.rn.f64 	%fd36, %fd36, %fd33, %fd32;

BB64_5:
	cvt.rzi.f64.f64	%fd34, %fd1;
	setp.neu.f64	%p4, %fd1, %fd34;
	@%p4 bra 	BB64_7;

	mov.f64 	%fd35, 0d0000000000000000;
	mul.rn.f64 	%fd36, %fd1, %fd35;

BB64_7:
	cvta.to.global.u32 	%r20, %r3;
	add.s32 	%r22, %r20, %r10;
	st.global.f64 	[%r22], %fd36;

BB64_8:
	ret;
}

	// .globl	vec_sqrt
.visible .entry vec_sqrt(
	.param .u32 vec_sqrt_param_0,
	.param .u32 vec_sqrt_param_1,
	.param .u32 vec_sqrt_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<3>;


	ld.param.u32 	%r4, [vec_sqrt_param_0];
	ld.param.u32 	%r2, [vec_sqrt_param_1];
	ld.param.u32 	%r3, [vec_sqrt_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB65_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd1, [%r10];
	sqrt.rn.f64 	%fd2, %fd1;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd2;

BB65_2:
	ret;
}

	// .globl	vec_tan
.visible .entry vec_tan(
	.param .u32 vec_tan_param_0,
	.param .u32 vec_tan_param_1,
	.param .u32 vec_tan_param_2
)
{
	.local .align 4 .b8 	__local_depot66[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<5>;
	.reg .b32 	%r<25>;
	.reg .f64 	%fd<67>;


	mov.u32 	%r24, __local_depot66;
	cvta.local.u32 	%SP, %r24;
	ld.param.u32 	%r8, [vec_tan_param_0];
	ld.param.u32 	%r6, [vec_tan_param_1];
	ld.param.u32 	%r7, [vec_tan_param_2];
	add.u32 	%r9, %SP, 0;
	cvta.to.local.u32 	%r1, %r9;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r2, %r10, %r11, %r12;
	setp.ge.u32	%p1, %r2, %r8;
	@%p1 bra 	BB66_8;

	cvta.to.global.u32 	%r13, %r7;
	shl.b32 	%r14, %r2, 3;
	add.s32 	%r15, %r13, %r14;
	ld.global.f64 	%fd64, [%r15];
	abs.f64 	%fd11, %fd64;
	setp.neu.f64	%p2, %fd11, 0d7FF0000000000000;
	@%p2 bra 	BB66_3;

	mov.f64 	%fd12, 0d0000000000000000;
	mul.rn.f64 	%fd64, %fd64, %fd12;

BB66_3:
	mul.f64 	%fd13, %fd64, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r23, %fd13;
	st.local.u32 	[%r1], %r23;
	cvt.rn.f64.s32	%fd14, %r23;
	neg.f64 	%fd15, %fd14;
	mov.f64 	%fd16, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd17, %fd15, %fd16, %fd64;
	mov.f64 	%fd18, 0d3C91A62633145C00;
	fma.rn.f64 	%fd19, %fd15, %fd18, %fd17;
	mov.f64 	%fd20, 0d397B839A252049C0;
	fma.rn.f64 	%fd65, %fd15, %fd20, %fd19;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd64;
	}
	and.b32  	%r17, %r16, 2145386496;
	setp.lt.u32	%p3, %r17, 1105199104;
	@%p3 bra 	BB66_5;

	// Callseq Start 7
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd64;
	.param .b32 param1;
	st.param.b32	[param1+0], %r9;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd65, [retval0+0];
	
	//{
	}// Callseq End 7
	ld.local.u32 	%r23, [%r1];

BB66_5:
	mul.f64 	%fd21, %fd65, %fd65;
	mov.f64 	%fd22, 0dBEF9757C5B27EBB1;
	mov.f64 	%fd23, 0d3EE48DAC2799BCB9;
	fma.rn.f64 	%fd24, %fd23, %fd21, %fd22;
	mov.f64 	%fd25, 0d3F0980E90FD91E04;
	fma.rn.f64 	%fd26, %fd24, %fd21, %fd25;
	mov.f64 	%fd27, 0dBEFAE2B0417D7E1D;
	fma.rn.f64 	%fd28, %fd26, %fd21, %fd27;
	mov.f64 	%fd29, 0d3F119F5341BFBA57;
	fma.rn.f64 	%fd30, %fd28, %fd21, %fd29;
	mov.f64 	%fd31, 0d3F15E791A00F6919;
	fma.rn.f64 	%fd32, %fd30, %fd21, %fd31;
	mov.f64 	%fd33, 0d3F2FF2E7FADEC73A;
	fma.rn.f64 	%fd34, %fd32, %fd21, %fd33;
	mov.f64 	%fd35, 0d3F434BC1B206DA62;
	fma.rn.f64 	%fd36, %fd34, %fd21, %fd35;
	mov.f64 	%fd37, 0d3F57DB18EF2F83F9;
	fma.rn.f64 	%fd38, %fd36, %fd21, %fd37;
	mov.f64 	%fd39, 0d3F6D6D2E7AE49FBC;
	fma.rn.f64 	%fd40, %fd38, %fd21, %fd39;
	mov.f64 	%fd41, 0d3F8226E3A816A776;
	fma.rn.f64 	%fd42, %fd40, %fd21, %fd41;
	mov.f64 	%fd43, 0d3F9664F485D25660;
	fma.rn.f64 	%fd44, %fd42, %fd21, %fd43;
	mov.f64 	%fd45, 0d3FABA1BA1BABF31D;
	fma.rn.f64 	%fd46, %fd44, %fd21, %fd45;
	mov.f64 	%fd47, 0d3FC11111111105D2;
	fma.rn.f64 	%fd48, %fd46, %fd21, %fd47;
	mov.f64 	%fd49, 0d3FD555555555555E;
	fma.rn.f64 	%fd50, %fd48, %fd21, %fd49;
	mul.f64 	%fd7, %fd21, %fd50;
	fma.rn.f64 	%fd66, %fd7, %fd65, %fd65;
	and.b32  	%r19, %r23, 1;
	setp.eq.b32	%p4, %r19, 1;
	@!%p4 bra 	BB66_7;
	bra.uni 	BB66_6;

BB66_6:
	sub.f64 	%fd53, %fd66, %fd65;
	neg.f64 	%fd54, %fd53;
	fma.rn.f64 	%fd55, %fd7, %fd65, %fd54;
	// inline asm
	rcp.approx.ftz.f64 %fd51,%fd66;
	// inline asm
	neg.f64 	%fd56, %fd66;
	mov.f64 	%fd57, 0d3FF0000000000000;
	fma.rn.f64 	%fd58, %fd56, %fd51, %fd57;
	fma.rn.f64 	%fd59, %fd58, %fd58, %fd58;
	fma.rn.f64 	%fd60, %fd59, %fd51, %fd51;
	neg.f64 	%fd61, %fd60;
	fma.rn.f64 	%fd62, %fd66, %fd61, %fd57;
	fma.rn.f64 	%fd63, %fd61, %fd55, %fd62;
	fma.rn.f64 	%fd66, %fd63, %fd61, %fd61;

BB66_7:
	cvta.to.global.u32 	%r20, %r6;
	add.s32 	%r22, %r20, %r14;
	st.global.f64 	[%r22], %fd66;

BB66_8:
	ret;
}

	// .globl	vec_tanh
.visible .entry vec_tanh(
	.param .u32 vec_tanh_param_0,
	.param .u32 vec_tanh_param_1,
	.param .u32 vec_tanh_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<25>;
	.reg .f64 	%fd<75>;


	ld.param.u32 	%r6, [vec_tanh_param_0];
	ld.param.u32 	%r4, [vec_tanh_param_1];
	ld.param.u32 	%r5, [vec_tanh_param_2];
	mov.u32 	%r7, %tid.x;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r1, %r8, %r9, %r7;
	setp.ge.u32	%p1, %r1, %r6;
	@%p1 bra 	BB67_5;

	cvta.to.global.u32 	%r10, %r5;
	shl.b32 	%r11, %r1, 3;
	add.s32 	%r12, %r10, %r11;
	ld.global.f64 	%fd1, [%r12];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	and.b32  	%r3, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd1;
	}
	mov.b64 	%fd2, {%r13, %r3};
	setp.ltu.f64	%p2, %fd2, 0d3FE1C7A398201CD6;
	@%p2 bra 	BB67_3;
	bra.uni 	BB67_2;

BB67_3:
	mul.f64 	%fd52, %fd1, %fd1;
	mov.f64 	%fd53, 0dBF2B9093D89F0E23;
	mov.f64 	%fd54, 0d3F0ABFFC9B5786C4;
	fma.rn.f64 	%fd55, %fd54, %fd52, %fd53;
	mov.f64 	%fd56, 0d3F42FA2744C30B61;
	fma.rn.f64 	%fd57, %fd55, %fd52, %fd56;
	mov.f64 	%fd58, 0dBF57CF3B9C1E491D;
	fma.rn.f64 	%fd59, %fd57, %fd52, %fd58;
	mov.f64 	%fd60, 0d3F6D6C61D450119A;
	fma.rn.f64 	%fd61, %fd59, %fd52, %fd60;
	mov.f64 	%fd62, 0dBF8226DDD44294F5;
	fma.rn.f64 	%fd63, %fd61, %fd52, %fd62;
	mov.f64 	%fd64, 0d3F9664F45C2B04A6;
	fma.rn.f64 	%fd65, %fd63, %fd52, %fd64;
	mov.f64 	%fd66, 0dBFABA1BA1AD70754;
	fma.rn.f64 	%fd67, %fd65, %fd52, %fd66;
	mov.f64 	%fd68, 0d3FC111111110295E;
	fma.rn.f64 	%fd69, %fd67, %fd52, %fd68;
	mov.f64 	%fd70, 0dBFD555555555549F;
	fma.rn.f64 	%fd71, %fd69, %fd52, %fd70;
	mul.f64 	%fd72, %fd52, %fd71;
	fma.rn.f64 	%fd74, %fd72, %fd1, %fd1;
	bra.uni 	BB67_4;

BB67_2:
	add.f64 	%fd8, %fd2, %fd2;
	mov.f64 	%fd9, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd10, %fd8, %fd9;
	mov.f64 	%fd11, 0d4338000000000000;
	add.rn.f64 	%fd12, %fd10, %fd11;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd12;
	}
	mov.f64 	%fd13, 0dC338000000000000;
	add.rn.f64 	%fd14, %fd12, %fd13;
	mov.f64 	%fd15, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd16, %fd14, %fd15, %fd8;
	mov.f64 	%fd17, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd18, %fd14, %fd17, %fd16;
	mov.f64 	%fd19, 0d3E5AF86D8EBD13CD;
	mov.f64 	%fd20, 0d3E21F4076ACD15B6;
	fma.rn.f64 	%fd21, %fd20, %fd18, %fd19;
	mov.f64 	%fd22, 0d3E927E5092BA033D;
	fma.rn.f64 	%fd23, %fd21, %fd18, %fd22;
	mov.f64 	%fd24, 0d3EC71DDE6C5F9DA1;
	fma.rn.f64 	%fd25, %fd23, %fd18, %fd24;
	mov.f64 	%fd26, 0d3EFA01A018D034E6;
	fma.rn.f64 	%fd27, %fd25, %fd18, %fd26;
	mov.f64 	%fd28, 0d3F2A01A01B3B6940;
	fma.rn.f64 	%fd29, %fd27, %fd18, %fd28;
	mov.f64 	%fd30, 0d3F56C16C16C1B5DD;
	fma.rn.f64 	%fd31, %fd29, %fd18, %fd30;
	mov.f64 	%fd32, 0d3F8111111110F74D;
	fma.rn.f64 	%fd33, %fd31, %fd18, %fd32;
	mov.f64 	%fd34, 0d3FA555555555554D;
	fma.rn.f64 	%fd35, %fd33, %fd18, %fd34;
	mov.f64 	%fd36, 0d3FC5555555555557;
	fma.rn.f64 	%fd37, %fd35, %fd18, %fd36;
	mov.f64 	%fd38, 0d3FE0000000000000;
	fma.rn.f64 	%fd39, %fd37, %fd18, %fd38;
	mul.f64 	%fd40, %fd18, %fd39;
	fma.rn.f64 	%fd41, %fd40, %fd18, %fd18;
	shl.b32 	%r15, %r14, 20;
	add.s32 	%r16, %r15, 1072693248;
	mov.u32 	%r17, 0;
	mov.b64 	%fd42, {%r17, %r16};
	fma.rn.f64 	%fd43, %fd41, %fd42, %fd42;
	add.f64 	%fd7, %fd43, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd6,%fd7;
	// inline asm
	neg.f64 	%fd44, %fd7;
	mov.f64 	%fd45, 0d3FF0000000000000;
	fma.rn.f64 	%fd46, %fd44, %fd6, %fd45;
	fma.rn.f64 	%fd47, %fd46, %fd46, %fd46;
	fma.rn.f64 	%fd48, %fd47, %fd6, %fd6;
	neg.f64 	%fd49, %fd48;
	mov.f64 	%fd50, 0d4000000000000000;
	fma.rn.f64 	%fd51, %fd50, %fd49, %fd45;
	setp.gt.u32	%p3, %r3, 1077936127;
	selp.f64	%fd74, 0d3FF0000000000000, %fd51, %p3;

BB67_4:
	cvta.to.global.u32 	%r18, %r4;
	and.b32  	%r19, %r2, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd74;
	}
	or.b32  	%r21, %r20, %r19;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd74;
	}
	mov.b64 	%fd73, {%r22, %r21};
	add.s32 	%r24, %r18, %r11;
	st.global.f64 	[%r24], %fd73;

BB67_5:
	ret;
}

	// .globl	vec_tgamma
.visible .entry vec_tgamma(
	.param .u32 vec_tgamma_param_0,
	.param .u32 vec_tgamma_param_1,
	.param .u32 vec_tgamma_param_2
)
{
	.reg .pred 	%p<29>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<97>;
	.reg .f64 	%fd<429>;
	.reg .b64 	%rd<2>;


	ld.param.u32 	%r25, [vec_tgamma_param_0];
	ld.param.u32 	%r24, [vec_tgamma_param_2];
	mov.u32 	%r26, %tid.x;
	mov.u32 	%r27, %ntid.x;
	mov.u32 	%r28, %ctaid.x;
	mad.lo.s32 	%r29, %r27, %r28, %r26;
	setp.ge.u32	%p1, %r29, %r25;
	@%p1 bra 	BB68_44;

	cvta.to.global.u32 	%r30, %r24;
	shl.b32 	%r35, %r29, 3;
	add.s32 	%r36, %r30, %r35;
	ld.global.f64 	%fd1, [%r36];
	setp.ltu.f64	%p2, %fd1, 0d0000000000000000;
	@%p2 bra 	BB68_18;
	bra.uni 	BB68_2;

BB68_18:
	setp.lt.f64	%p15, %fd1, 0d0000000000000000;
	@%p15 bra 	BB68_20;
	bra.uni 	BB68_19;

BB68_20:
	cvt.rzi.f64.f64	%fd267, %fd1;
	setp.eq.f64	%p16, %fd1, %fd267;
	mov.f64 	%fd428, 0dFFF8000000000000;
	@%p16 bra 	BB68_43;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd1;
	}
	setp.lt.u32	%p17, %r16, -1072693248;
	@%p17 bra 	BB68_34;
	bra.uni 	BB68_22;

BB68_34:
	setp.lt.u32	%p25, %r16, -1072955392;
	mov.f64 	%fd424, %fd1;
	mov.f64 	%fd423, %fd1;
	@%p25 bra 	BB68_36;

	fma.rn.f64 	%fd424, %fd1, %fd1, %fd1;
	add.f64 	%fd423, %fd1, 0d3FF0000000000000;

BB68_36:
	mov.f64 	%fd421, %fd423;
	mov.f64 	%fd422, %fd424;
	setp.lt.u32	%p26, %r16, -1073479680;
	@%p26 bra 	BB68_38;

	fma.rn.f64 	%fd422, %fd421, %fd422, %fd422;
	add.f64 	%fd421, %fd421, 0d3FF0000000000000;

BB68_38:
	mov.f64 	%fd419, %fd421;
	mov.f64 	%fd420, %fd422;
	setp.lt.u32	%p27, %r16, -1074266112;
	@%p27 bra 	BB68_40;

	fma.rn.f64 	%fd420, %fd419, %fd420, %fd420;
	add.f64 	%fd419, %fd419, 0d3FF0000000000000;

BB68_40:
	mov.f64 	%fd417, %fd419;
	mov.f64 	%fd418, %fd420;
	setp.lt.u32	%p28, %r16, -1075838976;
	@%p28 bra 	BB68_42;

	fma.rn.f64 	%fd418, %fd417, %fd418, %fd418;
	add.f64 	%fd417, %fd417, 0d3FF0000000000000;

BB68_42:
	mov.f64 	%fd369, 0dBE8AF7049AE8A594;
	mov.f64 	%fd370, 0d3E381B4960FCF5C9;
	fma.rn.f64 	%fd371, %fd370, %fd417, %fd369;
	mov.f64 	%fd372, 0d3EB301D46D4B22F5;
	fma.rn.f64 	%fd373, %fd371, %fd417, %fd372;
	mov.f64 	%fd374, 0dBEB50272AEED0FC4;
	fma.rn.f64 	%fd375, %fd373, %fd417, %fd374;
	mov.f64 	%fd376, 0dBEF51CE1A40516F8;
	fma.rn.f64 	%fd377, %fd375, %fd417, %fd376;
	mov.f64 	%fd378, 0d3F20C8AA7419084C;
	fma.rn.f64 	%fd379, %fd377, %fd417, %fd378;
	mov.f64 	%fd380, 0dBF2C3650196BAD8A;
	fma.rn.f64 	%fd381, %fd379, %fd417, %fd380;
	mov.f64 	%fd382, 0dBF531711365A3E26;
	fma.rn.f64 	%fd383, %fd381, %fd417, %fd382;
	mov.f64 	%fd384, 0d3F7D919C52A7DF35;
	fma.rn.f64 	%fd385, %fd383, %fd417, %fd384;
	mov.f64 	%fd386, 0dBF83B4AF28386F4D;
	fma.rn.f64 	%fd387, %fd385, %fd417, %fd386;
	mov.f64 	%fd388, 0dBFA59AF103C37B4D;
	fma.rn.f64 	%fd389, %fd387, %fd417, %fd388;
	mov.f64 	%fd390, 0d3FC5512320B439EF;
	fma.rn.f64 	%fd391, %fd389, %fd417, %fd390;
	mov.f64 	%fd392, 0dBFA5815E8FA26F4F;
	fma.rn.f64 	%fd393, %fd391, %fd417, %fd392;
	mov.f64 	%fd394, 0dBFE4FCF4026AFA2B;
	fma.rn.f64 	%fd395, %fd393, %fd417, %fd394;
	mov.f64 	%fd396, 0d3FE2788CFC6FB619;
	fma.rn.f64 	%fd397, %fd395, %fd417, %fd396;
	mov.f64 	%fd398, 0d3FF0000000000000;
	fma.rn.f64 	%fd399, %fd397, %fd417, %fd398;
	mul.f64 	%fd400, %fd418, %fd399;
	rcp.rn.f64 	%fd428, %fd400;
	bra.uni 	BB68_43;

BB68_2:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r93}, %fd1;
	}
	setp.lt.s32	%p3, %r93, 1074790400;
	@%p3 bra 	BB68_11;
	bra.uni 	BB68_3;

BB68_11:
	mov.f64 	%fd403, 0d3FF0000000000000;
	setp.lt.s32	%p10, %r93, 1074528256;
	mov.f64 	%fd427, %fd1;
	@%p10 bra 	BB68_13;

	mov.f64 	%fd227, 0dBFF0000000000000;
	mov.f64 	%fd228, 0d3FF0000000000000;
	fma.rn.f64 	%fd403, %fd1, %fd228, %fd227;
	add.f64 	%fd13, %fd1, 0dBFF0000000000000;
	mov.f64 	%fd427, %fd13;

BB68_13:
	mov.f64 	%fd409, %fd427;
	mov.f64 	%fd426, %fd409;
	setp.lt.s32	%p11, %r93, 1074003968;
	@%p11 bra 	BB68_15;

	neg.f64 	%fd229, %fd403;
	fma.rn.f64 	%fd403, %fd426, %fd403, %fd229;
	add.f64 	%fd426, %fd426, 0dBFF0000000000000;

BB68_15:
	mov.f64 	%fd425, %fd426;
	setp.lt.s32	%p12, %r93, 1073217536;
	@%p12 bra 	BB68_17;

	neg.f64 	%fd230, %fd403;
	fma.rn.f64 	%fd403, %fd425, %fd403, %fd230;
	add.f64 	%fd425, %fd425, 0dBFF0000000000000;

BB68_17:
	add.f64 	%fd231, %fd425, 0dBFF0000000000000;
	setp.gt.s32	%p13, %r93, 1071644671;
	selp.f64	%fd232, %fd231, %fd425, %p13;
	mov.f64 	%fd233, 0dBE8AF7049AE8A594;
	mov.f64 	%fd234, 0d3E381B4960FCF5C9;
	fma.rn.f64 	%fd235, %fd234, %fd232, %fd233;
	mov.f64 	%fd236, 0d3EB301D46D4B22F5;
	fma.rn.f64 	%fd237, %fd235, %fd232, %fd236;
	mov.f64 	%fd238, 0dBEB50272AEED0FC4;
	fma.rn.f64 	%fd239, %fd237, %fd232, %fd238;
	mov.f64 	%fd240, 0dBEF51CE1A40516F8;
	fma.rn.f64 	%fd241, %fd239, %fd232, %fd240;
	mov.f64 	%fd242, 0d3F20C8AA7419084C;
	fma.rn.f64 	%fd243, %fd241, %fd232, %fd242;
	mov.f64 	%fd244, 0dBF2C3650196BAD8A;
	fma.rn.f64 	%fd245, %fd243, %fd232, %fd244;
	mov.f64 	%fd246, 0dBF531711365A3E26;
	fma.rn.f64 	%fd247, %fd245, %fd232, %fd246;
	mov.f64 	%fd248, 0d3F7D919C52A7DF35;
	fma.rn.f64 	%fd249, %fd247, %fd232, %fd248;
	mov.f64 	%fd250, 0dBF83B4AF28386F4D;
	fma.rn.f64 	%fd251, %fd249, %fd232, %fd250;
	mov.f64 	%fd252, 0dBFA59AF103C37B4D;
	fma.rn.f64 	%fd253, %fd251, %fd232, %fd252;
	mov.f64 	%fd254, 0d3FC5512320B439EF;
	fma.rn.f64 	%fd255, %fd253, %fd232, %fd254;
	mov.f64 	%fd256, 0dBFA5815E8FA26F4F;
	fma.rn.f64 	%fd257, %fd255, %fd232, %fd256;
	mov.f64 	%fd258, 0dBFE4FCF4026AFA2B;
	fma.rn.f64 	%fd259, %fd257, %fd232, %fd258;
	mov.f64 	%fd260, 0d3FE2788CFC6FB619;
	fma.rn.f64 	%fd261, %fd259, %fd232, %fd260;
	mov.f64 	%fd262, 0d3FF0000000000000;
	fma.rn.f64 	%fd263, %fd261, %fd232, %fd262;
	mul.f64 	%fd264, %fd1, %fd263;
	setp.lt.s32	%p14, %r93, 1071644672;
	selp.f64	%fd265, %fd264, %fd263, %p14;
	div.rn.f64 	%fd428, %fd403, %fd265;
	bra.uni 	BB68_43;

BB68_19:
	add.f64 	%fd428, %fd1, %fd1;
	bra.uni 	BB68_43;

BB68_3:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r92, %temp}, %fd1;
	}
	shr.u32 	%r94, %r93, 20;
	setp.ne.s32	%p4, %r94, 0;
	@%p4 bra 	BB68_5;

	mul.f64 	%fd62, %fd1, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r93}, %fd62;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r92, %temp}, %fd62;
	}
	shr.u32 	%r37, %r93, 20;
	add.s32 	%r94, %r37, -54;

BB68_5:
	add.s32 	%r95, %r94, -1023;
	and.b32  	%r38, %r93, -2146435073;
	or.b32  	%r39, %r38, 1072693248;
	mov.b64 	%fd401, {%r92, %r39};
	setp.lt.u32	%p5, %r39, 1073127583;
	@%p5 bra 	BB68_7;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r40, %temp}, %fd401;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r41}, %fd401;
	}
	add.s32 	%r42, %r41, -1048576;
	mov.b64 	%fd401, {%r40, %r42};
	add.s32 	%r95, %r94, -1022;

BB68_7:
	add.f64 	%fd64, %fd401, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd63,%fd64;
	// inline asm
	neg.f64 	%fd65, %fd64;
	mov.f64 	%fd66, 0d3FF0000000000000;
	fma.rn.f64 	%fd67, %fd65, %fd63, %fd66;
	fma.rn.f64 	%fd68, %fd67, %fd67, %fd67;
	fma.rn.f64 	%fd69, %fd68, %fd63, %fd63;
	add.f64 	%fd70, %fd401, 0dBFF0000000000000;
	mul.f64 	%fd71, %fd70, %fd69;
	fma.rn.f64 	%fd72, %fd70, %fd69, %fd71;
	mul.f64 	%fd73, %fd72, %fd72;
	mov.f64 	%fd74, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd75, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd76, %fd75, %fd73, %fd74;
	mov.f64 	%fd77, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd78, %fd76, %fd73, %fd77;
	mov.f64 	%fd79, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd80, %fd78, %fd73, %fd79;
	mov.f64 	%fd81, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd82, %fd80, %fd73, %fd81;
	mov.f64 	%fd83, 0d3F6249249242B910;
	fma.rn.f64 	%fd84, %fd82, %fd73, %fd83;
	mov.f64 	%fd85, 0d3F89999999999DFB;
	fma.rn.f64 	%fd86, %fd84, %fd73, %fd85;
	sub.f64 	%fd87, %fd70, %fd72;
	add.f64 	%fd88, %fd87, %fd87;
	neg.f64 	%fd89, %fd72;
	fma.rn.f64 	%fd90, %fd89, %fd70, %fd88;
	mul.f64 	%fd91, %fd69, %fd90;
	fma.rn.f64 	%fd92, %fd73, %fd86, 0d3FB5555555555555;
	mov.f64 	%fd93, 0d3FB5555555555555;
	sub.f64 	%fd94, %fd93, %fd92;
	fma.rn.f64 	%fd95, %fd73, %fd86, %fd94;
	add.f64 	%fd96, %fd95, 0d0000000000000000;
	add.f64 	%fd97, %fd96, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd98, %fd92, %fd97;
	sub.f64 	%fd99, %fd92, %fd98;
	add.f64 	%fd100, %fd97, %fd99;
	mul.rn.f64 	%fd101, %fd72, %fd72;
	neg.f64 	%fd102, %fd101;
	fma.rn.f64 	%fd103, %fd72, %fd72, %fd102;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r43, %temp}, %fd91;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd91;
	}
	add.s32 	%r45, %r44, 1048576;
	mov.b64 	%fd104, {%r43, %r45};
	fma.rn.f64 	%fd105, %fd72, %fd104, %fd103;
	mul.rn.f64 	%fd106, %fd101, %fd72;
	neg.f64 	%fd107, %fd106;
	fma.rn.f64 	%fd108, %fd101, %fd72, %fd107;
	fma.rn.f64 	%fd109, %fd101, %fd91, %fd108;
	fma.rn.f64 	%fd110, %fd105, %fd72, %fd109;
	mul.rn.f64 	%fd111, %fd98, %fd106;
	neg.f64 	%fd112, %fd111;
	fma.rn.f64 	%fd113, %fd98, %fd106, %fd112;
	fma.rn.f64 	%fd114, %fd98, %fd110, %fd113;
	fma.rn.f64 	%fd115, %fd100, %fd106, %fd114;
	add.f64 	%fd116, %fd111, %fd115;
	sub.f64 	%fd117, %fd111, %fd116;
	add.f64 	%fd118, %fd115, %fd117;
	add.f64 	%fd119, %fd72, %fd116;
	sub.f64 	%fd120, %fd72, %fd119;
	add.f64 	%fd121, %fd116, %fd120;
	add.f64 	%fd122, %fd118, %fd121;
	add.f64 	%fd123, %fd91, %fd122;
	add.f64 	%fd124, %fd119, %fd123;
	sub.f64 	%fd125, %fd119, %fd124;
	add.f64 	%fd126, %fd123, %fd125;
	xor.b32  	%r46, %r95, -2147483648;
	mov.u32 	%r47, 1127219200;
	mov.b64 	%fd127, {%r46, %r47};
	mov.u32 	%r48, -2147483648;
	mov.b64 	%fd128, {%r48, %r47};
	sub.f64 	%fd129, %fd127, %fd128;
	mov.f64 	%fd130, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd131, %fd129, %fd130, %fd124;
	neg.f64 	%fd132, %fd129;
	fma.rn.f64 	%fd133, %fd132, %fd130, %fd131;
	sub.f64 	%fd134, %fd133, %fd124;
	sub.f64 	%fd135, %fd126, %fd134;
	mov.f64 	%fd136, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd137, %fd129, %fd136, %fd135;
	add.f64 	%fd138, %fd131, %fd137;
	sub.f64 	%fd139, %fd131, %fd138;
	add.f64 	%fd140, %fd137, %fd139;
	add.f64 	%fd141, %fd1, 0dBFE0000000000000;
	mul.rn.f64 	%fd142, %fd138, %fd141;
	neg.f64 	%fd143, %fd142;
	fma.rn.f64 	%fd144, %fd138, %fd141, %fd143;
	fma.rn.f64 	%fd145, %fd140, %fd141, %fd144;
	add.f64 	%fd146, %fd142, %fd145;
	sub.f64 	%fd147, %fd142, %fd146;
	add.f64 	%fd148, %fd145, %fd147;
	sub.f64 	%fd149, %fd146, %fd1;
	sub.f64 	%fd150, %fd146, %fd149;
	sub.f64 	%fd151, %fd150, %fd1;
	add.f64 	%fd152, %fd148, %fd151;
	add.f64 	%fd5, %fd149, %fd152;
	sub.f64 	%fd153, %fd149, %fd5;
	add.f64 	%fd6, %fd152, %fd153;
	mov.f64 	%fd154, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd155, %fd5, %fd154;
	mov.f64 	%fd156, 0d4338000000000000;
	add.rn.f64 	%fd157, %fd155, %fd156;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd157;
	}
	mov.f64 	%fd158, 0dC338000000000000;
	add.rn.f64 	%fd159, %fd157, %fd158;
	mov.f64 	%fd160, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd161, %fd159, %fd160, %fd5;
	mov.f64 	%fd162, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd163, %fd159, %fd162, %fd161;
	mov.f64 	%fd164, 0d3E928AF3FCA213EA;
	mov.f64 	%fd165, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd166, %fd165, %fd163, %fd164;
	mov.f64 	%fd167, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd168, %fd166, %fd163, %fd167;
	mov.f64 	%fd169, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd170, %fd168, %fd163, %fd169;
	mov.f64 	%fd171, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd172, %fd170, %fd163, %fd171;
	mov.f64 	%fd173, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd174, %fd172, %fd163, %fd173;
	mov.f64 	%fd175, 0d3F81111111122322;
	fma.rn.f64 	%fd176, %fd174, %fd163, %fd175;
	mov.f64 	%fd177, 0d3FA55555555502A1;
	fma.rn.f64 	%fd178, %fd176, %fd163, %fd177;
	mov.f64 	%fd179, 0d3FC5555555555511;
	fma.rn.f64 	%fd180, %fd178, %fd163, %fd179;
	mov.f64 	%fd181, 0d3FE000000000000B;
	fma.rn.f64 	%fd182, %fd180, %fd163, %fd181;
	fma.rn.f64 	%fd183, %fd182, %fd163, %fd66;
	fma.rn.f64 	%fd184, %fd183, %fd163, %fd66;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd184;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd184;
	}
	shl.b32 	%r49, %r13, 20;
	add.s32 	%r50, %r15, %r49;
	mov.b64 	%fd402, {%r14, %r50};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r51}, %fd5;
	}
	mov.b32 	 %f2, %r51;
	abs.f32 	%f1, %f2;
	setp.lt.f32	%p6, %f1, 0f4086232B;
	@%p6 bra 	BB68_10;

	setp.lt.f64	%p7, %fd5, 0d0000000000000000;
	add.f64 	%fd185, %fd5, 0d7FF0000000000000;
	selp.f64	%fd402, 0d0000000000000000, %fd185, %p7;
	setp.geu.f32	%p8, %f1, 0f40874800;
	@%p8 bra 	BB68_10;

	shr.u32 	%r52, %r13, 31;
	add.s32 	%r53, %r13, %r52;
	shr.s32 	%r54, %r53, 1;
	shl.b32 	%r55, %r54, 20;
	add.s32 	%r56, %r55, %r15;
	mov.b64 	%fd186, {%r14, %r56};
	sub.s32 	%r57, %r13, %r54;
	shl.b32 	%r58, %r57, 20;
	add.s32 	%r59, %r58, 1072693248;
	mov.u32 	%r60, 0;
	mov.b64 	%fd187, {%r60, %r59};
	mul.f64 	%fd402, %fd186, %fd187;

BB68_10:
	fma.rn.f64 	%fd190, %fd6, %fd402, %fd402;
	mul.f64 	%fd191, %fd190, 0dBCAA6A0D6F814637;
	mov.f64 	%fd192, 0d40040D931FF62706;
	fma.rn.f64 	%fd193, %fd190, %fd192, %fd191;
	// inline asm
	rcp.approx.ftz.f64 %fd188,%fd1;
	// inline asm
	neg.f64 	%fd194, %fd1;
	fma.rn.f64 	%fd196, %fd194, %fd188, %fd66;
	fma.rn.f64 	%fd197, %fd196, %fd196, %fd196;
	fma.rn.f64 	%fd198, %fd197, %fd188, %fd188;
	mov.f64 	%fd199, 0d3F73C25DA81303D5;
	mov.f64 	%fd200, 0dBF64BEE47C38A637;
	fma.rn.f64 	%fd201, %fd200, %fd198, %fd199;
	mov.f64 	%fd202, 0dBF6B7C37A96CFC72;
	fma.rn.f64 	%fd203, %fd201, %fd198, %fd202;
	mov.f64 	%fd204, 0d3F35A85ABDE1E324;
	fma.rn.f64 	%fd205, %fd203, %fd198, %fd204;
	mov.f64 	%fd206, 0d3F4A8B28F07B3F05;
	fma.rn.f64 	%fd207, %fd205, %fd198, %fd206;
	mov.f64 	%fd208, 0dBF0A15D1D45A282F;
	fma.rn.f64 	%fd209, %fd207, %fd198, %fd208;
	mov.f64 	%fd210, 0dBF4367D3468CB5BE;
	fma.rn.f64 	%fd211, %fd209, %fd198, %fd210;
	mov.f64 	%fd212, 0d3F12471B0E9F1005;
	fma.rn.f64 	%fd213, %fd211, %fd198, %fd212;
	mov.f64 	%fd214, 0d3F49B1004744D5C4;
	fma.rn.f64 	%fd215, %fd213, %fd198, %fd214;
	mov.f64 	%fd216, 0dBF2E13CE69AB4B7F;
	fma.rn.f64 	%fd217, %fd215, %fd198, %fd216;
	mov.f64 	%fd218, 0dBF65F7268ECF8A01;
	fma.rn.f64 	%fd219, %fd217, %fd198, %fd218;
	mov.f64 	%fd220, 0d3F6C71C71C71ACE0;
	fma.rn.f64 	%fd221, %fd219, %fd198, %fd220;
	mov.f64 	%fd222, 0d3FB5555555555556;
	fma.rn.f64 	%fd223, %fd221, %fd198, %fd222;
	mul.f64 	%fd224, %fd198, %fd223;
	fma.rn.f64 	%fd225, %fd224, %fd193, %fd193;
	setp.ltu.f64	%p9, %fd1, 0d406573FAE561F648;
	selp.f64	%fd428, %fd225, 0d7FF0000000000000, %p9;

BB68_43:
	mov.u32 	%r91, %tid.x;
	mov.u32 	%r90, %ctaid.x;
	mov.u32 	%r89, %ntid.x;
	mad.lo.s32 	%r88, %r89, %r90, %r91;
	shl.b32 	%r87, %r88, 3;
	ld.param.u32 	%r86, [vec_tgamma_param_1];
	cvta.to.global.u32 	%r83, %r86;
	add.s32 	%r85, %r83, %r87;
	st.global.f64 	[%r85], %fd428;

BB68_44:
	ret;

BB68_22:
	setp.lt.u32	%p18, %r16, -1066983424;
	@%p18 bra 	BB68_24;
	bra.uni 	BB68_23;

BB68_24:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r61, %temp}, %fd1;
	}
	add.s32 	%r62, %r16, 1048576;
	mov.b64 	%fd272, {%r61, %r62};
	cvt.rni.f64.f64	%fd273, %fd272;
	cvt.rzi.s64.f64	%rd1, %fd273;
	cvt.u32.u64	%r17, %rd1;
	neg.f64 	%fd274, %fd273;
	mov.f64 	%fd275, 0d3FE0000000000000;
	fma.rn.f64 	%fd276, %fd274, %fd275, %fd1;
	mul.f64 	%fd277, %fd276, 0d3CA1A62633145C07;
	mov.f64 	%fd278, 0d400921FB54442D18;
	fma.rn.f64 	%fd279, %fd276, %fd278, %fd277;
	and.b32  	%r63, %r17, 1;
	mul.rn.f64 	%fd27, %fd279, %fd279;
	setp.eq.s32	%p20, %r63, 0;
	selp.f64	%fd280, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p20;
	shl.b32 	%r64, %r63, 6;
	mov.u32 	%r65, __cudart_sin_cos_coeffs;
	add.s32 	%r66, %r64, %r65;
	ld.const.f64 	%fd281, [%r66+8];
	fma.rn.f64 	%fd282, %fd280, %fd27, %fd281;
	ld.const.f64 	%fd283, [%r66+16];
	fma.rn.f64 	%fd284, %fd282, %fd27, %fd283;
	ld.const.f64 	%fd285, [%r66+24];
	fma.rn.f64 	%fd286, %fd284, %fd27, %fd285;
	ld.const.f64 	%fd287, [%r66+32];
	fma.rn.f64 	%fd288, %fd286, %fd27, %fd287;
	ld.const.f64 	%fd289, [%r66+40];
	fma.rn.f64 	%fd290, %fd288, %fd27, %fd289;
	ld.const.f64 	%fd291, [%r66+48];
	fma.rn.f64 	%fd28, %fd290, %fd27, %fd291;
	fma.rn.f64 	%fd404, %fd28, %fd279, %fd279;
	@%p20 bra 	BB68_26;

	mov.f64 	%fd292, 0d3FF0000000000000;
	fma.rn.f64 	%fd404, %fd28, %fd27, %fd292;

BB68_26:
	and.b32  	%r67, %r17, 2;
	setp.eq.s32	%p21, %r67, 0;
	@%p21 bra 	BB68_28;

	mov.f64 	%fd293, 0d0000000000000000;
	mov.f64 	%fd294, 0dBFF0000000000000;
	fma.rn.f64 	%fd404, %fd404, %fd294, %fd293;

BB68_28:
	abs.f64 	%fd296, %fd1;
	// inline asm
	rcp.approx.ftz.f64 %fd295,%fd296;
	// inline asm
	neg.f64 	%fd297, %fd296;
	mov.f64 	%fd298, 0d3FF0000000000000;
	fma.rn.f64 	%fd299, %fd297, %fd295, %fd298;
	fma.rn.f64 	%fd300, %fd299, %fd299, %fd299;
	fma.rn.f64 	%fd301, %fd300, %fd295, %fd295;
	mov.f64 	%fd302, 0d3F73C25DA81303D5;
	mov.f64 	%fd303, 0dBF64BEE47C38A637;
	fma.rn.f64 	%fd304, %fd303, %fd301, %fd302;
	mov.f64 	%fd305, 0dBF6B7C37A96CFC72;
	fma.rn.f64 	%fd306, %fd304, %fd301, %fd305;
	mov.f64 	%fd307, 0d3F35A85ABDE1E324;
	fma.rn.f64 	%fd308, %fd306, %fd301, %fd307;
	mov.f64 	%fd309, 0d3F4A8B28F07B3F05;
	fma.rn.f64 	%fd310, %fd308, %fd301, %fd309;
	mov.f64 	%fd311, 0dBF0A15D1D45A282F;
	fma.rn.f64 	%fd312, %fd310, %fd301, %fd311;
	mov.f64 	%fd313, 0dBF4367D3468CB5BE;
	fma.rn.f64 	%fd314, %fd312, %fd301, %fd313;
	mov.f64 	%fd315, 0d3F12471B0E9F1005;
	fma.rn.f64 	%fd316, %fd314, %fd301, %fd315;
	mov.f64 	%fd317, 0d3F49B1004744D5C4;
	fma.rn.f64 	%fd318, %fd316, %fd301, %fd317;
	mov.f64 	%fd319, 0dBF2E13CE69AB4B7F;
	fma.rn.f64 	%fd320, %fd318, %fd301, %fd319;
	mov.f64 	%fd321, 0dBF65F7268ECF8A01;
	fma.rn.f64 	%fd322, %fd320, %fd301, %fd321;
	mov.f64 	%fd323, 0d3F6C71C71C71ACE0;
	fma.rn.f64 	%fd324, %fd322, %fd301, %fd323;
	mov.f64 	%fd325, 0d3FB5555555555556;
	fma.rn.f64 	%fd326, %fd324, %fd301, %fd325;
	mul.f64 	%fd327, %fd301, %fd326;
	fma.rn.f64 	%fd35, %fd327, %fd404, %fd404;
	mov.f64 	%fd328, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd329, %fd296, %fd328;
	mov.f64 	%fd330, 0d4338000000000000;
	add.rn.f64 	%fd331, %fd329, %fd330;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r18, %temp}, %fd331;
	}
	mov.f64 	%fd332, 0dC338000000000000;
	add.rn.f64 	%fd333, %fd331, %fd332;
	mov.f64 	%fd334, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd335, %fd333, %fd334, %fd296;
	mov.f64 	%fd336, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd337, %fd333, %fd336, %fd335;
	mov.f64 	%fd338, 0d3E928AF3FCA213EA;
	mov.f64 	%fd339, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd340, %fd339, %fd337, %fd338;
	mov.f64 	%fd341, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd342, %fd340, %fd337, %fd341;
	mov.f64 	%fd343, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd344, %fd342, %fd337, %fd343;
	mov.f64 	%fd345, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd346, %fd344, %fd337, %fd345;
	mov.f64 	%fd347, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd348, %fd346, %fd337, %fd347;
	mov.f64 	%fd349, 0d3F81111111122322;
	fma.rn.f64 	%fd350, %fd348, %fd337, %fd349;
	mov.f64 	%fd351, 0d3FA55555555502A1;
	fma.rn.f64 	%fd352, %fd350, %fd337, %fd351;
	mov.f64 	%fd353, 0d3FC5555555555511;
	fma.rn.f64 	%fd354, %fd352, %fd337, %fd353;
	mov.f64 	%fd355, 0d3FE000000000000B;
	fma.rn.f64 	%fd356, %fd354, %fd337, %fd355;
	fma.rn.f64 	%fd357, %fd356, %fd337, %fd298;
	fma.rn.f64 	%fd405, %fd357, %fd337, %fd298;
	abs.s32 	%r68, %r18;
	setp.lt.s32	%p22, %r68, 1023;
	@%p22 bra 	BB68_30;
	bra.uni 	BB68_29;

BB68_30:
	shl.b32 	%r74, %r18, 20;
	add.s32 	%r96, %r74, 1072693248;
	bra.uni 	BB68_31;

BB68_23:
	cvt.rmi.f64.f64	%fd268, %fd1;
	mul.f64 	%fd269, %fd268, 0d3FE0000000000000;
	cvt.rmi.f64.f64	%fd270, %fd269;
	fma.rn.f64 	%fd271, %fd270, 0dC000000000000000, %fd268;
	setp.eq.f64	%p19, %fd271, 0d3FF0000000000000;
	selp.f64	%fd428, 0d8000000000000000, 0d0000000000000000, %p19;
	bra.uni 	BB68_43;

BB68_29:
	add.s32 	%r69, %r18, 2046;
	shl.b32 	%r70, %r69, 19;
	and.b32  	%r71, %r70, -1048576;
	shl.b32 	%r72, %r69, 20;
	sub.s32 	%r96, %r72, %r71;
	mov.u32 	%r73, 0;
	mov.b64 	%fd358, {%r73, %r71};
	mul.f64 	%fd405, %fd405, %fd358;

BB68_31:
	mul.f64 	%fd359, %fd296, %fd35;
	mov.u32 	%r75, 0;
	mov.b64 	%fd360, {%r75, %r96};
	mul.f64 	%fd361, %fd405, %fd360;
	mul.f64 	%fd362, %fd361, 0dBC9A6A0D6F814637;
	mov.f64 	%fd363, 0d3FF40D931FF62706;
	fma.rn.f64 	%fd364, %fd361, %fd363, %fd362;
	div.rn.f64 	%fd39, %fd364, %fd359;
	add.f64 	%fd406, %fd296, 0dBFE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd296;
	}
	setp.lt.s32	%p23, %r22, 1080157184;
	@%p23 bra 	BB68_33;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r76, %temp}, %fd406;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r77}, %fd406;
	}
	add.s32 	%r78, %r77, -1048576;
	mov.b64 	%fd406, {%r76, %r78};

BB68_33:
	neg.f64 	%fd365, %fd406;
	// Callseq Start 8
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd296;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd365;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd366, [retval0+0];
	
	//{
	}// Callseq End 8
	mul.f64 	%fd367, %fd39, %fd366;
	setp.gt.s32	%p24, %r22, 1080157183;
	selp.f64	%fd368, %fd367, %fd39, %p24;
	mul.f64 	%fd428, %fd366, %fd368;
	bra.uni 	BB68_43;
}

	// .globl	vec_trunc
.visible .entry vec_trunc(
	.param .u32 vec_trunc_param_0,
	.param .u32 vec_trunc_param_1,
	.param .u32 vec_trunc_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<3>;


	ld.param.u32 	%r4, [vec_trunc_param_0];
	ld.param.u32 	%r2, [vec_trunc_param_1];
	ld.param.u32 	%r3, [vec_trunc_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB69_2;

	cvta.to.global.u32 	%r8, %r3;
	shl.b32 	%r9, %r1, 3;
	add.s32 	%r10, %r8, %r9;
	ld.global.f64 	%fd1, [%r10];
	cvt.rzi.f64.f64	%fd2, %fd1;
	cvta.to.global.u32 	%r11, %r2;
	add.s32 	%r12, %r11, %r9;
	st.global.f64 	[%r12], %fd2;

BB69_2:
	ret;
}

	// .globl	vec_y0
.visible .entry vec_y0(
	.param .u32 vec_y0_param_0,
	.param .u32 vec_y0_param_1,
	.param .u32 vec_y0_param_2
)
{
	.local .align 4 .b8 	__local_depot70[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b32 	%r<91>;
	.reg .f64 	%fd<545>;


	mov.u32 	%r90, __local_depot70;
	cvta.local.u32 	%SP, %r90;
	ld.param.u32 	%r31, [vec_y0_param_0];
	ld.param.u32 	%r29, [vec_y0_param_1];
	ld.param.u32 	%r30, [vec_y0_param_2];
	add.u32 	%r32, %SP, 0;
	cvta.to.local.u32 	%r1, %r32;
	mov.u32 	%r35, %ntid.x;
	mov.u32 	%r36, %ctaid.x;
	mov.u32 	%r37, %tid.x;
	mad.lo.s32 	%r4, %r35, %r36, %r37;
	setp.ge.u32	%p1, %r4, %r31;
	@%p1 bra 	BB70_50;

	cvta.to.global.u32 	%r38, %r30;
	shl.b32 	%r39, %r4, 3;
	add.s32 	%r40, %r38, %r39;
	ld.global.f64 	%fd1, [%r40];
	abs.f64 	%fd2, %fd1;
	setp.gtu.f64	%p2, %fd2, 0d3FE97F4A8F9D3F28;
	@%p2 bra 	BB70_31;
	bra.uni 	BB70_2;

BB70_31:
	setp.gtu.f64	%p20, %fd2, 0d4000347C4AB37B18;
	@%p20 bra 	BB70_33;
	bra.uni 	BB70_32;

BB70_33:
	setp.gtu.f64	%p21, %fd2, 0d40161663B5D9A628;
	@%p21 bra 	BB70_35;
	bra.uni 	BB70_34;

BB70_35:
	setp.gtu.f64	%p22, %fd2, 0d40214EF30C0C06ED;
	@%p22 bra 	BB70_37;
	bra.uni 	BB70_36;

BB70_37:
	abs.f64 	%fd444, %fd2;
	mov.f64 	%fd544, 0d0000000000000000;
	setp.eq.f64	%p23, %fd444, 0d7FF0000000000000;
	@%p23 bra 	BB70_49;

	// inline asm
	rcp.approx.ftz.f64 %fd445,%fd2;
	// inline asm
	neg.f64 	%fd447, %fd2;
	mov.f64 	%fd448, 0d3FF0000000000000;
	fma.rn.f64 	%fd449, %fd447, %fd445, %fd448;
	fma.rn.f64 	%fd450, %fd449, %fd449, %fd449;
	fma.rn.f64 	%fd451, %fd450, %fd445, %fd445;
	mul.f64 	%fd452, %fd451, %fd451;
	mov.f64 	%fd453, 0d4093F56A049CDDE7;
	mov.f64 	%fd454, 0dC0C5E91E6AC3AD03;
	fma.rn.f64 	%fd455, %fd454, %fd452, %fd453;
	mov.f64 	%fd456, 0dC05572D39DFB8433;
	fma.rn.f64 	%fd457, %fd455, %fd452, %fd456;
	mov.f64 	%fd458, 0d4016A6041CAA59E5;
	fma.rn.f64 	%fd459, %fd457, %fd452, %fd458;
	mov.f64 	%fd460, 0dBFE155E3A0493880;
	fma.rn.f64 	%fd461, %fd459, %fd452, %fd460;
	mov.f64 	%fd462, 0d3FBA7FB92F417F7F;
	fma.rn.f64 	%fd463, %fd461, %fd452, %fd462;
	mov.f64 	%fd464, 0dBFAFFFFFB12E32F5;
	fma.rn.f64 	%fd465, %fd463, %fd452, %fd464;
	mov.f64 	%fd466, 0d3FEFFFFFFFFECED5;
	fma.rn.f64 	%fd467, %fd465, %fd452, %fd466;
	mov.f64 	%fd468, 0dC15709C79AAC5813;
	mov.f64 	%fd469, 0d418A86A64BE101DC;
	fma.rn.f64 	%fd470, %fd469, %fd452, %fd468;
	mov.f64 	%fd471, 0d41142A31C980A287;
	fma.rn.f64 	%fd472, %fd470, %fd452, %fd471;
	mov.f64 	%fd473, 0dC0C9CBE68930485D;
	fma.rn.f64 	%fd474, %fd472, %fd452, %fd473;
	mov.f64 	%fd475, 0d407F583E14E8A4E8;
	fma.rn.f64 	%fd476, %fd474, %fd452, %fd475;
	mov.f64 	%fd477, 0dC0374A629C650680;
	fma.rn.f64 	%fd478, %fd476, %fd452, %fd477;
	mov.f64 	%fd479, 0d3FFA32A7AF17FAE9;
	fma.rn.f64 	%fd480, %fd478, %fd452, %fd479;
	mov.f64 	%fd481, 0dBFCAD32497785CD6;
	fma.rn.f64 	%fd482, %fd480, %fd452, %fd481;
	mov.f64 	%fd483, 0d3FB0AAAA9FB75F7B;
	fma.rn.f64 	%fd484, %fd482, %fd452, %fd483;
	mov.f64 	%fd485, 0dBFBFFFFFFFFE320F;
	fma.rn.f64 	%fd486, %fd484, %fd452, %fd485;
	fma.rn.f64 	%fd41, %fd486, %fd451, %fd2;
	rsqrt.approx.f64 	%fd487, %fd2;
	mul.f64 	%fd488, %fd487, 0d3FE9884533D43651;
	mul.f64 	%fd42, %fd467, %fd488;
	mul.f64 	%fd489, %fd41, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r88, %fd489;
	st.local.u32 	[%r1], %r88;
	cvt.rn.f64.s32	%fd490, %r88;
	neg.f64 	%fd491, %fd490;
	mov.f64 	%fd492, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd493, %fd491, %fd492, %fd41;
	mov.f64 	%fd494, 0d3C91A62633145C00;
	fma.rn.f64 	%fd495, %fd491, %fd494, %fd493;
	mov.f64 	%fd496, 0d397B839A252049C0;
	fma.rn.f64 	%fd540, %fd491, %fd496, %fd495;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r67}, %fd41;
	}
	and.b32  	%r68, %r67, 2145386496;
	setp.lt.u32	%p24, %r68, 1105199104;
	@%p24 bra 	BB70_40;

	// Callseq Start 11
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd41;
	.param .b32 param1;
	st.param.b32	[param1+0], %r32;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd540, [retval0+0];
	
	//{
	}// Callseq End 11
	ld.local.u32 	%r88, [%r1];

BB70_40:
	and.b32  	%r70, %r88, 3;
	cvt.rn.f64.s32	%fd497, %r70;
	add.f64 	%fd498, %fd540, 0dC002D97C7F3321D2;
	fma.rn.f64 	%fd541, %fd497, 0d3FF921FB54442D18, %fd498;
	abs.f64 	%fd499, %fd541;
	setp.neu.f64	%p25, %fd499, 0d7FF0000000000000;
	@%p25 bra 	BB70_42;

	mov.f64 	%fd500, 0d0000000000000000;
	mul.rn.f64 	%fd541, %fd541, %fd500;

BB70_42:
	mov.f64 	%fd531, 0d397B839A252049C0;
	mov.f64 	%fd530, 0d3C91A62633145C00;
	mov.f64 	%fd529, 0d3FF921FB54442D18;
	mul.f64 	%fd501, %fd541, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r89, %fd501;
	st.local.u32 	[%r1], %r89;
	cvt.rn.f64.s32	%fd502, %r89;
	neg.f64 	%fd503, %fd502;
	fma.rn.f64 	%fd505, %fd503, %fd529, %fd541;
	fma.rn.f64 	%fd507, %fd503, %fd530, %fd505;
	fma.rn.f64 	%fd542, %fd503, %fd531, %fd507;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r71}, %fd541;
	}
	and.b32  	%r72, %r71, 2145386496;
	setp.lt.u32	%p26, %r72, 1105199104;
	@%p26 bra 	BB70_44;

	// Callseq Start 12
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd541;
	.param .b32 param1;
	st.param.b32	[param1+0], %r32;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd542, [retval0+0];
	
	//{
	}// Callseq End 12
	ld.local.u32 	%r89, [%r1];

BB70_44:
	add.s32 	%r28, %r89, 1;
	and.b32  	%r74, %r28, 1;
	setp.eq.s32	%p27, %r74, 0;
	selp.f64	%fd509, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p27;
	shl.b32 	%r75, %r74, 6;
	mov.u32 	%r76, __cudart_sin_cos_coeffs;
	add.s32 	%r77, %r75, %r76;
	ld.const.f64 	%fd510, [%r77+8];
	mul.rn.f64 	%fd52, %fd542, %fd542;
	fma.rn.f64 	%fd511, %fd509, %fd52, %fd510;
	ld.const.f64 	%fd512, [%r77+16];
	fma.rn.f64 	%fd513, %fd511, %fd52, %fd512;
	ld.const.f64 	%fd514, [%r77+24];
	fma.rn.f64 	%fd515, %fd513, %fd52, %fd514;
	ld.const.f64 	%fd516, [%r77+32];
	fma.rn.f64 	%fd517, %fd515, %fd52, %fd516;
	ld.const.f64 	%fd518, [%r77+40];
	fma.rn.f64 	%fd519, %fd517, %fd52, %fd518;
	ld.const.f64 	%fd520, [%r77+48];
	fma.rn.f64 	%fd53, %fd519, %fd52, %fd520;
	fma.rn.f64 	%fd543, %fd53, %fd542, %fd542;
	@%p27 bra 	BB70_46;

	fma.rn.f64 	%fd543, %fd53, %fd52, %fd448;

BB70_46:
	and.b32  	%r78, %r28, 2;
	setp.eq.s32	%p28, %r78, 0;
	@%p28 bra 	BB70_48;

	mov.f64 	%fd522, 0d0000000000000000;
	mov.f64 	%fd523, 0dBFF0000000000000;
	fma.rn.f64 	%fd543, %fd543, %fd523, %fd522;

BB70_48:
	mul.f64 	%fd544, %fd42, %fd543;
	bra.uni 	BB70_49;

BB70_2:
	mul.f64 	%fd61, %fd2, %fd2;
	mov.f64 	%fd62, 0dBD13098C51C18514;
	mov.f64 	%fd63, 0d3C8EFBD0A1B77C65;
	fma.rn.f64 	%fd64, %fd63, %fd61, %fd62;
	mov.f64 	%fd65, 0d3D923102D2F5F2F5;
	fma.rn.f64 	%fd66, %fd64, %fd61, %fd65;
	mov.f64 	%fd67, 0dBE0A5F2DEE7D526E;
	fma.rn.f64 	%fd68, %fd66, %fd61, %fd67;
	mov.f64 	%fd69, 0d3E7BB77E758B38AF;
	fma.rn.f64 	%fd70, %fd68, %fd61, %fd69;
	mov.f64 	%fd71, 0dBEE3D1A206EC4F36;
	fma.rn.f64 	%fd72, %fd70, %fd61, %fd71;
	mov.f64 	%fd73, 0d3F4183DCD3ED6294;
	fma.rn.f64 	%fd74, %fd72, %fd61, %fd73;
	mov.f64 	%fd75, 0dBF903921CF04F123;
	fma.rn.f64 	%fd76, %fd74, %fd61, %fd75;
	mov.f64 	%fd77, 0d3FC5DB69D7753176;
	fma.rn.f64 	%fd78, %fd76, %fd61, %fd77;
	add.f64 	%fd79, %fd61, 0dBFDBA96740000000;
	add.f64 	%fd80, %fd79, 0d3E15A30C80000000;
	mul.f64 	%fd3, %fd80, %fd78;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r82}, %fd2;
	}
	setp.lt.s32	%p3, %r82, 2146435072;
	setp.gt.f64	%p4, %fd2, 0d0000000000000000;
	and.pred  	%p5, %p4, %p3;
	@%p5 bra 	BB70_7;
	bra.uni 	BB70_3;

BB70_7:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r83, %temp}, %fd2;
	}
	mov.u32 	%r84, -1023;
	setp.gt.s32	%p9, %r82, 1048575;
	@%p9 bra 	BB70_9;

	mul.f64 	%fd82, %fd2, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r82}, %fd82;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r83, %temp}, %fd82;
	}
	mov.u32 	%r84, -1077;

BB70_9:
	shr.u32 	%r43, %r82, 20;
	add.s32 	%r85, %r84, %r43;
	and.b32  	%r44, %r82, -2146435073;
	or.b32  	%r45, %r44, 1072693248;
	mov.b64 	%fd532, {%r83, %r45};
	setp.lt.s32	%p10, %r45, 1073127583;
	@%p10 bra 	BB70_11;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r46, %temp}, %fd532;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r47}, %fd532;
	}
	add.s32 	%r48, %r47, -1048576;
	mov.b64 	%fd532, {%r46, %r48};
	add.s32 	%r85, %r85, 1;

BB70_11:
	add.f64 	%fd84, %fd532, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd83,%fd84;
	// inline asm
	neg.f64 	%fd85, %fd84;
	mov.f64 	%fd86, 0d3FF0000000000000;
	fma.rn.f64 	%fd87, %fd85, %fd83, %fd86;
	fma.rn.f64 	%fd88, %fd87, %fd87, %fd87;
	fma.rn.f64 	%fd89, %fd88, %fd83, %fd83;
	add.f64 	%fd90, %fd532, 0dBFF0000000000000;
	mul.f64 	%fd91, %fd90, %fd89;
	fma.rn.f64 	%fd92, %fd90, %fd89, %fd91;
	mul.f64 	%fd93, %fd92, %fd92;
	mov.f64 	%fd94, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd95, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd96, %fd95, %fd93, %fd94;
	mov.f64 	%fd97, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd98, %fd96, %fd93, %fd97;
	mov.f64 	%fd99, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd100, %fd98, %fd93, %fd99;
	mov.f64 	%fd101, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd102, %fd100, %fd93, %fd101;
	mov.f64 	%fd103, 0d3F624924923BE72D;
	fma.rn.f64 	%fd104, %fd102, %fd93, %fd103;
	mov.f64 	%fd105, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd106, %fd104, %fd93, %fd105;
	mov.f64 	%fd107, 0d3FB5555555555554;
	fma.rn.f64 	%fd108, %fd106, %fd93, %fd107;
	sub.f64 	%fd109, %fd90, %fd92;
	add.f64 	%fd110, %fd109, %fd109;
	neg.f64 	%fd111, %fd92;
	fma.rn.f64 	%fd112, %fd111, %fd90, %fd110;
	mul.f64 	%fd113, %fd89, %fd112;
	mul.f64 	%fd114, %fd93, %fd108;
	fma.rn.f64 	%fd115, %fd114, %fd92, %fd113;
	xor.b32  	%r49, %r85, -2147483648;
	mov.u32 	%r50, 1127219200;
	mov.b64 	%fd116, {%r49, %r50};
	mov.u32 	%r51, -2147483648;
	mov.b64 	%fd117, {%r51, %r50};
	sub.f64 	%fd118, %fd116, %fd117;
	mov.f64 	%fd119, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd120, %fd118, %fd119, %fd92;
	neg.f64 	%fd121, %fd118;
	fma.rn.f64 	%fd122, %fd121, %fd119, %fd120;
	sub.f64 	%fd123, %fd122, %fd92;
	sub.f64 	%fd124, %fd115, %fd123;
	mov.f64 	%fd125, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd126, %fd118, %fd125, %fd124;
	add.f64 	%fd534, %fd120, %fd126;
	abs.f64 	%fd533, %fd2;
	bra.uni 	BB70_12;

BB70_32:
	add.f64 	%fd312, %fd2, 0dBFEC982EB8D417EA;
	add.f64 	%fd313, %fd312, 0dBC7EA9D270347F83;
	mov.f64 	%fd314, 0d3F3D054B05D3C52D;
	mov.f64 	%fd315, 0dBF01630132D75FC3;
	fma.rn.f64 	%fd316, %fd315, %fd313, %fd314;
	mov.f64 	%fd317, 0dBF66DAC0B314B2E5;
	fma.rn.f64 	%fd318, %fd316, %fd313, %fd317;
	mov.f64 	%fd319, 0d3F86A5D1DE76263F;
	fma.rn.f64 	%fd320, %fd318, %fd313, %fd319;
	mov.f64 	%fd321, 0dBF9FD16652824592;
	fma.rn.f64 	%fd322, %fd320, %fd313, %fd321;
	mov.f64 	%fd323, 0d3FB0F69A9CC79FBD;
	fma.rn.f64 	%fd324, %fd322, %fd313, %fd323;
	mov.f64 	%fd325, 0dBFBCCE40EF15583E;
	fma.rn.f64 	%fd326, %fd324, %fd313, %fd325;
	mov.f64 	%fd327, 0d3FC446B11780E4FC;
	fma.rn.f64 	%fd328, %fd326, %fd313, %fd327;
	mov.f64 	%fd329, 0dBFC89AE7E19621F7;
	fma.rn.f64 	%fd330, %fd328, %fd313, %fd329;
	mov.f64 	%fd331, 0d3FCACBA1B38EF7B8;
	fma.rn.f64 	%fd332, %fd330, %fd313, %fd331;
	mov.f64 	%fd333, 0dBFCB4166A03BBFA5;
	fma.rn.f64 	%fd334, %fd332, %fd313, %fd333;
	mov.f64 	%fd335, 0d3FCACCA4D5D4889A;
	fma.rn.f64 	%fd336, %fd334, %fd313, %fd335;
	mov.f64 	%fd337, 0dBFCA1455932B9392;
	fma.rn.f64 	%fd338, %fd336, %fd313, %fd337;
	mov.f64 	%fd339, 0d3FC96D8DB8D844EC;
	fma.rn.f64 	%fd340, %fd338, %fd313, %fd339;
	mov.f64 	%fd341, 0dBFC8F7FB77522EDF;
	fma.rn.f64 	%fd342, %fd340, %fd313, %fd341;
	mov.f64 	%fd343, 0d3FC8C0926ABC9AB0;
	fma.rn.f64 	%fd344, %fd342, %fd313, %fd343;
	mov.f64 	%fd345, 0dBFC8D35B8FEA468C;
	fma.rn.f64 	%fd346, %fd344, %fd313, %fd345;
	mov.f64 	%fd347, 0d3FC9424B8A0C8F94;
	fma.rn.f64 	%fd348, %fd346, %fd313, %fd347;
	mov.f64 	%fd349, 0dBFCA396A7F3403EF;
	fma.rn.f64 	%fd350, %fd348, %fd313, %fd349;
	mov.f64 	%fd351, 0d3FCC068086C37055;
	fma.rn.f64 	%fd352, %fd350, %fd313, %fd351;
	mov.f64 	%fd353, 0dBFCCF18E6A4C5C4E;
	fma.rn.f64 	%fd354, %fd352, %fd313, %fd353;
	mov.f64 	%fd355, 0d3FCC3B1338AF4239;
	fma.rn.f64 	%fd356, %fd354, %fd313, %fd355;
	mov.f64 	%fd357, 0dBFDF7E38A46D70DB;
	fma.rn.f64 	%fd358, %fd356, %fd313, %fd357;
	mov.f64 	%fd359, 0d3FEC24371844B88A;
	fma.rn.f64 	%fd360, %fd358, %fd313, %fd359;
	mul.f64 	%fd544, %fd313, %fd360;
	bra.uni 	BB70_49;

BB70_3:
	abs.f64 	%fd533, %fd2;
	setp.gtu.f64	%p6, %fd533, 0d7FF0000000000000;
	@%p6 bra 	BB70_6;
	bra.uni 	BB70_4;

BB70_6:
	add.f64 	%fd534, %fd2, %fd2;
	bra.uni 	BB70_12;

BB70_34:
	add.f64 	%fd361, %fd2, 0dC00FA9534D98569C;
	add.f64 	%fd362, %fd361, 0d3C9F06AE7804384E;
	mov.f64 	%fd363, 0dBCD2434958151AC7;
	mov.f64 	%fd364, 0dBCDAEA62AC8BDA68;
	fma.rn.f64 	%fd365, %fd364, %fd362, %fd363;
	mov.f64 	%fd366, 0d3D11C24A40D33FE1;
	fma.rn.f64 	%fd367, %fd365, %fd362, %fd366;
	mov.f64 	%fd368, 0d3D237CD62FA08CA4;
	fma.rn.f64 	%fd369, %fd367, %fd362, %fd368;
	mov.f64 	%fd370, 0dBD43902E0298C52A;
	fma.rn.f64 	%fd371, %fd369, %fd362, %fd370;
	mov.f64 	%fd372, 0dBD1DDAAD11CAB40F;
	fma.rn.f64 	%fd373, %fd371, %fd362, %fd372;
	mov.f64 	%fd374, 0dBD5209D9F06D7DE4;
	fma.rn.f64 	%fd375, %fd373, %fd362, %fd374;
	mov.f64 	%fd376, 0d3D8BB9F464468E1A;
	fma.rn.f64 	%fd377, %fd375, %fd362, %fd376;
	mov.f64 	%fd378, 0dBDA8F67B07D1B440;
	fma.rn.f64 	%fd379, %fd377, %fd362, %fd378;
	mov.f64 	%fd380, 0d3DC7C8D60F9EAECF;
	fma.rn.f64 	%fd381, %fd379, %fd362, %fd380;
	mov.f64 	%fd382, 0dBDE9703405B49A8D;
	fma.rn.f64 	%fd383, %fd381, %fd362, %fd382;
	mov.f64 	%fd384, 0d3E0A6B64E76417E4;
	fma.rn.f64 	%fd385, %fd383, %fd362, %fd384;
	mov.f64 	%fd386, 0dBE2F6B5AFB2F1359;
	fma.rn.f64 	%fd387, %fd385, %fd362, %fd386;
	mov.f64 	%fd388, 0d3E54526B71C21EC1;
	fma.rn.f64 	%fd389, %fd387, %fd362, %fd388;
	mov.f64 	%fd390, 0dBE5776DBCBBC8E1D;
	fma.rn.f64 	%fd391, %fd389, %fd362, %fd390;
	mov.f64 	%fd392, 0dBE93B211FC2DF90E;
	fma.rn.f64 	%fd393, %fd391, %fd362, %fd392;
	mov.f64 	%fd394, 0dBED486372E8562DC;
	fma.rn.f64 	%fd395, %fd393, %fd362, %fd394;
	mov.f64 	%fd396, 0d3F0AB2C1FBC3A254;
	fma.rn.f64 	%fd397, %fd395, %fd362, %fd396;
	mov.f64 	%fd398, 0d3F299827653353B8;
	fma.rn.f64 	%fd399, %fd397, %fd362, %fd398;
	mov.f64 	%fd400, 0dBF61E32BC4ED7084;
	fma.rn.f64 	%fd401, %fd399, %fd362, %fd400;
	mov.f64 	%fd402, 0dBF7C116FDC599A09;
	fma.rn.f64 	%fd403, %fd401, %fd362, %fd402;
	mov.f64 	%fd404, 0d3FADF6D59BF50C77;
	fma.rn.f64 	%fd405, %fd403, %fd362, %fd404;
	mov.f64 	%fd406, 0d3FAA09C92903680B;
	fma.rn.f64 	%fd407, %fd405, %fd362, %fd406;
	mov.f64 	%fd408, 0dBFD9C34256A12A0B;
	fma.rn.f64 	%fd409, %fd407, %fd362, %fd408;
	mul.f64 	%fd544, %fd362, %fd409;
	bra.uni 	BB70_49;

BB70_4:
	setp.eq.f64	%p7, %fd2, 0d0000000000000000;
	mov.f64 	%fd534, 0dFFF0000000000000;
	@%p7 bra 	BB70_12;

	setp.eq.f64	%p8, %fd2, 0d7FF0000000000000;
	selp.f64	%fd534, %fd2, 0dFFF8000000000000, %p8;

BB70_12:
	setp.gtu.f64	%p11, %fd533, 0d400FB319F277BBE5;
	@%p11 bra 	BB70_14;
	bra.uni 	BB70_13;

BB70_14:
	setp.gtu.f64	%p12, %fd533, 0d401C58FD1A62F5EC;
	@%p12 bra 	BB70_16;
	bra.uni 	BB70_15;

BB70_16:
	setp.gtu.f64	%p13, %fd533, 0d402471FCB6A7A8C0;
	@%p13 bra 	BB70_18;
	bra.uni 	BB70_17;

BB70_18:
	abs.f64 	%fd233, %fd533;
	mov.f64 	%fd539, 0d0000000000000000;
	setp.eq.f64	%p14, %fd233, 0d7FF0000000000000;
	@%p14 bra 	BB70_30;

	// inline asm
	rcp.approx.ftz.f64 %fd234,%fd533;
	// inline asm
	neg.f64 	%fd236, %fd533;
	mov.f64 	%fd237, 0d3FF0000000000000;
	fma.rn.f64 	%fd238, %fd236, %fd234, %fd237;
	fma.rn.f64 	%fd239, %fd238, %fd238, %fd238;
	fma.rn.f64 	%fd240, %fd239, %fd234, %fd234;
	mul.f64 	%fd241, %fd240, %fd240;
	mov.f64 	%fd242, 0d409927467A655012;
	mov.f64 	%fd243, 0dC0D115CB8C11A9DC;
	fma.rn.f64 	%fd244, %fd243, %fd241, %fd242;
	mov.f64 	%fd245, 0dC05751787E247BD4;
	fma.rn.f64 	%fd246, %fd244, %fd241, %fd245;
	mov.f64 	%fd247, 0d401704C4E5FC36B2;
	fma.rn.f64 	%fd248, %fd246, %fd241, %fd247;
	mov.f64 	%fd249, 0dBFE15B747A2FD531;
	fma.rn.f64 	%fd250, %fd248, %fd241, %fd249;
	mov.f64 	%fd251, 0d3FBA7FEACF6CB79B;
	fma.rn.f64 	%fd252, %fd250, %fd241, %fd251;
	mov.f64 	%fd253, 0dBFAFFFFFEDDCF548;
	fma.rn.f64 	%fd254, %fd252, %fd241, %fd253;
	mov.f64 	%fd255, 0d3FEFFFFFFFFFC9E5;
	fma.rn.f64 	%fd256, %fd254, %fd241, %fd255;
	mov.f64 	%fd257, 0d410ECD4523B12B84;
	mov.f64 	%fd258, 0dC14602FE1C34685E;
	fma.rn.f64 	%fd259, %fd258, %fd241, %fd257;
	mov.f64 	%fd260, 0dC0C7A2FC1972F05A;
	fma.rn.f64 	%fd261, %fd259, %fd241, %fd260;
	mov.f64 	%fd262, 0d407EBA131F7E5BEB;
	fma.rn.f64 	%fd263, %fd261, %fd241, %fd262;
	mov.f64 	%fd264, 0dC0373B92E6E7CC7D;
	fma.rn.f64 	%fd265, %fd263, %fd241, %fd264;
	mov.f64 	%fd266, 0d3FFA31BEE63A2F08;
	fma.rn.f64 	%fd267, %fd265, %fd241, %fd266;
	mov.f64 	%fd268, 0dBFCAD320104D5D05;
	fma.rn.f64 	%fd269, %fd267, %fd241, %fd268;
	mov.f64 	%fd270, 0d3FB0AAAA9C76D07E;
	fma.rn.f64 	%fd271, %fd269, %fd241, %fd270;
	mov.f64 	%fd272, 0dBFBFFFFFFFFDACEC;
	fma.rn.f64 	%fd273, %fd271, %fd241, %fd272;
	fma.rn.f64 	%fd17, %fd273, %fd240, %fd533;
	rsqrt.approx.f64 	%fd274, %fd533;
	mul.f64 	%fd275, %fd274, 0d3FE9884533D43651;
	mul.f64 	%fd18, %fd256, %fd275;
	mul.f64 	%fd276, %fd17, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r86, %fd276;
	st.local.u32 	[%r1], %r86;
	cvt.rn.f64.s32	%fd277, %r86;
	neg.f64 	%fd278, %fd277;
	mov.f64 	%fd279, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd280, %fd278, %fd279, %fd17;
	mov.f64 	%fd281, 0d3C91A62633145C00;
	fma.rn.f64 	%fd282, %fd278, %fd281, %fd280;
	mov.f64 	%fd283, 0d397B839A252049C0;
	fma.rn.f64 	%fd535, %fd278, %fd283, %fd282;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r52}, %fd17;
	}
	and.b32  	%r53, %r52, 2145386496;
	setp.lt.u32	%p15, %r53, 1105199104;
	@%p15 bra 	BB70_21;

	// Callseq Start 9
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd17;
	.param .b32 param1;
	st.param.b32	[param1+0], %r32;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd535, [retval0+0];
	
	//{
	}// Callseq End 9
	ld.local.u32 	%r86, [%r1];

BB70_21:
	and.b32  	%r55, %r86, 3;
	cvt.rn.f64.s32	%fd284, %r55;
	add.f64 	%fd285, %fd535, 0dBFE921FB54442D18;
	fma.rn.f64 	%fd536, %fd284, 0d3FF921FB54442D18, %fd285;
	abs.f64 	%fd286, %fd536;
	setp.neu.f64	%p16, %fd286, 0d7FF0000000000000;
	@%p16 bra 	BB70_23;

	mov.f64 	%fd287, 0d0000000000000000;
	mul.rn.f64 	%fd536, %fd536, %fd287;

BB70_23:
	mov.f64 	%fd527, 0d397B839A252049C0;
	mov.f64 	%fd526, 0d3C91A62633145C00;
	mov.f64 	%fd525, 0d3FF921FB54442D18;
	mul.f64 	%fd288, %fd536, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r87, %fd288;
	st.local.u32 	[%r1], %r87;
	cvt.rn.f64.s32	%fd289, %r87;
	neg.f64 	%fd290, %fd289;
	fma.rn.f64 	%fd292, %fd290, %fd525, %fd536;
	fma.rn.f64 	%fd294, %fd290, %fd526, %fd292;
	fma.rn.f64 	%fd537, %fd290, %fd527, %fd294;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r58}, %fd536;
	}
	and.b32  	%r59, %r58, 2145386496;
	setp.lt.u32	%p17, %r59, 1105199104;
	@%p17 bra 	BB70_25;

	// Callseq Start 10
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd536;
	.param .b32 param1;
	st.param.b32	[param1+0], %r32;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd537, [retval0+0];
	
	//{
	}// Callseq End 10
	ld.local.u32 	%r87, [%r1];

BB70_25:
	add.s32 	%r21, %r87, 1;
	and.b32  	%r62, %r21, 1;
	setp.eq.s32	%p18, %r62, 0;
	selp.f64	%fd296, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p18;
	shl.b32 	%r63, %r62, 6;
	mov.u32 	%r64, __cudart_sin_cos_coeffs;
	add.s32 	%r65, %r63, %r64;
	ld.const.f64 	%fd297, [%r65+8];
	mul.rn.f64 	%fd28, %fd537, %fd537;
	fma.rn.f64 	%fd298, %fd296, %fd28, %fd297;
	ld.const.f64 	%fd299, [%r65+16];
	fma.rn.f64 	%fd300, %fd298, %fd28, %fd299;
	ld.const.f64 	%fd301, [%r65+24];
	fma.rn.f64 	%fd302, %fd300, %fd28, %fd301;
	ld.const.f64 	%fd303, [%r65+32];
	fma.rn.f64 	%fd304, %fd302, %fd28, %fd303;
	ld.const.f64 	%fd305, [%r65+40];
	fma.rn.f64 	%fd306, %fd304, %fd28, %fd305;
	ld.const.f64 	%fd307, [%r65+48];
	fma.rn.f64 	%fd29, %fd306, %fd28, %fd307;
	fma.rn.f64 	%fd538, %fd29, %fd537, %fd537;
	@%p18 bra 	BB70_27;

	mov.f64 	%fd528, 0d3FF0000000000000;
	fma.rn.f64 	%fd538, %fd29, %fd28, %fd528;

BB70_27:
	and.b32  	%r66, %r21, 2;
	setp.eq.s32	%p19, %r66, 0;
	@%p19 bra 	BB70_29;

	mov.f64 	%fd309, 0d0000000000000000;
	mov.f64 	%fd310, 0dBFF0000000000000;
	fma.rn.f64 	%fd538, %fd538, %fd310, %fd309;

BB70_29:
	mul.f64 	%fd539, %fd18, %fd538;
	bra.uni 	BB70_30;

BB70_13:
	add.f64 	%fd127, %fd533, 0dC0033D152E971B40;
	add.f64 	%fd128, %fd127, 0d3CA0F539D7DA258E;
	mov.f64 	%fd129, 0dBCFCF8F9A8C294BC;
	mov.f64 	%fd130, 0dBCC0D18564C48C61;
	fma.rn.f64 	%fd131, %fd130, %fd128, %fd129;
	mov.f64 	%fd132, 0d3D3FAB983CAE498B;
	fma.rn.f64 	%fd133, %fd131, %fd128, %fd132;
	mov.f64 	%fd134, 0d3D7CD7C018579B88;
	fma.rn.f64 	%fd135, %fd133, %fd128, %fd134;
	mov.f64 	%fd136, 0dBDBBDD2342D64FDD;
	fma.rn.f64 	%fd137, %fd135, %fd128, %fd136;
	mov.f64 	%fd138, 0dBDF5C2D9416B1E2B;
	fma.rn.f64 	%fd139, %fd137, %fd128, %fd138;
	mov.f64 	%fd140, 0d3E32951D73174DD5;
	fma.rn.f64 	%fd141, %fd139, %fd128, %fd140;
	mov.f64 	%fd142, 0d3E67FF99802CAEB5;
	fma.rn.f64 	%fd143, %fd141, %fd128, %fd142;
	mov.f64 	%fd144, 0dBEA1CCE305C4C9F7;
	fma.rn.f64 	%fd145, %fd143, %fd128, %fd144;
	mov.f64 	%fd146, 0dBED232C77E29E1BB;
	fma.rn.f64 	%fd147, %fd145, %fd128, %fd146;
	mov.f64 	%fd148, 0d3F06ED3B9F0EF757;
	fma.rn.f64 	%fd149, %fd147, %fd128, %fd148;
	mov.f64 	%fd150, 0d3F315382BA096A62;
	fma.rn.f64 	%fd151, %fd149, %fd128, %fd150;
	mov.f64 	%fd152, 0dBF61F992590D1AE4;
	fma.rn.f64 	%fd153, %fd151, %fd128, %fd152;
	mov.f64 	%fd154, 0dBF81BB1CBE1A465F;
	fma.rn.f64 	%fd155, %fd153, %fd128, %fd154;
	mov.f64 	%fd156, 0d3FACFAE864368D84;
	fma.rn.f64 	%fd157, %fd155, %fd128, %fd156;
	mov.f64 	%fd158, 0d3FBBA1DEEA0294A3;
	fma.rn.f64 	%fd159, %fd157, %fd128, %fd158;
	mov.f64 	%fd160, 0dBFE09CDB36551280;
	fma.rn.f64 	%fd161, %fd159, %fd128, %fd160;
	mul.f64 	%fd539, %fd128, %fd161;
	bra.uni 	BB70_30;

BB70_15:
	add.f64 	%fd162, %fd533, 0dC016148F5B2C2E45;
	add.f64 	%fd163, %fd162, 0dBC975054CD60A517;
	mov.f64 	%fd164, 0d3CF83FD1F333EB61;
	mov.f64 	%fd165, 0d3CBCB0A8F126B343;
	fma.rn.f64 	%fd166, %fd165, %fd163, %fd164;
	mov.f64 	%fd167, 0dBD4100E33E3FB413;
	fma.rn.f64 	%fd168, %fd166, %fd163, %fd167;
	mov.f64 	%fd169, 0dBD7846076D004627;
	fma.rn.f64 	%fd170, %fd168, %fd163, %fd169;
	mov.f64 	%fd171, 0d3DBE2F1D4F90720D;
	fma.rn.f64 	%fd172, %fd170, %fd163, %fd171;
	mov.f64 	%fd173, 0d3DF1D03B1E4A119B;
	fma.rn.f64 	%fd174, %fd172, %fd163, %fd173;
	mov.f64 	%fd175, 0dBE341D72B1B3BCE9;
	fma.rn.f64 	%fd176, %fd174, %fd163, %fd175;
	mov.f64 	%fd177, 0dBE62DA37CE2A9EF8;
	fma.rn.f64 	%fd178, %fd176, %fd163, %fd177;
	mov.f64 	%fd179, 0d3EA32E6D9974F763;
	fma.rn.f64 	%fd180, %fd178, %fd163, %fd179;
	mov.f64 	%fd181, 0d3ECAD77D744A1879;
	fma.rn.f64 	%fd182, %fd180, %fd163, %fd181;
	mov.f64 	%fd183, 0dBF0863F481A37337;
	fma.rn.f64 	%fd184, %fd182, %fd163, %fd183;
	mov.f64 	%fd185, 0dBF26F641F418F0F4;
	fma.rn.f64 	%fd186, %fd184, %fd163, %fd185;
	mov.f64 	%fd187, 0d3F627E31FE9A969E;
	fma.rn.f64 	%fd188, %fd186, %fd163, %fd187;
	mov.f64 	%fd189, 0d3F72F7FFE9025628;
	fma.rn.f64 	%fd190, %fd188, %fd163, %fd189;
	mov.f64 	%fd191, 0dBFAB2150CB41E8BF;
	fma.rn.f64 	%fd192, %fd190, %fd163, %fd191;
	mov.f64 	%fd193, 0dBF9F8F72E7A848DE;
	fma.rn.f64 	%fd194, %fd192, %fd163, %fd193;
	mov.f64 	%fd195, 0d3FD5C6E60A097823;
	fma.rn.f64 	%fd196, %fd194, %fd163, %fd195;
	mul.f64 	%fd539, %fd163, %fd196;
	bra.uni 	BB70_30;

BB70_36:
	add.f64 	%fd410, %fd2, 0dC01C581DC4E72103;
	add.f64 	%fd411, %fd410, 0d3C99774A495F56CF;
	mov.f64 	%fd412, 0dBD3F443BB4F53D75;
	mov.f64 	%fd413, 0d3CF1CB3ABA718B8E;
	fma.rn.f64 	%fd414, %fd413, %fd411, %fd412;
	mov.f64 	%fd415, 0dBD770F737BD6A786;
	fma.rn.f64 	%fd416, %fd414, %fd411, %fd415;
	mov.f64 	%fd417, 0d3DBF0E9A20459E14;
	fma.rn.f64 	%fd418, %fd416, %fd411, %fd417;
	mov.f64 	%fd419, 0d3DEFA6B137D5E108;
	fma.rn.f64 	%fd420, %fd418, %fd411, %fd419;
	mov.f64 	%fd421, 0dBE344296729FB7FA;
	fma.rn.f64 	%fd422, %fd420, %fd411, %fd421;
	mov.f64 	%fd423, 0dBE60A2813A80DFAA;
	fma.rn.f64 	%fd424, %fd422, %fd411, %fd423;
	mov.f64 	%fd425, 0d3EA34AA737A83EB4;
	fma.rn.f64 	%fd426, %fd424, %fd411, %fd425;
	mov.f64 	%fd427, 0d3EC6A9227332D03C;
	fma.rn.f64 	%fd428, %fd426, %fd411, %fd427;
	mov.f64 	%fd429, 0dBF08177E4F93C81E;
	fma.rn.f64 	%fd430, %fd428, %fd411, %fd429;
	mov.f64 	%fd431, 0dBF226DD71E391775;
	fma.rn.f64 	%fd432, %fd430, %fd411, %fd431;
	mov.f64 	%fd433, 0d3F61D35E85FD7B22;
	fma.rn.f64 	%fd434, %fd432, %fd411, %fd433;
	mov.f64 	%fd435, 0d3F6B2F14A955285C;
	fma.rn.f64 	%fd436, %fd434, %fd411, %fd435;
	mov.f64 	%fd437, 0dBFA8969C64CBF388;
	fma.rn.f64 	%fd438, %fd436, %fd411, %fd437;
	mov.f64 	%fd439, 0dBF95AEF611FC4D5A;
	fma.rn.f64 	%fd440, %fd438, %fd411, %fd439;
	mov.f64 	%fd441, 0d3FD334CCA0697A5A;
	fma.rn.f64 	%fd442, %fd440, %fd411, %fd441;
	mul.f64 	%fd544, %fd411, %fd442;
	bra.uni 	BB70_49;

BB70_17:
	add.f64 	%fd197, %fd533, 0dC0214EB56CCCDECA;
	add.f64 	%fd198, %fd197, 0d3CB51970714C7C25;
	mov.f64 	%fd199, 0dBCF4B3A71AAAC629;
	mov.f64 	%fd200, 0dBCBDB7FFCF659E24;
	fma.rn.f64 	%fd201, %fd200, %fd198, %fd199;
	mov.f64 	%fd202, 0d3D417EC150ECDCE7;
	fma.rn.f64 	%fd203, %fd201, %fd198, %fd202;
	mov.f64 	%fd204, 0d3D7438F5EA1D10B2;
	fma.rn.f64 	%fd205, %fd203, %fd198, %fd204;
	mov.f64 	%fd206, 0dBDBEDAE7EC2C9E87;
	fma.rn.f64 	%fd207, %fd205, %fd198, %fd206;
	mov.f64 	%fd208, 0dBDECADD2C4B91F58;
	fma.rn.f64 	%fd209, %fd207, %fd198, %fd208;
	mov.f64 	%fd210, 0d3E34582C8EE12204;
	fma.rn.f64 	%fd211, %fd209, %fd198, %fd210;
	mov.f64 	%fd212, 0d3E5CEDA451DD20F8;
	fma.rn.f64 	%fd213, %fd211, %fd198, %fd212;
	mov.f64 	%fd214, 0dBEA30E8CC3165E2F;
	fma.rn.f64 	%fd215, %fd213, %fd198, %fd214;
	mov.f64 	%fd216, 0dBEC3324842BB1A2E;
	fma.rn.f64 	%fd217, %fd215, %fd198, %fd216;
	mov.f64 	%fd218, 0d3F07800BC54FBDDB;
	fma.rn.f64 	%fd219, %fd217, %fd198, %fd218;
	mov.f64 	%fd220, 0d3F1D79605276949A;
	fma.rn.f64 	%fd221, %fd219, %fd198, %fd220;
	mov.f64 	%fd222, 0dBF60E0D60385A629;
	fma.rn.f64 	%fd223, %fd221, %fd198, %fd222;
	mov.f64 	%fd224, 0dBF648E63600D82F3;
	fma.rn.f64 	%fd225, %fd223, %fd198, %fd224;
	mov.f64 	%fd226, 0d3FA68B984EC6493A;
	fma.rn.f64 	%fd227, %fd225, %fd198, %fd226;
	mov.f64 	%fd228, 0d3F900F7FCF183E0B;
	fma.rn.f64 	%fd229, %fd227, %fd198, %fd228;
	mov.f64 	%fd230, 0dBFD15F7977A772D4;
	fma.rn.f64 	%fd231, %fd229, %fd198, %fd230;
	mul.f64 	%fd539, %fd198, %fd231;

BB70_30:
	mul.f64 	%fd311, %fd534, 0d3FE45F306DC9C883;
	fma.rn.f64 	%fd544, %fd311, %fd539, %fd3;

BB70_49:
	setp.lt.f64	%p29, %fd1, 0d0000000000000000;
	selp.f64	%fd524, 0dFFF8000000000000, %fd544, %p29;
	cvta.to.global.u32 	%r79, %r29;
	add.s32 	%r81, %r79, %r39;
	st.global.f64 	[%r81], %fd524;

BB70_50:
	ret;
}

	// .globl	vec_y1
.visible .entry vec_y1(
	.param .u32 vec_y1_param_0,
	.param .u32 vec_y1_param_1,
	.param .u32 vec_y1_param_2
)
{
	.local .align 4 .b8 	__local_depot71[4];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<34>;
	.reg .b32 	%r<102>;
	.reg .f64 	%fd<540>;


	mov.u32 	%r101, __local_depot71;
	cvta.local.u32 	%SP, %r101;
	ld.param.u32 	%r27, [vec_y1_param_0];
	ld.param.u32 	%r25, [vec_y1_param_1];
	ld.param.u32 	%r26, [vec_y1_param_2];
	mov.u32 	%r28, %ntid.x;
	mov.u32 	%r29, %ctaid.x;
	mov.u32 	%r30, %tid.x;
	mad.lo.s32 	%r31, %r28, %r29, %r30;
	setp.ge.u32	%p1, %r31, %r27;
	@%p1 bra 	BB71_54;

	cvta.to.global.u32 	%r32, %r26;
	shl.b32 	%r37, %r31, 3;
	add.s32 	%r38, %r32, %r37;
	ld.global.f64 	%fd1, [%r38];
	abs.f64 	%fd2, %fd1;
	setp.lt.f64	%p2, %fd2, 0d000730D67819E8D2;
	@%p2 bra 	BB71_50;
	bra.uni 	BB71_2;

BB71_50:
	mov.f64 	%fd517, 0dBFE45F306DC9C883;
	div.rn.f64 	%fd539, %fd517, %fd2;
	bra.uni 	BB71_51;

BB71_2:
	setp.gtu.f64	%p3, %fd2, 0d3FF4C6F208132576;
	@%p3 bra 	BB71_32;
	bra.uni 	BB71_3;

BB71_32:
	setp.gtu.f64	%p23, %fd2, 0d4009B510EC2ADC83;
	@%p23 bra 	BB71_34;
	bra.uni 	BB71_33;

BB71_34:
	setp.gtu.f64	%p24, %fd2, 0d401C0D26D5A541CB;
	@%p24 bra 	BB71_36;
	bra.uni 	BB71_35;

BB71_36:
	setp.gtu.f64	%p25, %fd2, 0d4022585C739ACDDD;
	@%p25 bra 	BB71_38;
	bra.uni 	BB71_37;

BB71_38:
	abs.f64 	%fd437, %fd2;
	mov.f64 	%fd539, 0d0000000000000000;
	setp.eq.f64	%p26, %fd437, 0d7FF0000000000000;
	@%p26 bra 	BB71_51;

	// inline asm
	rcp.approx.ftz.f64 %fd438,%fd2;
	// inline asm
	neg.f64 	%fd440, %fd2;
	mov.f64 	%fd441, 0d3FF0000000000000;
	fma.rn.f64 	%fd442, %fd440, %fd438, %fd441;
	fma.rn.f64 	%fd443, %fd442, %fd442, %fd442;
	fma.rn.f64 	%fd444, %fd443, %fd438, %fd438;
	mul.f64 	%fd445, %fd444, %fd444;
	mov.f64 	%fd446, 0dC09C26E89385D5B1;
	mov.f64 	%fd447, 0d40D13DB326ECEBFE;
	fma.rn.f64 	%fd448, %fd447, %fd445, %fd446;
	mov.f64 	%fd449, 0d405C6AB923C6F55E;
	fma.rn.f64 	%fd450, %fd448, %fd445, %fd449;
	mov.f64 	%fd451, 0dC01E61EAF3BD2FA1;
	fma.rn.f64 	%fd452, %fd450, %fd445, %fd451;
	mov.f64 	%fd453, 0d3FE9BF15D9B97DD1;
	fma.rn.f64 	%fd454, %fd452, %fd445, %fd453;
	mov.f64 	%fd455, 0dBFC8BFECF93D7D19;
	fma.rn.f64 	%fd456, %fd454, %fd445, %fd455;
	mov.f64 	%fd457, 0d3FC7FFFFF756AA6C;
	fma.rn.f64 	%fd458, %fd456, %fd445, %fd457;
	mov.f64 	%fd459, 0d3FF0000000003646;
	fma.rn.f64 	%fd460, %fd458, %fd445, %fd459;
	mov.f64 	%fd461, 0d416024E99BA46E7B;
	mov.f64 	%fd462, 0dC1943281A050209C;
	fma.rn.f64 	%fd463, %fd462, %fd445, %fd461;
	mov.f64 	%fd464, 0dC11A6875D7DFBD65;
	fma.rn.f64 	%fd465, %fd463, %fd445, %fd464;
	mov.f64 	%fd466, 0d40D032C041790233;
	fma.rn.f64 	%fd467, %fd465, %fd445, %fd466;
	mov.f64 	%fd468, 0dC0839F895BC22946;
	fma.rn.f64 	%fd469, %fd467, %fd445, %fd468;
	mov.f64 	%fd470, 0d403E77CC78ECD2D8;
	fma.rn.f64 	%fd471, %fd469, %fd445, %fd470;
	mov.f64 	%fd472, 0dC002F368D0117BE9;
	fma.rn.f64 	%fd473, %fd471, %fd445, %fd472;
	mov.f64 	%fd474, 0d3FD7BCC786009A25;
	fma.rn.f64 	%fd475, %fd473, %fd445, %fd474;
	mov.f64 	%fd476, 0dBFC4FFFFFC51BC7A;
	fma.rn.f64 	%fd477, %fd475, %fd445, %fd476;
	mov.f64 	%fd478, 0d3FD7FFFFFFFFB5EA;
	fma.rn.f64 	%fd479, %fd477, %fd445, %fd478;
	fma.rn.f64 	%fd41, %fd479, %fd444, %fd2;
	rsqrt.approx.f64 	%fd480, %fd2;
	mul.f64 	%fd481, %fd480, 0d3FE9884533D43651;
	mul.f64 	%fd42, %fd460, %fd481;
	mul.f64 	%fd482, %fd41, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r99, %fd482;
	add.u32 	%r68, %SP, 0;
	cvta.to.local.u32 	%r69, %r68;
	st.local.u32 	[%r69], %r99;
	cvt.rn.f64.s32	%fd483, %r99;
	neg.f64 	%fd484, %fd483;
	mov.f64 	%fd485, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd486, %fd484, %fd485, %fd41;
	mov.f64 	%fd487, 0d3C91A62633145C00;
	fma.rn.f64 	%fd488, %fd484, %fd487, %fd486;
	mov.f64 	%fd489, 0d397B839A252049C0;
	fma.rn.f64 	%fd535, %fd484, %fd489, %fd488;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r70}, %fd41;
	}
	and.b32  	%r71, %r70, 2145386496;
	setp.lt.u32	%p27, %r71, 1105199104;
	@%p27 bra 	BB71_41;

	// Callseq Start 15
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd41;
	.param .b32 param1;
	st.param.b32	[param1+0], %r68;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd535, [retval0+0];
	
	//{
	}// Callseq End 15
	ld.local.u32 	%r99, [%r69];

BB71_41:
	and.b32  	%r74, %r99, 3;
	cvt.rn.f64.s32	%fd490, %r74;
	add.f64 	%fd491, %fd535, 0dC00F6A7A2955385E;
	fma.rn.f64 	%fd536, %fd490, 0d3FF921FB54442D18, %fd491;
	abs.f64 	%fd492, %fd536;
	setp.neu.f64	%p28, %fd492, 0d7FF0000000000000;
	@%p28 bra 	BB71_43;

	mov.f64 	%fd493, 0d0000000000000000;
	mul.rn.f64 	%fd536, %fd536, %fd493;

BB71_43:
	mov.f64 	%fd525, 0d397B839A252049C0;
	mov.f64 	%fd524, 0d3C91A62633145C00;
	mov.f64 	%fd523, 0d3FF921FB54442D18;
	mul.f64 	%fd494, %fd536, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r100, %fd494;
	st.local.u32 	[%r69], %r100;
	cvt.rn.f64.s32	%fd495, %r100;
	neg.f64 	%fd496, %fd495;
	fma.rn.f64 	%fd498, %fd496, %fd523, %fd536;
	fma.rn.f64 	%fd500, %fd496, %fd524, %fd498;
	fma.rn.f64 	%fd537, %fd496, %fd525, %fd500;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r77}, %fd536;
	}
	and.b32  	%r78, %r77, 2145386496;
	setp.lt.u32	%p29, %r78, 1105199104;
	@%p29 bra 	BB71_45;

	// Callseq Start 16
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd536;
	.param .b32 param1;
	st.param.b32	[param1+0], %r68;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd537, [retval0+0];
	
	//{
	}// Callseq End 16
	ld.local.u32 	%r100, [%r69];

BB71_45:
	add.s32 	%r24, %r100, 1;
	and.b32  	%r81, %r24, 1;
	setp.eq.s32	%p30, %r81, 0;
	selp.f64	%fd502, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p30;
	shl.b32 	%r82, %r81, 6;
	mov.u32 	%r83, __cudart_sin_cos_coeffs;
	add.s32 	%r84, %r82, %r83;
	ld.const.f64 	%fd503, [%r84+8];
	mul.rn.f64 	%fd52, %fd537, %fd537;
	fma.rn.f64 	%fd504, %fd502, %fd52, %fd503;
	ld.const.f64 	%fd505, [%r84+16];
	fma.rn.f64 	%fd506, %fd504, %fd52, %fd505;
	ld.const.f64 	%fd507, [%r84+24];
	fma.rn.f64 	%fd508, %fd506, %fd52, %fd507;
	ld.const.f64 	%fd509, [%r84+32];
	fma.rn.f64 	%fd510, %fd508, %fd52, %fd509;
	ld.const.f64 	%fd511, [%r84+40];
	fma.rn.f64 	%fd512, %fd510, %fd52, %fd511;
	ld.const.f64 	%fd513, [%r84+48];
	fma.rn.f64 	%fd53, %fd512, %fd52, %fd513;
	fma.rn.f64 	%fd538, %fd53, %fd537, %fd537;
	@%p30 bra 	BB71_47;

	mov.f64 	%fd526, 0d3FF0000000000000;
	fma.rn.f64 	%fd538, %fd53, %fd52, %fd526;

BB71_47:
	and.b32  	%r85, %r24, 2;
	setp.eq.s32	%p31, %r85, 0;
	@%p31 bra 	BB71_49;

	mov.f64 	%fd515, 0d0000000000000000;
	mov.f64 	%fd516, 0dBFF0000000000000;
	fma.rn.f64 	%fd538, %fd538, %fd516, %fd515;

BB71_49:
	mul.f64 	%fd539, %fd42, %fd538;
	bra.uni 	BB71_51;

BB71_3:
	mul.f64 	%fd64, %fd2, %fd2;
	mov.f64 	%fd65, 0dBDCF0B5B1FB7B95E;
	mov.f64 	%fd66, 0d3D5249F90687428C;
	fma.rn.f64 	%fd67, %fd66, %fd64, %fd65;
	mov.f64 	%fd68, 0d3E432E589311FA14;
	fma.rn.f64 	%fd69, %fd67, %fd64, %fd68;
	mov.f64 	%fd70, 0dBEB0A780AA4A92E9;
	fma.rn.f64 	%fd71, %fd69, %fd64, %fd70;
	mov.f64 	%fd72, 0d3F12C7DBFFCAEC2B;
	fma.rn.f64 	%fd73, %fd71, %fd64, %fd72;
	mov.f64 	%fd74, 0dBF6835B97894BA4A;
	fma.rn.f64 	%fd75, %fd73, %fd64, %fd74;
	mov.f64 	%fd76, 0d3FABD3975C75B4A3;
	fma.rn.f64 	%fd77, %fd75, %fd64, %fd76;
	mov.f64 	%fd78, 0dBFC91866143CBC8A;
	fma.rn.f64 	%fd3, %fd77, %fd64, %fd78;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r93}, %fd2;
	}
	setp.lt.s32	%p4, %r93, 2146435072;
	setp.gt.f64	%p5, %fd2, 0d0000000000000000;
	and.pred  	%p6, %p5, %p4;
	@%p6 bra 	BB71_8;
	bra.uni 	BB71_4;

BB71_8:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r94, %temp}, %fd2;
	}
	mov.u32 	%r95, -1023;
	setp.gt.s32	%p10, %r93, 1048575;
	@%p10 bra 	BB71_10;

	mul.f64 	%fd80, %fd2, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r93}, %fd80;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r94, %temp}, %fd80;
	}
	mov.u32 	%r95, -1077;

BB71_10:
	shr.u32 	%r41, %r93, 20;
	add.s32 	%r96, %r95, %r41;
	and.b32  	%r42, %r93, -2146435073;
	or.b32  	%r43, %r42, 1072693248;
	mov.b64 	%fd527, {%r94, %r43};
	setp.lt.s32	%p11, %r43, 1073127583;
	@%p11 bra 	BB71_12;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r44, %temp}, %fd527;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r45}, %fd527;
	}
	add.s32 	%r46, %r45, -1048576;
	mov.b64 	%fd527, {%r44, %r46};
	add.s32 	%r96, %r96, 1;

BB71_12:
	add.f64 	%fd82, %fd527, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd81,%fd82;
	// inline asm
	neg.f64 	%fd83, %fd82;
	mov.f64 	%fd84, 0d3FF0000000000000;
	fma.rn.f64 	%fd85, %fd83, %fd81, %fd84;
	fma.rn.f64 	%fd86, %fd85, %fd85, %fd85;
	fma.rn.f64 	%fd87, %fd86, %fd81, %fd81;
	add.f64 	%fd88, %fd527, 0dBFF0000000000000;
	mul.f64 	%fd89, %fd88, %fd87;
	fma.rn.f64 	%fd90, %fd88, %fd87, %fd89;
	mul.f64 	%fd91, %fd90, %fd90;
	mov.f64 	%fd92, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd93, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd94, %fd93, %fd91, %fd92;
	mov.f64 	%fd95, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd96, %fd94, %fd91, %fd95;
	mov.f64 	%fd97, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd98, %fd96, %fd91, %fd97;
	mov.f64 	%fd99, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd100, %fd98, %fd91, %fd99;
	mov.f64 	%fd101, 0d3F624924923BE72D;
	fma.rn.f64 	%fd102, %fd100, %fd91, %fd101;
	mov.f64 	%fd103, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd104, %fd102, %fd91, %fd103;
	mov.f64 	%fd105, 0d3FB5555555555554;
	fma.rn.f64 	%fd106, %fd104, %fd91, %fd105;
	sub.f64 	%fd107, %fd88, %fd90;
	add.f64 	%fd108, %fd107, %fd107;
	neg.f64 	%fd109, %fd90;
	fma.rn.f64 	%fd110, %fd109, %fd88, %fd108;
	mul.f64 	%fd111, %fd87, %fd110;
	mul.f64 	%fd112, %fd91, %fd106;
	fma.rn.f64 	%fd113, %fd112, %fd90, %fd111;
	xor.b32  	%r47, %r96, -2147483648;
	mov.u32 	%r48, 1127219200;
	mov.b64 	%fd114, {%r47, %r48};
	mov.u32 	%r49, -2147483648;
	mov.b64 	%fd115, {%r49, %r48};
	sub.f64 	%fd116, %fd114, %fd115;
	mov.f64 	%fd117, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd118, %fd116, %fd117, %fd90;
	neg.f64 	%fd119, %fd116;
	fma.rn.f64 	%fd120, %fd119, %fd117, %fd118;
	sub.f64 	%fd121, %fd120, %fd90;
	sub.f64 	%fd122, %fd113, %fd121;
	mov.f64 	%fd123, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd124, %fd116, %fd123, %fd122;
	add.f64 	%fd529, %fd118, %fd124;
	abs.f64 	%fd528, %fd2;
	bra.uni 	BB71_13;

BB71_33:
	add.f64 	%fd311, %fd2, 0dC00193BED4DFF243;
	add.f64 	%fd312, %fd311, 0d3C8BD1E50D219BFD;
	mov.f64 	%fd313, 0d3E4833AAE4D8B975;
	mov.f64 	%fd314, 0dBE2B87B0BE2AA150;
	fma.rn.f64 	%fd315, %fd314, %fd312, %fd313;
	mov.f64 	%fd316, 0dBE44E279B423E68F;
	fma.rn.f64 	%fd317, %fd315, %fd312, %fd316;
	mov.f64 	%fd318, 0d3E129DC6A747EB4F;
	fma.rn.f64 	%fd319, %fd317, %fd312, %fd318;
	mov.f64 	%fd320, 0dBE61D15534496CD8;
	fma.rn.f64 	%fd321, %fd319, %fd312, %fd320;
	mov.f64 	%fd322, 0d3E7EEC8D48FECE00;
	fma.rn.f64 	%fd323, %fd321, %fd312, %fd322;
	mov.f64 	%fd324, 0dBE8D1180AF70A134;
	fma.rn.f64 	%fd325, %fd323, %fd312, %fd324;
	mov.f64 	%fd326, 0d3E9C8386A0EA1388;
	fma.rn.f64 	%fd327, %fd325, %fd312, %fd326;
	mov.f64 	%fd328, 0dBEB01A014E7F3250;
	fma.rn.f64 	%fd329, %fd327, %fd312, %fd328;
	mov.f64 	%fd330, 0d3EC1FB752010A320;
	fma.rn.f64 	%fd331, %fd329, %fd312, %fd330;
	mov.f64 	%fd332, 0dBED3AA0AFF4E332B;
	fma.rn.f64 	%fd333, %fd331, %fd312, %fd332;
	mov.f64 	%fd334, 0d3EE584A6C77F6700;
	fma.rn.f64 	%fd335, %fd333, %fd312, %fd334;
	mov.f64 	%fd336, 0dBEF794C520FC2EBB;
	fma.rn.f64 	%fd337, %fd335, %fd312, %fd336;
	mov.f64 	%fd338, 0d3F09D18D2D35CC71;
	fma.rn.f64 	%fd339, %fd337, %fd312, %fd338;
	mov.f64 	%fd340, 0dBF1C3FB7315C4599;
	fma.rn.f64 	%fd341, %fd339, %fd312, %fd340;
	mov.f64 	%fd342, 0d3F2EEA7ADECCE927;
	fma.rn.f64 	%fd343, %fd341, %fd312, %fd342;
	mov.f64 	%fd344, 0dBF40B2D85257446F;
	fma.rn.f64 	%fd345, %fd343, %fd312, %fd344;
	mov.f64 	%fd346, 0d3F517AB4B1FE5D5B;
	fma.rn.f64 	%fd347, %fd345, %fd312, %fd346;
	mov.f64 	%fd348, 0dBF65429DC6516C0D;
	fma.rn.f64 	%fd349, %fd347, %fd312, %fd348;
	mov.f64 	%fd350, 0d3F7E671C7D0B090B;
	fma.rn.f64 	%fd351, %fd349, %fd312, %fd350;
	mov.f64 	%fd352, 0dBF73A6DEC36FB27C;
	fma.rn.f64 	%fd353, %fd351, %fd312, %fd352;
	mov.f64 	%fd354, 0dBFA0D2AF4E931FD1;
	fma.rn.f64 	%fd355, %fd353, %fd312, %fd354;
	mov.f64 	%fd356, 0dBFBE56F82217B964;
	fma.rn.f64 	%fd357, %fd355, %fd312, %fd356;
	mov.f64 	%fd358, 0d3FE0AA48442F014B;
	fma.rn.f64 	%fd359, %fd357, %fd312, %fd358;
	mul.f64 	%fd539, %fd312, %fd359;
	bra.uni 	BB71_51;

BB71_4:
	abs.f64 	%fd528, %fd2;
	setp.gtu.f64	%p7, %fd528, 0d7FF0000000000000;
	@%p7 bra 	BB71_7;
	bra.uni 	BB71_5;

BB71_7:
	add.f64 	%fd529, %fd2, %fd2;
	bra.uni 	BB71_13;

BB71_35:
	add.f64 	%fd360, %fd2, 0dC015B7FE4E87B02E;
	add.f64 	%fd361, %fd360, 0dBCBDFE7BAC228E8C;
	mov.f64 	%fd362, 0d3CC69A30996793E2;
	mov.f64 	%fd363, 0d3CBA3C76069F1D8C;
	fma.rn.f64 	%fd364, %fd363, %fd361, %fd362;
	mov.f64 	%fd365, 0dBCDDD8432FE756E7;
	fma.rn.f64 	%fd366, %fd364, %fd361, %fd365;
	mov.f64 	%fd367, 0dBD143158EEE220F7;
	fma.rn.f64 	%fd368, %fd366, %fd361, %fd367;
	mov.f64 	%fd369, 0d3D28D44491230F5A;
	fma.rn.f64 	%fd370, %fd368, %fd361, %fd369;
	mov.f64 	%fd371, 0dBD438842EAF4EDBC;
	fma.rn.f64 	%fd372, %fd370, %fd361, %fd371;
	mov.f64 	%fd373, 0d3D74958DAFBFAF5A;
	fma.rn.f64 	%fd374, %fd372, %fd361, %fd373;
	mov.f64 	%fd375, 0dBD9449A60E664848;
	fma.rn.f64 	%fd376, %fd374, %fd361, %fd375;
	mov.f64 	%fd377, 0d3D838BC8CD594A76;
	fma.rn.f64 	%fd378, %fd376, %fd361, %fd377;
	mov.f64 	%fd379, 0dBDFA798002141323;
	fma.rn.f64 	%fd380, %fd378, %fd361, %fd379;
	mov.f64 	%fd381, 0d3E380B4198956AAA;
	fma.rn.f64 	%fd382, %fd380, %fd361, %fd381;
	mov.f64 	%fd383, 0d3E5B62B5F21BACD4;
	fma.rn.f64 	%fd384, %fd382, %fd361, %fd383;
	mov.f64 	%fd385, 0dBEA255E729FB6AAE;
	fma.rn.f64 	%fd386, %fd384, %fd361, %fd385;
	mov.f64 	%fd387, 0dBEC80618F6BAE5AA;
	fma.rn.f64 	%fd388, %fd386, %fd361, %fd387;
	mov.f64 	%fd389, 0d3F085B940F8E8D36;
	fma.rn.f64 	%fd390, %fd388, %fd361, %fd389;
	mov.f64 	%fd391, 0d3F2337C7E10E14E8;
	fma.rn.f64 	%fd392, %fd390, %fd361, %fd391;
	mov.f64 	%fd393, 0dBF61BE6DB99332CA;
	fma.rn.f64 	%fd394, %fd392, %fd361, %fd393;
	mov.f64 	%fd395, 0dBF710A329E2BE9B8;
	fma.rn.f64 	%fd396, %fd394, %fd361, %fd395;
	mov.f64 	%fd397, 0d3FAA15D92DFE3FCF;
	fma.rn.f64 	%fd398, %fd396, %fd361, %fd397;
	mov.f64 	%fd399, 0d3FA00B9F8571C9BE;
	fma.rn.f64 	%fd400, %fd398, %fd361, %fd399;
	mov.f64 	%fd401, 0dBFD5C7C556F0C19A;
	fma.rn.f64 	%fd402, %fd400, %fd361, %fd401;
	mul.f64 	%fd539, %fd361, %fd402;
	bra.uni 	BB71_51;

BB71_5:
	setp.eq.f64	%p8, %fd2, 0d0000000000000000;
	mov.f64 	%fd529, 0dFFF0000000000000;
	@%p8 bra 	BB71_13;

	setp.eq.f64	%p9, %fd2, 0d7FF0000000000000;
	selp.f64	%fd529, %fd2, 0dFFF8000000000000, %p9;

BB71_13:
	setp.gtu.f64	%p12, %fd528, 0d400353AABAD7B784;
	@%p12 bra 	BB71_15;
	bra.uni 	BB71_14;

BB71_15:
	setp.gtu.f64	%p13, %fd528, 0d4015B1D0574614EA;
	@%p13 bra 	BB71_17;
	bra.uni 	BB71_16;

BB71_17:
	setp.gtu.f64	%p14, %fd528, 0d40213065E54C1AA9;
	@%p14 bra 	BB71_19;
	bra.uni 	BB71_18;

BB71_19:
	abs.f64 	%fd223, %fd528;
	mov.f64 	%fd534, 0d0000000000000000;
	setp.eq.f64	%p15, %fd223, 0d7FF0000000000000;
	@%p15 bra 	BB71_31;

	// inline asm
	rcp.approx.ftz.f64 %fd224,%fd528;
	// inline asm
	neg.f64 	%fd226, %fd528;
	mov.f64 	%fd227, 0d3FF0000000000000;
	fma.rn.f64 	%fd228, %fd226, %fd224, %fd227;
	fma.rn.f64 	%fd229, %fd228, %fd228, %fd228;
	fma.rn.f64 	%fd230, %fd229, %fd224, %fd224;
	mul.f64 	%fd231, %fd230, %fd230;
	mov.f64 	%fd232, 0dC099C06322A3F8BE;
	mov.f64 	%fd233, 0d40CD02EA3F2F6751;
	fma.rn.f64 	%fd234, %fd233, %fd231, %fd232;
	mov.f64 	%fd235, 0d405B89354DA77324;
	fma.rn.f64 	%fd236, %fd234, %fd231, %fd235;
	mov.f64 	%fd237, 0dC01E352294653188;
	fma.rn.f64 	%fd238, %fd236, %fd231, %fd237;
	mov.f64 	%fd239, 0d3FE9BC7DB16BD7A7;
	fma.rn.f64 	%fd240, %fd238, %fd231, %fd239;
	mov.f64 	%fd241, 0dBFC8BFE1C3A4F741;
	fma.rn.f64 	%fd242, %fd240, %fd231, %fd241;
	mov.f64 	%fd243, 0d3FC7FFFFF0D00BE2;
	fma.rn.f64 	%fd244, %fd242, %fd231, %fd243;
	mov.f64 	%fd245, 0d3FF00000000068CC;
	fma.rn.f64 	%fd246, %fd244, %fd231, %fd245;
	mov.f64 	%fd247, 0d415A30AC6857BEE0;
	mov.f64 	%fd248, 0dC18DA26B212FDC9A;
	fma.rn.f64 	%fd249, %fd248, %fd231, %fd247;
	mov.f64 	%fd250, 0dC11764222AD7C910;
	fma.rn.f64 	%fd251, %fd249, %fd231, %fd250;
	mov.f64 	%fd252, 0d40CEB02E0C306857;
	fma.rn.f64 	%fd253, %fd251, %fd231, %fd252;
	mov.f64 	%fd254, 0dC08351859FA2B23B;
	fma.rn.f64 	%fd255, %fd253, %fd231, %fd254;
	mov.f64 	%fd256, 0d403E65A07AF51F42;
	fma.rn.f64 	%fd257, %fd255, %fd231, %fd256;
	mov.f64 	%fd258, 0dC002F2B817F77A57;
	fma.rn.f64 	%fd259, %fd257, %fd231, %fd258;
	mov.f64 	%fd260, 0d3FD7BCC34DA069FD;
	fma.rn.f64 	%fd261, %fd259, %fd231, %fd260;
	mov.f64 	%fd262, 0dBFC4FFFFF8A44463;
	fma.rn.f64 	%fd263, %fd261, %fd231, %fd262;
	mov.f64 	%fd264, 0d3FD7FFFFFFFF5CD7;
	fma.rn.f64 	%fd265, %fd263, %fd231, %fd264;
	fma.rn.f64 	%fd17, %fd265, %fd230, %fd528;
	rsqrt.approx.f64 	%fd266, %fd528;
	mul.f64 	%fd267, %fd266, 0d3FE9884533D43651;
	mul.f64 	%fd18, %fd246, %fd267;
	mul.f64 	%fd268, %fd17, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r97, %fd268;
	add.u32 	%r50, %SP, 0;
	cvta.to.local.u32 	%r51, %r50;
	st.local.u32 	[%r51], %r97;
	cvt.rn.f64.s32	%fd269, %r97;
	neg.f64 	%fd270, %fd269;
	mov.f64 	%fd271, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd272, %fd270, %fd271, %fd17;
	mov.f64 	%fd273, 0d3C91A62633145C00;
	fma.rn.f64 	%fd274, %fd270, %fd273, %fd272;
	mov.f64 	%fd275, 0d397B839A252049C0;
	fma.rn.f64 	%fd530, %fd270, %fd275, %fd274;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r52}, %fd17;
	}
	and.b32  	%r53, %r52, 2145386496;
	setp.lt.u32	%p16, %r53, 1105199104;
	@%p16 bra 	BB71_22;

	// Callseq Start 13
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd17;
	.param .b32 param1;
	st.param.b32	[param1+0], %r50;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd530, [retval0+0];
	
	//{
	}// Callseq End 13
	ld.local.u32 	%r97, [%r51];

BB71_22:
	and.b32  	%r56, %r97, 3;
	cvt.rn.f64.s32	%fd276, %r56;
	add.f64 	%fd277, %fd530, 0dC002D97C7F3321D2;
	fma.rn.f64 	%fd531, %fd276, 0d3FF921FB54442D18, %fd277;
	abs.f64 	%fd278, %fd531;
	setp.neu.f64	%p17, %fd278, 0d7FF0000000000000;
	@%p17 bra 	BB71_24;

	mov.f64 	%fd279, 0d0000000000000000;
	mul.rn.f64 	%fd531, %fd531, %fd279;

BB71_24:
	mov.f64 	%fd520, 0d397B839A252049C0;
	mov.f64 	%fd519, 0d3C91A62633145C00;
	mov.f64 	%fd518, 0d3FF921FB54442D18;
	mul.f64 	%fd280, %fd531, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r98, %fd280;
	st.local.u32 	[%r51], %r98;
	cvt.rn.f64.s32	%fd281, %r98;
	neg.f64 	%fd282, %fd281;
	fma.rn.f64 	%fd284, %fd282, %fd518, %fd531;
	fma.rn.f64 	%fd286, %fd282, %fd519, %fd284;
	fma.rn.f64 	%fd532, %fd282, %fd520, %fd286;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r59}, %fd531;
	}
	and.b32  	%r60, %r59, 2145386496;
	setp.lt.u32	%p18, %r60, 1105199104;
	@%p18 bra 	BB71_26;

	// Callseq Start 14
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd531;
	.param .b32 param1;
	st.param.b32	[param1+0], %r50;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd532, [retval0+0];
	
	//{
	}// Callseq End 14
	ld.local.u32 	%r98, [%r51];

BB71_26:
	add.s32 	%r17, %r98, 1;
	and.b32  	%r63, %r17, 1;
	setp.eq.s32	%p19, %r63, 0;
	selp.f64	%fd288, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p19;
	shl.b32 	%r64, %r63, 6;
	mov.u32 	%r65, __cudart_sin_cos_coeffs;
	add.s32 	%r66, %r64, %r65;
	ld.const.f64 	%fd289, [%r66+8];
	mul.rn.f64 	%fd28, %fd532, %fd532;
	fma.rn.f64 	%fd290, %fd288, %fd28, %fd289;
	ld.const.f64 	%fd291, [%r66+16];
	fma.rn.f64 	%fd292, %fd290, %fd28, %fd291;
	ld.const.f64 	%fd293, [%r66+24];
	fma.rn.f64 	%fd294, %fd292, %fd28, %fd293;
	ld.const.f64 	%fd295, [%r66+32];
	fma.rn.f64 	%fd296, %fd294, %fd28, %fd295;
	ld.const.f64 	%fd297, [%r66+40];
	fma.rn.f64 	%fd298, %fd296, %fd28, %fd297;
	ld.const.f64 	%fd299, [%r66+48];
	fma.rn.f64 	%fd29, %fd298, %fd28, %fd299;
	fma.rn.f64 	%fd533, %fd29, %fd532, %fd532;
	@%p19 bra 	BB71_28;

	mov.f64 	%fd521, 0d3FF0000000000000;
	fma.rn.f64 	%fd533, %fd29, %fd28, %fd521;

BB71_28:
	and.b32  	%r67, %r17, 2;
	setp.eq.s32	%p20, %r67, 0;
	@%p20 bra 	BB71_30;

	mov.f64 	%fd301, 0d0000000000000000;
	mov.f64 	%fd302, 0dBFF0000000000000;
	fma.rn.f64 	%fd533, %fd533, %fd302, %fd301;

BB71_30:
	mul.f64 	%fd534, %fd18, %fd533;
	bra.uni 	BB71_31;

BB71_14:
	mov.f64 	%fd125, 0dBD4DD167A0DC3F55;
	mov.f64 	%fd126, 0d3D020E4ADCDE2AD3;
	fma.rn.f64 	%fd127, %fd126, %fd528, %fd125;
	mov.f64 	%fd128, 0d3D5503F5A491E487;
	fma.rn.f64 	%fd129, %fd127, %fd528, %fd128;
	mov.f64 	%fd130, 0d3DC1F29940C2403A;
	fma.rn.f64 	%fd131, %fd129, %fd528, %fd130;
	mov.f64 	%fd132, 0d3D84CF9302EACDEF;
	fma.rn.f64 	%fd133, %fd131, %fd528, %fd132;
	mov.f64 	%fd134, 0dBE384A53DBBCA436;
	fma.rn.f64 	%fd135, %fd133, %fd528, %fd134;
	mov.f64 	%fd136, 0d3D9779BEE4F63BCC;
	fma.rn.f64 	%fd137, %fd135, %fd528, %fd136;
	mov.f64 	%fd138, 0d3EA6C160E414F3F0;
	fma.rn.f64 	%fd139, %fd137, %fd528, %fd138;
	mov.f64 	%fd140, 0d3D8F3D2F12430699;
	fma.rn.f64 	%fd141, %fd139, %fd528, %fd140;
	mov.f64 	%fd142, 0dBF0C71C72C0CED04;
	fma.rn.f64 	%fd143, %fd141, %fd528, %fd142;
	mov.f64 	%fd144, 0d3D659BCA506F1128;
	fma.rn.f64 	%fd145, %fd143, %fd528, %fd144;
	mov.f64 	%fd146, 0d3F65555555506982;
	fma.rn.f64 	%fd147, %fd145, %fd528, %fd146;
	mov.f64 	%fd148, 0d3D15BA0B425F1BFB;
	fma.rn.f64 	%fd149, %fd147, %fd528, %fd148;
	mov.f64 	%fd150, 0dBFB0000000000065;
	fma.rn.f64 	%fd151, %fd149, %fd528, %fd150;
	mov.f64 	%fd152, 0d3C8729A7253FB679;
	fma.rn.f64 	%fd153, %fd151, %fd528, %fd152;
	mov.f64 	%fd154, 0d3FE0000000000000;
	fma.rn.f64 	%fd155, %fd153, %fd528, %fd154;
	mul.f64 	%fd534, %fd528, %fd155;
	bra.uni 	BB71_31;

BB71_16:
	add.f64 	%fd156, %fd528, 0dC00EA75575AF6F09;
	add.f64 	%fd157, %fd156, 0d3CA60155A9D1B256;
	mov.f64 	%fd158, 0d3D41011A1DF02DAD;
	mov.f64 	%fd159, 0dBCF8D3CDBB60175E;
	fma.rn.f64 	%fd160, %fd159, %fd157, %fd158;
	mov.f64 	%fd161, 0d3D76013AC1E5E222;
	fma.rn.f64 	%fd162, %fd160, %fd157, %fd161;
	mov.f64 	%fd163, 0dBDBEC315D96D5F03;
	fma.rn.f64 	%fd164, %fd162, %fd157, %fd163;
	mov.f64 	%fd165, 0dBDF03BE1B4B57207;
	fma.rn.f64 	%fd166, %fd164, %fd157, %fd165;
	mov.f64 	%fd167, 0d3E345695F8B660F7;
	fma.rn.f64 	%fd168, %fd166, %fd157, %fd167;
	mov.f64 	%fd169, 0d3E617069FCFCFFF4;
	fma.rn.f64 	%fd170, %fd168, %fd157, %fd169;
	mov.f64 	%fd171, 0dBEA33825C36745EB;
	fma.rn.f64 	%fd172, %fd170, %fd157, %fd171;
	mov.f64 	%fd173, 0dBEC9799D4F90931B;
	fma.rn.f64 	%fd174, %fd172, %fd157, %fd173;
	mov.f64 	%fd175, 0d3F083A06E2F7DF13;
	fma.rn.f64 	%fd176, %fd174, %fd157, %fd175;
	mov.f64 	%fd177, 0d3F26E4C2D53A7CF6;
	fma.rn.f64 	%fd178, %fd176, %fd157, %fd177;
	mov.f64 	%fd179, 0dBF624B3409957B1C;
	fma.rn.f64 	%fd180, %fd178, %fd157, %fd179;
	mov.f64 	%fd181, 0dBF7537544C3325DF;
	fma.rn.f64 	%fd182, %fd180, %fd157, %fd181;
	mov.f64 	%fd183, 0d3FAB589D1DA138E2;
	fma.rn.f64 	%fd184, %fd182, %fd157, %fd183;
	mov.f64 	%fd185, 0d3FAAE8A39F51AD13;
	fma.rn.f64 	%fd186, %fd184, %fd157, %fd185;
	mov.f64 	%fd187, 0dBFD9C6CF582CBF7F;
	fma.rn.f64 	%fd188, %fd186, %fd157, %fd187;
	mul.f64 	%fd534, %fd157, %fd188;
	bra.uni 	BB71_31;

BB71_37:
	add.f64 	%fd403, %fd2, 0dC0213127AE6169B4;
	add.f64 	%fd404, %fd403, 0dBCB479CC068D9046;
	mov.f64 	%fd405, 0dBD43515F67644276;
	mov.f64 	%fd406, 0d3CB09CCC22945996;
	fma.rn.f64 	%fd407, %fd406, %fd404, %fd405;
	mov.f64 	%fd408, 0dBD72C5B978E9F5C7;
	fma.rn.f64 	%fd409, %fd407, %fd404, %fd408;
	mov.f64 	%fd410, 0d3DBEC1151613913C;
	fma.rn.f64 	%fd411, %fd409, %fd404, %fd410;
	mov.f64 	%fd412, 0d3DE9E38D13C4A824;
	fma.rn.f64 	%fd413, %fd411, %fd404, %fd412;
	mov.f64 	%fd414, 0dBE341E75E1088EB5;
	fma.rn.f64 	%fd415, %fd413, %fd404, %fd414;
	mov.f64 	%fd416, 0dBE5A384EBB13CFE1;
	fma.rn.f64 	%fd417, %fd415, %fd404, %fd416;
	mov.f64 	%fd418, 0d3EA2BECB27F8C8F8;
	fma.rn.f64 	%fd419, %fd417, %fd404, %fd418;
	mov.f64 	%fd420, 0d3EC176E72B989FD8;
	fma.rn.f64 	%fd421, %fd419, %fd404, %fd420;
	mov.f64 	%fd422, 0dBF06F7BAB102F822;
	fma.rn.f64 	%fd423, %fd421, %fd404, %fd422;
	mov.f64 	%fd424, 0dBF1B50D7E1D278E1;
	fma.rn.f64 	%fd425, %fd423, %fd404, %fd424;
	mov.f64 	%fd426, 0d3F607A678D60004F;
	fma.rn.f64 	%fd427, %fd425, %fd404, %fd426;
	mov.f64 	%fd428, 0d3F63CED2A2E69115;
	fma.rn.f64 	%fd429, %fd427, %fd404, %fd428;
	mov.f64 	%fd430, 0dBFA6395DFE49FCD4;
	fma.rn.f64 	%fd431, %fd429, %fd404, %fd430;
	mov.f64 	%fd432, 0dBF902B3933CF21B1;
	fma.rn.f64 	%fd433, %fd431, %fd404, %fd432;
	mov.f64 	%fd434, 0d3FD15F993FCEAB5C;
	fma.rn.f64 	%fd435, %fd433, %fd404, %fd434;
	mul.f64 	%fd539, %fd404, %fd435;
	bra.uni 	BB71_51;

BB71_18:
	add.f64 	%fd189, %fd528, 0dC01C0FF5F3B47250;
	add.f64 	%fd190, %fd189, 0d3C9B226D9D243827;
	mov.f64 	%fd191, 0dBD40E8363DB649A9;
	mov.f64 	%fd192, 0d3CF3EB867515FAD6;
	fma.rn.f64 	%fd193, %fd192, %fd190, %fd191;
	mov.f64 	%fd194, 0dBD73B7DD4A6608FB;
	fma.rn.f64 	%fd195, %fd193, %fd190, %fd194;
	mov.f64 	%fd196, 0d3DBEC5E01482C750;
	fma.rn.f64 	%fd197, %fd195, %fd190, %fd196;
	mov.f64 	%fd198, 0d3DEC62BB9E882103;
	fma.rn.f64 	%fd199, %fd197, %fd190, %fd198;
	mov.f64 	%fd200, 0dBE34462EED732A23;
	fma.rn.f64 	%fd201, %fd199, %fd190, %fd200;
	mov.f64 	%fd202, 0dBE5D48DCAD7DC59B;
	fma.rn.f64 	%fd203, %fd201, %fd190, %fd202;
	mov.f64 	%fd204, 0d3EA3026DF29167E9;
	fma.rn.f64 	%fd205, %fd203, %fd190, %fd204;
	mov.f64 	%fd206, 0d3EC4255B0119666C;
	fma.rn.f64 	%fd207, %fd205, %fd190, %fd206;
	mov.f64 	%fd208, 0dBF0796A751B32693;
	fma.rn.f64 	%fd209, %fd207, %fd190, %fd208;
	mov.f64 	%fd210, 0dBF207358BBDBA284;
	fma.rn.f64 	%fd211, %fd209, %fd190, %fd210;
	mov.f64 	%fd212, 0d3F613FBC7D6927B1;
	fma.rn.f64 	%fd213, %fd211, %fd190, %fd212;
	mov.f64 	%fd214, 0d3F69A4B292E3DD75;
	fma.rn.f64 	%fd215, %fd213, %fd190, %fd214;
	mov.f64 	%fd216, 0dBFA80C83BDEEE4FB;
	fma.rn.f64 	%fd217, %fd215, %fd190, %fd216;
	mov.f64 	%fd218, 0dBF95E70DC60362BF;
	fma.rn.f64 	%fd219, %fd217, %fd190, %fd218;
	mov.f64 	%fd220, 0d3FD33518B3874E8A;
	fma.rn.f64 	%fd221, %fd219, %fd190, %fd220;
	mul.f64 	%fd534, %fd190, %fd221;

BB71_31:
	abs.f64 	%fd522, %fd1;
	neg.f64 	%fd303, %fd534;
	setp.lt.f64	%p21, %fd522, 0d0000000000000000;
	selp.f64	%fd304, %fd303, %fd534, %p21;
	mul.f64 	%fd305, %fd522, 0d3FE0000000000000;
	setp.lt.f64	%p22, %fd528, 0d39B4484BFEEBC2A0;
	selp.f64	%fd306, %fd305, %fd304, %p22;
	mov.f64 	%fd307, 0dBFF0000000000000;
	div.rn.f64 	%fd308, %fd307, %fd522;
	fma.rn.f64 	%fd309, %fd529, %fd306, %fd308;
	mul.f64 	%fd310, %fd309, 0d3FE45F306DC9C883;
	fma.rn.f64 	%fd539, %fd522, %fd3, %fd310;

BB71_51:
	setp.gtu.f64	%p32, %fd1, 0d0000000000000000;
	@%p32 bra 	BB71_53;

	setp.eq.f64	%p33, %fd1, 0d0000000000000000;
	selp.f64	%fd539, 0dFFF0000000000000, 0dFFF8000000000000, %p33;

BB71_53:
	cvta.to.global.u32 	%r90, %r25;
	add.s32 	%r92, %r90, %r37;
	st.global.f64 	[%r92], %fd539;

BB71_54:
	ret;
}

	// .globl	vec_copysign
.visible .entry vec_copysign(
	.param .u32 vec_copysign_param_0,
	.param .u32 vec_copysign_param_1,
	.param .u32 vec_copysign_param_2,
	.param .u32 vec_copysign_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<22>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r5, [vec_copysign_param_0];
	ld.param.u32 	%r2, [vec_copysign_param_1];
	ld.param.u32 	%r3, [vec_copysign_param_2];
	ld.param.u32 	%r4, [vec_copysign_param_3];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB72_2;

	cvta.to.global.u32 	%r9, %r3;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	cvta.to.global.u32 	%r12, %r4;
	add.s32 	%r13, %r12, %r10;
	ld.global.f64 	%fd1, [%r13];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd1;
	}
	ld.global.f64 	%fd2, [%r11];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r15, %temp}, %fd2;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd2;
	}
	and.b32  	%r17, %r14, -2147483648;
	and.b32  	%r18, %r16, 2147483647;
	or.b32  	%r19, %r18, %r17;
	mov.b64 	%fd3, {%r15, %r19};
	cvta.to.global.u32 	%r20, %r2;
	add.s32 	%r21, %r20, %r10;
	st.global.f64 	[%r21], %fd3;

BB72_2:
	ret;
}

	// .globl	vec_fdim
.visible .entry vec_fdim(
	.param .u32 vec_fdim_param_0,
	.param .u32 vec_fdim_param_1,
	.param .u32 vec_fdim_param_2,
	.param .u32 vec_fdim_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<16>;
	.reg .f64 	%fd<5>;


	ld.param.u32 	%r5, [vec_fdim_param_0];
	ld.param.u32 	%r2, [vec_fdim_param_1];
	ld.param.u32 	%r3, [vec_fdim_param_2];
	ld.param.u32 	%r4, [vec_fdim_param_3];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB73_2;

	cvta.to.global.u32 	%r9, %r3;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	cvta.to.global.u32 	%r12, %r4;
	add.s32 	%r13, %r12, %r10;
	ld.global.f64 	%fd1, [%r13];
	ld.global.f64 	%fd2, [%r11];
	sub.f64 	%fd3, %fd2, %fd1;
	setp.gtu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd4, %fd3, 0d0000000000000000, %p2;
	cvta.to.global.u32 	%r14, %r2;
	add.s32 	%r15, %r14, %r10;
	st.global.f64 	[%r15], %fd4;

BB73_2:
	ret;
}

	// .globl	vec_fdivide
.visible .entry vec_fdivide(
	.param .u32 vec_fdivide_param_0,
	.param .u32 vec_fdivide_param_1,
	.param .u32 vec_fdivide_param_2,
	.param .u32 vec_fdivide_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<16>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r5, [vec_fdivide_param_0];
	ld.param.u32 	%r2, [vec_fdivide_param_1];
	ld.param.u32 	%r3, [vec_fdivide_param_2];
	ld.param.u32 	%r4, [vec_fdivide_param_3];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB74_2;

	cvta.to.global.u32 	%r9, %r3;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	cvta.to.global.u32 	%r12, %r4;
	add.s32 	%r13, %r12, %r10;
	ld.global.f64 	%fd1, [%r13];
	ld.global.f64 	%fd2, [%r11];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u32 	%r14, %r2;
	add.s32 	%r15, %r14, %r10;
	st.global.f64 	[%r15], %fd3;

BB74_2:
	ret;
}

	// .globl	vec_fmax
.visible .entry vec_fmax(
	.param .u32 vec_fmax_param_0,
	.param .u32 vec_fmax_param_1,
	.param .u32 vec_fmax_param_2,
	.param .u32 vec_fmax_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<16>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r5, [vec_fmax_param_0];
	ld.param.u32 	%r2, [vec_fmax_param_1];
	ld.param.u32 	%r3, [vec_fmax_param_2];
	ld.param.u32 	%r4, [vec_fmax_param_3];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB75_2;

	cvta.to.global.u32 	%r9, %r3;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	cvta.to.global.u32 	%r12, %r4;
	add.s32 	%r13, %r12, %r10;
	ld.global.f64 	%fd1, [%r13];
	ld.global.f64 	%fd2, [%r11];
	max.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u32 	%r14, %r2;
	add.s32 	%r15, %r14, %r10;
	st.global.f64 	[%r15], %fd3;

BB75_2:
	ret;
}

	// .globl	vec_fmin
.visible .entry vec_fmin(
	.param .u32 vec_fmin_param_0,
	.param .u32 vec_fmin_param_1,
	.param .u32 vec_fmin_param_2,
	.param .u32 vec_fmin_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<16>;
	.reg .f64 	%fd<4>;


	ld.param.u32 	%r5, [vec_fmin_param_0];
	ld.param.u32 	%r2, [vec_fmin_param_1];
	ld.param.u32 	%r3, [vec_fmin_param_2];
	ld.param.u32 	%r4, [vec_fmin_param_3];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB76_2;

	cvta.to.global.u32 	%r9, %r3;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	cvta.to.global.u32 	%r12, %r4;
	add.s32 	%r13, %r12, %r10;
	ld.global.f64 	%fd1, [%r13];
	ld.global.f64 	%fd2, [%r11];
	min.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u32 	%r14, %r2;
	add.s32 	%r15, %r14, %r10;
	st.global.f64 	[%r15], %fd3;

BB76_2:
	ret;
}

	// .globl	vec_fmod
.visible .entry vec_fmod(
	.param .u32 vec_fmod_param_0,
	.param .u32 vec_fmod_param_1,
	.param .u32 vec_fmod_param_2,
	.param .u32 vec_fmod_param_3
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<39>;
	.reg .f64 	%fd<36>;


	ld.param.u32 	%r12, [vec_fmod_param_0];
	ld.param.u32 	%r9, [vec_fmod_param_1];
	ld.param.u32 	%r10, [vec_fmod_param_2];
	ld.param.u32 	%r11, [vec_fmod_param_3];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r1, %r14, %r15, %r13;
	setp.ge.u32	%p1, %r1, %r12;
	@%p1 bra 	BB77_18;

	cvta.to.global.u32 	%r16, %r10;
	shl.b32 	%r17, %r1, 3;
	add.s32 	%r18, %r16, %r17;
	cvta.to.global.u32 	%r19, %r11;
	add.s32 	%r20, %r19, %r17;
	ld.global.f64 	%fd1, [%r18];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	and.b32  	%r21, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd1;
	}
	ld.global.f64 	%fd2, [%r20];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd2;
	}
	and.b32  	%r37, %r23, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r38, %temp}, %fd2;
	}
	mov.b64 	%fd34, {%r22, %r21};
	mov.b64 	%fd4, {%r38, %r37};
	setp.gt.u32	%p2, %r21, 2146435071;
	setp.gt.u32	%p3, %r37, 2146435071;
	or.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB77_14;
	bra.uni 	BB77_2;

BB77_14:
	setp.gtu.f64	%p17, %fd34, 0d7FF0000000000000;
	setp.gtu.f64	%p18, %fd4, 0d7FF0000000000000;
	or.pred  	%p19, %p17, %p18;
	@%p19 bra 	BB77_16;
	bra.uni 	BB77_15;

BB77_16:
	add.f64 	%fd35, %fd1, %fd2;
	bra.uni 	BB77_17;

BB77_2:
	setp.eq.f64	%p5, %fd4, 0d0000000000000000;
	mov.f64 	%fd21, 0dFFF8000000000000;
	mov.f64 	%fd35, %fd21;
	@%p5 bra 	BB77_17;

	setp.ltu.f64	%p6, %fd34, %fd4;
	mov.f64 	%fd35, %fd1;
	@%p6 bra 	BB77_17;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r24}, %fd4;
	}
	mov.f64 	%fd30, 0d0000000000000000;
	setp.gt.s32	%p7, %r24, 1048575;
	@%p7 bra 	BB77_9;

	setp.geu.f64	%p8, %fd4, %fd34;
	mov.f64 	%fd31, %fd4;
	@%p8 bra 	BB77_8;

	mov.f64 	%fd32, %fd4;

BB77_7:
	add.f64 	%fd32, %fd32, %fd32;
	setp.lt.f64	%p9, %fd32, %fd34;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd32;
	}
	setp.lt.s32	%p10, %r25, 1048576;
	and.pred  	%p11, %p9, %p10;
	mov.f64 	%fd26, %fd32;
	mov.f64 	%fd31, %fd26;
	@%p11 bra 	BB77_7;

BB77_8:
	mov.f64 	%fd27, %fd31;
	mov.f64 	%fd30, %fd27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r37}, %fd30;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r38, %temp}, %fd30;
	}

BB77_9:
	mov.f64 	%fd29, %fd30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r26}, %fd34;
	}
	setp.lt.s32	%p12, %r26, 1048576;
	@%p12 bra 	BB77_11;

	and.b32  	%r27, %r37, 1048575;
	and.b32  	%r28, %r2, 2146435072;
	or.b32  	%r29, %r27, %r28;
	mov.b64 	%fd29, {%r38, %r29};

BB77_11:
	mul.f64 	%fd23, %fd29, 0d3FE0000000000000;
	setp.gt.f64	%p13, %fd29, %fd34;
	selp.f64	%fd33, %fd23, %fd29, %p13;
	setp.ltu.f64	%p14, %fd33, %fd4;
	@%p14 bra 	BB77_13;

BB77_12:
	sub.f64 	%fd24, %fd34, %fd33;
	setp.ltu.f64	%p15, %fd34, %fd33;
	selp.f64	%fd34, %fd34, %fd24, %p15;
	mul.f64 	%fd33, %fd33, 0d3FE0000000000000;
	setp.ge.f64	%p16, %fd33, %fd4;
	@%p16 bra 	BB77_12;

BB77_13:
	and.b32  	%r30, %r2, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r31}, %fd34;
	}
	or.b32  	%r32, %r31, %r30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r33, %temp}, %fd34;
	}
	mov.b64 	%fd35, {%r33, %r32};
	bra.uni 	BB77_17;

BB77_15:
	setp.eq.f64	%p20, %fd34, 0d7FF0000000000000;
	selp.f64	%fd35, 0dFFF8000000000000, %fd1, %p20;

BB77_17:
	cvta.to.global.u32 	%r34, %r9;
	add.s32 	%r36, %r34, %r17;
	st.global.f64 	[%r36], %fd35;

BB77_18:
	ret;
}

	// .globl	vec_hypot
.visible .entry vec_hypot(
	.param .u32 vec_hypot_param_0,
	.param .u32 vec_hypot_param_1,
	.param .u32 vec_hypot_param_2,
	.param .u32 vec_hypot_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<24>;
	.reg .f64 	%fd<30>;
	.reg .b64 	%rd<5>;


	ld.param.u32 	%r5, [vec_hypot_param_0];
	ld.param.u32 	%r2, [vec_hypot_param_1];
	ld.param.u32 	%r3, [vec_hypot_param_2];
	ld.param.u32 	%r4, [vec_hypot_param_3];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB78_2;

	cvta.to.global.u32 	%r9, %r3;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	cvta.to.global.u32 	%r12, %r4;
	add.s32 	%r13, %r12, %r10;
	ld.global.f64 	%fd3, [%r11];
	abs.f64 	%fd4, %fd3;
	ld.global.f64 	%fd5, [%r13];
	abs.f64 	%fd6, %fd5;
	mov.b64 	 %rd1, %fd6;
	mov.b64 	 %rd2, %fd4;
	min.u64 	%rd3, %rd1, %rd2;
	mov.b64 	 %fd7, %rd3;
	max.u64 	%rd4, %rd2, %rd1;
	mov.b64 	 %fd8, %rd4;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd8;
	}
	and.b32  	%r15, %r14, -4194304;
	mov.u32 	%r16, 2144337920;
	sub.s32 	%r17, %r16, %r15;
	mov.u32 	%r18, 0;
	mov.b64 	%fd9, {%r18, %r17};
	mul.f64 	%fd10, %fd7, %fd9;
	mul.f64 	%fd11, %fd8, %fd9;
	mul.f64 	%fd12, %fd10, %fd10;
	fma.rn.f64 	%fd13, %fd11, %fd11, %fd12;
	mov.f64 	%fd14, 0d7FEFFFFFFFFFFFFF;
	min.f64 	%fd2, %fd13, %fd14;
	// inline asm
	rsqrt.approx.ftz.f64 %fd1, %fd2;
	// inline asm
	mul.rn.f64 	%fd15, %fd1, %fd1;
	neg.f64 	%fd16, %fd15;
	mov.f64 	%fd17, 0d3FF0000000000000;
	fma.rn.f64 	%fd18, %fd2, %fd16, %fd17;
	mov.f64 	%fd19, 0d3FE0000000000000;
	mov.f64 	%fd20, 0d3FD8000000000000;
	fma.rn.f64 	%fd21, %fd20, %fd18, %fd19;
	mul.rn.f64 	%fd22, %fd18, %fd1;
	fma.rn.f64 	%fd23, %fd21, %fd22, %fd1;
	mul.f64 	%fd24, %fd13, %fd23;
	add.s32 	%r19, %r15, 1048576;
	mov.b64 	%fd25, {%r18, %r19};
	mul.f64 	%fd26, %fd24, %fd25;
	setp.eq.f64	%p2, %fd7, 0d0000000000000000;
	selp.f64	%fd27, %fd8, %fd26, %p2;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd7;
	}
	mov.f64 	%fd28, 0d7FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd28;
	}
	setp.lt.u32	%p3, %r20, %r21;
	selp.f64	%fd29, %fd27, %fd7, %p3;
	cvta.to.global.u32 	%r22, %r2;
	add.s32 	%r23, %r22, %r10;
	st.global.f64 	[%r23], %fd29;

BB78_2:
	ret;
}

	// .globl	vec_nextafter
.visible .entry vec_nextafter(
	.param .u32 vec_nextafter_param_0,
	.param .u32 vec_nextafter_param_1,
	.param .u32 vec_nextafter_param_2,
	.param .u32 vec_nextafter_param_3
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<22>;
	.reg .f64 	%fd<11>;
	.reg .b64 	%rd<14>;


	ld.param.u32 	%r5, [vec_nextafter_param_0];
	ld.param.u32 	%r2, [vec_nextafter_param_1];
	ld.param.u32 	%r3, [vec_nextafter_param_2];
	ld.param.u32 	%r4, [vec_nextafter_param_3];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB79_9;

	cvta.to.global.u32 	%r9, %r3;
	shl.b32 	%r10, %r1, 3;
	add.s32 	%r11, %r9, %r10;
	cvta.to.global.u32 	%r12, %r4;
	add.s32 	%r13, %r12, %r10;
	ld.global.f64 	%fd1, [%r11];
	mov.b64 	 %rd1, %fd1;
	ld.global.f64 	%fd10, [%r13];
	abs.f64 	%fd7, %fd1;
	setp.gtu.f64	%p2, %fd7, 0d7FF0000000000000;
	@%p2 bra 	BB79_7;

	abs.f64 	%fd8, %fd10;
	setp.gtu.f64	%p3, %fd8, 0d7FF0000000000000;
	@%p3 bra 	BB79_7;
	bra.uni 	BB79_3;

BB79_7:
	add.f64 	%fd10, %fd1, %fd10;

BB79_8:
	cvta.to.global.u32 	%r19, %r2;
	add.s32 	%r21, %r19, %r10;
	st.global.f64 	[%r21], %fd10;

BB79_9:
	ret;

BB79_3:
	mov.b64 	 %rd2, %fd10;
	or.b64  	%rd3, %rd2, %rd1;
	and.b64  	%rd4, %rd3, 9223372036854775807;
	setp.eq.s64	%p4, %rd4, 0;
	@%p4 bra 	BB79_8;

	shl.b64 	%rd5, %rd1, 1;
	setp.eq.s64	%p5, %rd5, 0;
	@%p5 bra 	BB79_6;
	bra.uni 	BB79_5;

BB79_6:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd10;
	}
	and.b32  	%r15, %r14, -2147483648;
	mov.f64 	%fd9, 0d0000000000000001;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd9;
	}
	or.b32  	%r17, %r16, %r15;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r18, %temp}, %fd9;
	}
	mov.b64 	%fd10, {%r18, %r17};
	bra.uni 	BB79_8;

BB79_5:
	setp.lt.f64	%p6, %fd1, %fd10;
	setp.lt.f64	%p7, %fd1, 0d0000000000000000;
	and.pred  	%p8, %p6, %p7;
	selp.s64	%rd6, -1, 0, %p8;
	add.s64 	%rd7, %rd6, %rd1;
	setp.gt.f64	%p9, %fd1, 0d0000000000000000;
	and.pred  	%p10, %p6, %p9;
	selp.u64	%rd8, 1, 0, %p10;
	add.s64 	%rd9, %rd7, %rd8;
	setp.gt.f64	%p11, %fd1, %fd10;
	and.pred  	%p12, %p11, %p7;
	selp.u64	%rd10, 1, 0, %p12;
	add.s64 	%rd11, %rd9, %rd10;
	and.pred  	%p13, %p11, %p9;
	selp.s64	%rd12, -1, 0, %p13;
	add.s64 	%rd13, %rd11, %rd12;
	mov.b64 	 %fd10, %rd13;
	bra.uni 	BB79_8;
}

	// .globl	vec_pow
.visible .entry vec_pow(
	.param .u32 vec_pow_param_0,
	.param .u32 vec_pow_param_1,
	.param .u32 vec_pow_param_2,
	.param .u32 vec_pow_param_3
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<24>;
	.reg .b64 	%rd<3>;


	ld.param.u32 	%r7, [vec_pow_param_0];
	ld.param.u32 	%r4, [vec_pow_param_1];
	ld.param.u32 	%r5, [vec_pow_param_2];
	ld.param.u32 	%r6, [vec_pow_param_3];
	mov.u32 	%r8, %tid.x;
	mov.u32 	%r9, %ntid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r1, %r9, %r10, %r8;
	setp.ge.u32	%p2, %r1, %r7;
	@%p2 bra 	BB80_15;

	cvta.to.global.u32 	%r11, %r5;
	shl.b32 	%r12, %r1, 3;
	add.s32 	%r13, %r11, %r12;
	cvta.to.global.u32 	%r14, %r6;
	add.s32 	%r15, %r14, %r12;
	ld.global.f64 	%fd1, [%r13];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	ld.global.f64 	%fd2, [%r15];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd2;
	}
	bfe.u32 	%r16, %r3, 20, 11;
	add.s32 	%r17, %r16, -1012;
	mov.b64 	 %rd2, %fd2;
	shl.b64 	%rd1, %rd2, %r17;
	setp.eq.s64	%p3, %rd1, -9223372036854775808;
	abs.f64 	%fd3, %fd1;
	// Callseq Start 17
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd2;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd23, [retval0+0];
	
	//{
	}// Callseq End 17
	setp.lt.s32	%p4, %r2, 0;
	and.pred  	%p1, %p4, %p3;
	@!%p1 bra 	BB80_3;
	bra.uni 	BB80_2;

BB80_2:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd23;
	}
	xor.b32  	%r19, %r18, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r20, %temp}, %fd23;
	}
	mov.b64 	%fd23, {%r20, %r19};

BB80_3:
	mov.f64 	%fd22, %fd23;
	setp.eq.f64	%p5, %fd1, 0d0000000000000000;
	@%p5 bra 	BB80_6;
	bra.uni 	BB80_4;

BB80_6:
	selp.b32	%r21, %r2, 0, %p3;
	or.b32  	%r22, %r21, 2146435072;
	setp.lt.s32	%p9, %r3, 0;
	selp.b32	%r23, %r22, %r21, %p9;
	mov.u32 	%r24, 0;
	mov.b64 	%fd22, {%r24, %r23};
	bra.uni 	BB80_7;

BB80_4:
	setp.gt.s32	%p6, %r2, -1;
	@%p6 bra 	BB80_7;

	cvt.rzi.f64.f64	%fd15, %fd2;
	setp.neu.f64	%p7, %fd15, %fd2;
	selp.f64	%fd22, 0dFFF8000000000000, %fd22, %p7;

BB80_7:
	mov.f64 	%fd9, %fd22;
	add.f64 	%fd10, %fd1, %fd2;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd10;
	}
	and.b32  	%r26, %r25, 2146435072;
	setp.ne.s32	%p10, %r26, 2146435072;
	mov.f64 	%fd21, %fd9;
	@%p10 bra 	BB80_14;

	setp.gtu.f64	%p11, %fd3, 0d7FF0000000000000;
	mov.f64 	%fd21, %fd10;
	@%p11 bra 	BB80_14;

	abs.f64 	%fd11, %fd2;
	setp.gtu.f64	%p12, %fd11, 0d7FF0000000000000;
	mov.f64 	%fd20, %fd10;
	mov.f64 	%fd21, %fd20;
	@%p12 bra 	BB80_14;

	setp.eq.f64	%p13, %fd11, 0d7FF0000000000000;
	@%p13 bra 	BB80_13;
	bra.uni 	BB80_11;

BB80_13:
	setp.gt.f64	%p15, %fd3, 0d3FF0000000000000;
	selp.b32	%r33, 2146435072, 0, %p15;
	xor.b32  	%r34, %r33, 2146435072;
	setp.lt.s32	%p16, %r3, 0;
	selp.b32	%r35, %r34, %r33, %p16;
	setp.eq.f64	%p17, %fd1, 0dBFF0000000000000;
	selp.b32	%r36, 1072693248, %r35, %p17;
	mov.u32 	%r37, 0;
	mov.b64 	%fd21, {%r37, %r36};
	bra.uni 	BB80_14;

BB80_11:
	setp.neu.f64	%p14, %fd3, 0d7FF0000000000000;
	mov.f64 	%fd21, %fd9;
	@%p14 bra 	BB80_14;

	shr.s32 	%r27, %r3, 31;
	and.b32  	%r28, %r27, -2146435072;
	add.s32 	%r29, %r28, 2146435072;
	or.b32  	%r30, %r29, -2147483648;
	selp.b32	%r31, %r30, %r29, %p1;
	mov.u32 	%r32, 0;
	mov.b64 	%fd21, {%r32, %r31};

BB80_14:
	setp.eq.f64	%p18, %fd2, 0d0000000000000000;
	setp.eq.f64	%p19, %fd1, 0d3FF0000000000000;
	or.pred  	%p20, %p19, %p18;
	selp.f64	%fd16, 0d3FF0000000000000, %fd21, %p20;
	cvta.to.global.u32 	%r38, %r4;
	add.s32 	%r40, %r38, %r12;
	st.global.f64 	[%r40], %fd16;

BB80_15:
	ret;
}

	// .globl	vec_remainder
.visible .entry vec_remainder(
	.param .u32 vec_remainder_param_0,
	.param .u32 vec_remainder_param_1,
	.param .u32 vec_remainder_param_2,
	.param .u32 vec_remainder_param_3
)
{
	.reg .pred 	%p<28>;
	.reg .b32 	%r<51>;
	.reg .f64 	%fd<39>;


	ld.param.u32 	%r16, [vec_remainder_param_0];
	ld.param.u32 	%r13, [vec_remainder_param_1];
	ld.param.u32 	%r14, [vec_remainder_param_2];
	ld.param.u32 	%r15, [vec_remainder_param_3];
	mov.u32 	%r17, %tid.x;
	mov.u32 	%r18, %ntid.x;
	mov.u32 	%r19, %ctaid.x;
	mad.lo.s32 	%r1, %r18, %r19, %r17;
	setp.ge.u32	%p3, %r1, %r16;
	@%p3 bra 	BB81_21;

	cvta.to.global.u32 	%r20, %r14;
	shl.b32 	%r21, %r1, 3;
	add.s32 	%r22, %r20, %r21;
	cvta.to.global.u32 	%r23, %r15;
	add.s32 	%r24, %r23, %r21;
	ld.global.f64 	%fd1, [%r22];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	and.b32  	%r25, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r26, %temp}, %fd1;
	}
	ld.global.f64 	%fd2, [%r24];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r27}, %fd2;
	}
	and.b32  	%r47, %r27, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r48, %temp}, %fd2;
	}
	mov.b64 	%fd37, {%r26, %r25};
	mov.b64 	%fd4, {%r48, %r47};
	setp.gt.u32	%p4, %r25, 2146435071;
	setp.gt.u32	%p5, %r47, 2146435071;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	BB81_17;
	bra.uni 	BB81_2;

BB81_17:
	setp.gtu.f64	%p23, %fd37, 0d7FF0000000000000;
	setp.gtu.f64	%p24, %fd4, 0d7FF0000000000000;
	or.pred  	%p25, %p23, %p24;
	@%p25 bra 	BB81_19;
	bra.uni 	BB81_18;

BB81_19:
	add.f64 	%fd38, %fd1, %fd2;
	bra.uni 	BB81_20;

BB81_2:
	setp.eq.f64	%p7, %fd4, 0d0000000000000000;
	mov.f64 	%fd38, 0dFFF8000000000000;
	@%p7 bra 	BB81_20;

	setp.ltu.f64	%p8, %fd37, %fd4;
	mov.u32 	%r50, 0;
	@%p8 bra 	BB81_14;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r29}, %fd4;
	}
	mov.f64 	%fd33, 0d0000000000000000;
	setp.gt.s32	%p9, %r29, 1048575;
	@%p9 bra 	BB81_9;

	setp.geu.f64	%p10, %fd4, %fd37;
	mov.f64 	%fd34, %fd4;
	@%p10 bra 	BB81_8;

	mov.f64 	%fd35, %fd4;

BB81_7:
	add.f64 	%fd35, %fd35, %fd35;
	setp.lt.f64	%p11, %fd35, %fd37;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd35;
	}
	setp.lt.s32	%p12, %r30, 1048576;
	and.pred  	%p13, %p11, %p12;
	mov.f64 	%fd29, %fd35;
	mov.f64 	%fd34, %fd29;
	@%p13 bra 	BB81_7;

BB81_8:
	mov.f64 	%fd30, %fd34;
	mov.f64 	%fd33, %fd30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r47}, %fd33;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r48, %temp}, %fd33;
	}

BB81_9:
	mov.f64 	%fd32, %fd33;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r31}, %fd37;
	}
	setp.lt.s32	%p14, %r31, 1048576;
	@%p14 bra 	BB81_11;

	and.b32  	%r32, %r47, 1048575;
	and.b32  	%r33, %r2, 2146435072;
	or.b32  	%r34, %r32, %r33;
	mov.b64 	%fd32, {%r48, %r34};

BB81_11:
	mul.f64 	%fd24, %fd32, 0d3FE0000000000000;
	setp.gt.f64	%p15, %fd32, %fd37;
	selp.f64	%fd36, %fd24, %fd32, %p15;
	setp.ltu.f64	%p16, %fd36, %fd4;
	mov.u32 	%r49, -1;
	@%p16 bra 	BB81_14;

BB81_12:
	setp.ltu.f64	%p17, %fd37, %fd36;
	selp.u32	%r37, 1, 0, %p17;
	shl.b32 	%r38, %r49, 1;
	add.s32 	%r49, %r37, %r38;
	sub.f64 	%fd25, %fd37, %fd36;
	selp.f64	%fd37, %fd37, %fd25, %p17;
	mul.f64 	%fd36, %fd36, 0d3FE0000000000000;
	setp.ge.f64	%p18, %fd36, %fd4;
	@%p18 bra 	BB81_12;

	not.b32 	%r39, %r49;
	and.b32  	%r50, %r39, 1;

BB81_14:
	add.f64 	%fd17, %fd37, %fd37;
	setp.gt.f64	%p20, %fd17, %fd4;
	mov.pred 	%p27, -1;
	@%p20 bra 	BB81_16;

	setp.eq.f64	%p21, %fd17, %fd4;
	setp.ne.s32	%p22, %r50, 0;
	and.pred  	%p27, %p21, %p22;

BB81_16:
	sub.f64 	%fd26, %fd37, %fd4;
	selp.f64	%fd27, %fd26, %fd37, %p27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r40}, %fd27;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r41, %temp}, %fd27;
	}
	and.b32  	%r42, %r2, -2147483648;
	xor.b32  	%r43, %r40, %r42;
	mov.b64 	%fd38, {%r41, %r43};
	bra.uni 	BB81_20;

BB81_18:
	setp.eq.f64	%p26, %fd37, 0d7FF0000000000000;
	selp.f64	%fd38, 0dFFF8000000000000, %fd1, %p26;

BB81_20:
	cvta.to.global.u32 	%r44, %r13;
	add.s32 	%r46, %r44, %r21;
	st.global.f64 	[%r46], %fd38;

BB81_21:
	ret;
}

.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b32 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 8 .b8 	__local_depot82[40];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<9>;
	.reg .b32 	%r<61>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<76>;


	mov.u32 	%r60, __local_depot82;
	cvta.local.u32 	%SP, %r60;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	ld.param.u32 	%r26, [__internal_trig_reduction_slowpathd_param_1];
	add.u32 	%r27, %SP, 0;
	cvta.to.local.u32 	%r1, %r27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd4;
	}
	and.b32  	%r58, %r2, -2147483648;
	shr.u32 	%r4, %r2, 20;
	bfe.u32 	%r5, %r2, 20, 11;
	setp.eq.s32	%p1, %r5, 2047;
	@%p1 bra 	BB82_13;

	add.s32 	%r28, %r5, -1024;
	shr.u32 	%r29, %r28, 6;
	mov.u32 	%r30, 16;
	sub.s32 	%r6, %r30, %r29;
	mov.u32 	%r31, 19;
	sub.s32 	%r32, %r31, %r29;
	mov.u32 	%r33, 18;
	min.s32 	%r7, %r33, %r32;
	setp.gt.s32	%p2, %r6, %r7;
	mov.u64 	%rd70, 0;
	mov.u32 	%r57, %r1;
	@%p2 bra 	BB82_4;

	mov.b64 	 %rd26, %fd4;
	shl.b64 	%rd27, %rd26, 11;
	or.b64  	%rd1, %rd27, -9223372036854775808;
	add.s32 	%r56, %r6, -1;
	bfe.u32 	%r34, %r2, 20, 11;
	add.s32 	%r35, %r34, -1024;
	shr.u32 	%r36, %r35, 6;
	mov.u32 	%r37, 15;
	sub.s32 	%r38, %r37, %r36;
	shl.b32 	%r39, %r38, 3;
	mov.u32 	%r40, __cudart_i2opi_d;
	add.s32 	%r54, %r40, %r39;
	mov.u64 	%rd70, 0;
	mov.u32 	%r55, %r1;

BB82_3:
	.pragma "nounroll";
	mov.u32 	%r12, %r55;
	ld.const.u64 	%rd30, [%r54];
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
	mov.b64         {alo,ahi}, %rd30;    
	mov.b64         {blo,bhi}, %rd1;    
	mov.b64         {clo,chi}, %rd70;    
	mad.lo.cc.u32   r0, alo, blo, clo;
	madc.hi.cc.u32  r1, alo, blo, chi;
	madc.hi.u32     r2, alo, bhi,   0;
	mad.lo.cc.u32   r1, alo, bhi,  r1;
	madc.hi.cc.u32  r2, ahi, blo,  r2;
	madc.hi.u32     r3, ahi, bhi,   0;
	mad.lo.cc.u32   r1, ahi, blo,  r1;
	madc.lo.cc.u32  r2, ahi, bhi,  r2;
	addc.u32        r3,  r3,   0;     
	mov.b64         %rd28, {r0,r1};      
	mov.b64         %rd70, {r2,r3};      
	}
	// inline asm
	st.local.u64 	[%r12], %rd28;
	add.s32 	%r15, %r12, 8;
	mov.u32 	%r57, %r15;
	add.s32 	%r54, %r54, 8;
	add.s32 	%r56, %r56, 1;
	setp.lt.s32	%p3, %r56, %r7;
	mov.u32 	%r55, %r15;
	@%p3 bra 	BB82_3;

BB82_4:
	st.local.u64 	[%r57], %rd70;
	ld.local.u64 	%rd71, [%r1+16];
	ld.local.u64 	%rd72, [%r1+24];
	and.b32  	%r20, %r4, 63;
	setp.eq.s32	%p4, %r20, 0;
	@%p4 bra 	BB82_6;

	mov.u32 	%r41, 64;
	sub.s32 	%r42, %r41, %r20;
	shl.b64 	%rd33, %rd72, %r20;
	shr.u64 	%rd34, %rd71, %r42;
	or.b64  	%rd72, %rd33, %rd34;
	shl.b64 	%rd35, %rd71, %r20;
	ld.local.u64 	%rd36, [%r1+8];
	shr.u64 	%rd37, %rd36, %r42;
	or.b64  	%rd71, %rd37, %rd35;

BB82_6:
	cvta.to.local.u32 	%r43, %r26;
	shr.u64 	%rd38, %rd72, 62;
	cvt.u32.u64	%r44, %rd38;
	shr.u64 	%rd39, %rd71, 62;
	shl.b64 	%rd40, %rd72, 2;
	or.b64  	%rd74, %rd40, %rd39;
	shl.b64 	%rd73, %rd71, 2;
	shr.u64 	%rd41, %rd72, 61;
	cvt.u32.u64	%r45, %rd41;
	and.b32  	%r46, %r45, 1;
	add.s32 	%r47, %r46, %r44;
	neg.s32 	%r48, %r47;
	setp.eq.s32	%p5, %r58, 0;
	selp.b32	%r49, %r47, %r48, %p5;
	st.local.u32 	[%r43], %r49;
	setp.eq.s32	%p6, %r46, 0;
	@%p6 bra 	BB82_8;

	mov.u64 	%rd45, 0;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd45;
	mov.b64         {a2,a3}, %rd45;
	mov.b64         {b0,b1}, %rd73;
	mov.b64         {b2,b3}, %rd74;
	sub.cc.u32      r0, a0, b0; 
	subc.cc.u32     r1, a1, b1; 
	subc.cc.u32     r2, a2, b2; 
	subc.u32        r3, a3, b3; 
	mov.b64         %rd73, {r0,r1};
	mov.b64         %rd74, {r2,r3};
	}
	// inline asm
	xor.b32  	%r58, %r58, -2147483648;

BB82_8:
	clz.b64 	%r59, %rd74;
	setp.eq.s32	%p7, %r59, 0;
	@%p7 bra 	BB82_10;

	shl.b64 	%rd48, %rd74, %r59;
	mov.u32 	%r50, 64;
	sub.s32 	%r51, %r50, %r59;
	shr.u64 	%rd49, %rd73, %r51;
	or.b64  	%rd74, %rd49, %rd48;

BB82_10:
	mov.u64 	%rd53, -3958705157555305931;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
	mov.b64         {alo,ahi}, %rd74;   
	mov.b64         {blo,bhi}, %rd53;   
	mul.lo.u32      r0, alo, blo;    
	mul.hi.u32      r1, alo, blo;    
	mad.lo.cc.u32   r1, alo, bhi, r1;
	madc.hi.u32     r2, alo, bhi,  0;
	mad.lo.cc.u32   r1, ahi, blo, r1;
	madc.hi.cc.u32  r2, ahi, blo, r2;
	madc.hi.u32     r3, ahi, bhi,  0;
	mad.lo.cc.u32   r2, ahi, bhi, r2;
	addc.u32        r3, r3,  0;      
	mov.b64         %rd50, {r0,r1};     
	mov.b64         %rd75, {r2,r3};     
	}
	// inline asm
	setp.lt.s64	%p8, %rd75, 1;
	@%p8 bra 	BB82_12;

	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd50;
	mov.b64         {a2,a3}, %rd75;
	mov.b64         {b0,b1}, %rd50;
	mov.b64         {b2,b3}, %rd75;
	add.cc.u32      r0, a0, b0; 
	addc.cc.u32     r1, a1, b1; 
	addc.cc.u32     r2, a2, b2; 
	addc.u32        r3, a3, b3; 
	mov.b64         %rd54, {r0,r1};
	mov.b64         %rd75, {r2,r3};
	}
	// inline asm
	add.s32 	%r59, %r59, 1;

BB82_12:
	cvt.u64.u32	%rd60, %r58;
	shl.b64 	%rd61, %rd60, 32;
	mov.u32 	%r52, 1022;
	sub.s32 	%r53, %r52, %r59;
	cvt.u64.u32	%rd62, %r53;
	shl.b64 	%rd63, %rd62, 52;
	add.s64 	%rd64, %rd75, 1;
	shr.u64 	%rd65, %rd64, 10;
	add.s64 	%rd66, %rd65, 1;
	shr.u64 	%rd67, %rd66, 1;
	add.s64 	%rd68, %rd67, %rd63;
	or.b64  	%rd69, %rd68, %rd61;
	mov.b64 	 %fd4, %rd69;

BB82_13:
	st.param.f64	[func_retval0+0], %fd4;
	ret;
}

.func  (.param .b64 func_retval0) __internal_accurate_pow(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<49>;
	.reg .f64 	%fd<136>;


	ld.param.f64 	%fd12, [__internal_accurate_pow_param_0];
	ld.param.f64 	%fd13, [__internal_accurate_pow_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r46}, %fd12;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r45, %temp}, %fd12;
	}
	shr.u32 	%r47, %r46, 20;
	setp.ne.s32	%p1, %r47, 0;
	@%p1 bra 	BB83_2;

	mul.f64 	%fd14, %fd12, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r46}, %fd14;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r45, %temp}, %fd14;
	}
	shr.u32 	%r16, %r46, 20;
	add.s32 	%r47, %r16, -54;

BB83_2:
	add.s32 	%r48, %r47, -1023;
	and.b32  	%r17, %r46, -2146435073;
	or.b32  	%r18, %r17, 1072693248;
	mov.b64 	%fd134, {%r45, %r18};
	setp.lt.u32	%p2, %r18, 1073127583;
	@%p2 bra 	BB83_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd134;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd134;
	}
	add.s32 	%r21, %r20, -1048576;
	mov.b64 	%fd134, {%r19, %r21};
	add.s32 	%r48, %r47, -1022;

BB83_4:
	add.f64 	%fd16, %fd134, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd15,%fd16;
	// inline asm
	neg.f64 	%fd17, %fd16;
	mov.f64 	%fd18, 0d3FF0000000000000;
	fma.rn.f64 	%fd19, %fd17, %fd15, %fd18;
	fma.rn.f64 	%fd20, %fd19, %fd19, %fd19;
	fma.rn.f64 	%fd21, %fd20, %fd15, %fd15;
	add.f64 	%fd22, %fd134, 0dBFF0000000000000;
	mul.f64 	%fd23, %fd22, %fd21;
	fma.rn.f64 	%fd24, %fd22, %fd21, %fd23;
	mul.f64 	%fd25, %fd24, %fd24;
	mov.f64 	%fd26, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd27, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd28, %fd27, %fd25, %fd26;
	mov.f64 	%fd29, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd30, %fd28, %fd25, %fd29;
	mov.f64 	%fd31, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd32, %fd30, %fd25, %fd31;
	mov.f64 	%fd33, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd34, %fd32, %fd25, %fd33;
	mov.f64 	%fd35, 0d3F6249249242B910;
	fma.rn.f64 	%fd36, %fd34, %fd25, %fd35;
	mov.f64 	%fd37, 0d3F89999999999DFB;
	fma.rn.f64 	%fd38, %fd36, %fd25, %fd37;
	sub.f64 	%fd39, %fd22, %fd24;
	add.f64 	%fd40, %fd39, %fd39;
	neg.f64 	%fd41, %fd24;
	fma.rn.f64 	%fd42, %fd41, %fd22, %fd40;
	mul.f64 	%fd43, %fd21, %fd42;
	fma.rn.f64 	%fd44, %fd25, %fd38, 0d3FB5555555555555;
	mov.f64 	%fd45, 0d3FB5555555555555;
	sub.f64 	%fd46, %fd45, %fd44;
	fma.rn.f64 	%fd47, %fd25, %fd38, %fd46;
	add.f64 	%fd48, %fd47, 0d0000000000000000;
	add.f64 	%fd49, %fd48, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd50, %fd44, %fd49;
	sub.f64 	%fd51, %fd44, %fd50;
	add.f64 	%fd52, %fd49, %fd51;
	mul.rn.f64 	%fd53, %fd24, %fd24;
	neg.f64 	%fd54, %fd53;
	fma.rn.f64 	%fd55, %fd24, %fd24, %fd54;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd43;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd43;
	}
	add.s32 	%r24, %r23, 1048576;
	mov.b64 	%fd56, {%r22, %r24};
	fma.rn.f64 	%fd57, %fd24, %fd56, %fd55;
	mul.rn.f64 	%fd58, %fd53, %fd24;
	neg.f64 	%fd59, %fd58;
	fma.rn.f64 	%fd60, %fd53, %fd24, %fd59;
	fma.rn.f64 	%fd61, %fd53, %fd43, %fd60;
	fma.rn.f64 	%fd62, %fd57, %fd24, %fd61;
	mul.rn.f64 	%fd63, %fd50, %fd58;
	neg.f64 	%fd64, %fd63;
	fma.rn.f64 	%fd65, %fd50, %fd58, %fd64;
	fma.rn.f64 	%fd66, %fd50, %fd62, %fd65;
	fma.rn.f64 	%fd67, %fd52, %fd58, %fd66;
	add.f64 	%fd68, %fd63, %fd67;
	sub.f64 	%fd69, %fd63, %fd68;
	add.f64 	%fd70, %fd67, %fd69;
	add.f64 	%fd71, %fd24, %fd68;
	sub.f64 	%fd72, %fd24, %fd71;
	add.f64 	%fd73, %fd68, %fd72;
	add.f64 	%fd74, %fd70, %fd73;
	add.f64 	%fd75, %fd43, %fd74;
	add.f64 	%fd76, %fd71, %fd75;
	sub.f64 	%fd77, %fd71, %fd76;
	add.f64 	%fd78, %fd75, %fd77;
	xor.b32  	%r25, %r48, -2147483648;
	mov.u32 	%r26, 1127219200;
	mov.b64 	%fd79, {%r25, %r26};
	mov.u32 	%r27, -2147483648;
	mov.b64 	%fd80, {%r27, %r26};
	sub.f64 	%fd81, %fd79, %fd80;
	mov.f64 	%fd82, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd83, %fd81, %fd82, %fd76;
	neg.f64 	%fd84, %fd81;
	fma.rn.f64 	%fd85, %fd84, %fd82, %fd83;
	sub.f64 	%fd86, %fd85, %fd76;
	sub.f64 	%fd87, %fd78, %fd86;
	mov.f64 	%fd88, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd89, %fd81, %fd88, %fd87;
	add.f64 	%fd90, %fd83, %fd89;
	sub.f64 	%fd91, %fd83, %fd90;
	add.f64 	%fd92, %fd89, %fd91;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd13;
	}
	add.s32 	%r29, %r28, %r28;
	setp.gt.u32	%p3, %r29, -33554433;
	and.b32  	%r30, %r28, -15728641;
	selp.b32	%r31, %r30, %r28, %p3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd13;
	}
	mov.b64 	%fd93, {%r32, %r31};
	mul.rn.f64 	%fd94, %fd90, %fd93;
	neg.f64 	%fd95, %fd94;
	fma.rn.f64 	%fd96, %fd90, %fd93, %fd95;
	fma.rn.f64 	%fd97, %fd92, %fd93, %fd96;
	add.f64 	%fd4, %fd94, %fd97;
	sub.f64 	%fd98, %fd94, %fd4;
	add.f64 	%fd5, %fd97, %fd98;
	mov.f64 	%fd99, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd100, %fd4, %fd99;
	mov.f64 	%fd101, 0d4338000000000000;
	add.rn.f64 	%fd102, %fd100, %fd101;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd102;
	}
	mov.f64 	%fd103, 0dC338000000000000;
	add.rn.f64 	%fd104, %fd102, %fd103;
	mov.f64 	%fd105, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd106, %fd104, %fd105, %fd4;
	mov.f64 	%fd107, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd108, %fd104, %fd107, %fd106;
	mov.f64 	%fd109, 0d3E928AF3FCA213EA;
	mov.f64 	%fd110, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd111, %fd110, %fd108, %fd109;
	mov.f64 	%fd112, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd113, %fd111, %fd108, %fd112;
	mov.f64 	%fd114, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd115, %fd113, %fd108, %fd114;
	mov.f64 	%fd116, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd117, %fd115, %fd108, %fd116;
	mov.f64 	%fd118, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd119, %fd117, %fd108, %fd118;
	mov.f64 	%fd120, 0d3F81111111122322;
	fma.rn.f64 	%fd121, %fd119, %fd108, %fd120;
	mov.f64 	%fd122, 0d3FA55555555502A1;
	fma.rn.f64 	%fd123, %fd121, %fd108, %fd122;
	mov.f64 	%fd124, 0d3FC5555555555511;
	fma.rn.f64 	%fd125, %fd123, %fd108, %fd124;
	mov.f64 	%fd126, 0d3FE000000000000B;
	fma.rn.f64 	%fd127, %fd125, %fd108, %fd126;
	fma.rn.f64 	%fd128, %fd127, %fd108, %fd18;
	fma.rn.f64 	%fd129, %fd128, %fd108, %fd18;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd129;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd129;
	}
	shl.b32 	%r33, %r13, 20;
	add.s32 	%r34, %r15, %r33;
	mov.b64 	%fd135, {%r14, %r34};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd4;
	}
	mov.b32 	 %f2, %r35;
	abs.f32 	%f1, %f2;
	setp.lt.f32	%p4, %f1, 0f4086232B;
	@%p4 bra 	BB83_7;

	setp.lt.f64	%p5, %fd4, 0d0000000000000000;
	add.f64 	%fd130, %fd4, 0d7FF0000000000000;
	selp.f64	%fd135, 0d0000000000000000, %fd130, %p5;
	setp.geu.f32	%p6, %f1, 0f40874800;
	@%p6 bra 	BB83_7;

	shr.u32 	%r36, %r13, 31;
	add.s32 	%r37, %r13, %r36;
	shr.s32 	%r38, %r37, 1;
	shl.b32 	%r39, %r38, 20;
	add.s32 	%r40, %r39, %r15;
	mov.b64 	%fd131, {%r14, %r40};
	sub.s32 	%r41, %r13, %r38;
	shl.b32 	%r42, %r41, 20;
	add.s32 	%r43, %r42, 1072693248;
	mov.u32 	%r44, 0;
	mov.b64 	%fd132, {%r44, %r43};
	mul.f64 	%fd135, %fd131, %fd132;

BB83_7:
	abs.f64 	%fd133, %fd135;
	setp.eq.f64	%p7, %fd133, 0d7FF0000000000000;
	@%p7 bra 	BB83_9;

	fma.rn.f64 	%fd135, %fd135, %fd5, %fd135;

BB83_9:
	st.param.f64	[func_retval0+0], %fd135;
	ret;
}

.func  (.param .b64 func_retval0) __internal_lgamma_pos(
	.param .b64 __internal_lgamma_pos_param_0
)
{
	.reg .pred 	%p<22>;
	.reg .b32 	%r<54>;
	.reg .f64 	%fd<291>;


	ld.param.f64 	%fd24, [__internal_lgamma_pos_param_0];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd24;
	}
	setp.gt.s32	%p1, %r50, 1074266111;
	@%p1 bra 	BB84_16;
	bra.uni 	BB84_1;

BB84_16:
	setp.gt.s32	%p12, %r50, 1075838975;
	@%p12 bra 	BB84_18;
	bra.uni 	BB84_17;

BB84_18:
	// inline asm
	rcp.approx.ftz.f64 %fd215,%fd24;
	// inline asm
	neg.f64 	%fd13, %fd24;
	mov.f64 	%fd217, 0d3FF0000000000000;
	fma.rn.f64 	%fd218, %fd13, %fd215, %fd217;
	fma.rn.f64 	%fd219, %fd218, %fd218, %fd218;
	fma.rn.f64 	%fd220, %fd219, %fd215, %fd215;
	mul.f64 	%fd221, %fd220, %fd220;
	mov.f64 	%fd222, 0d3F4B68B992738FBF;
	mov.f64 	%fd223, 0dBF5AC321034783F9;
	fma.rn.f64 	%fd224, %fd223, %fd221, %fd222;
	mov.f64 	%fd225, 0dBF4380D01E4F7B8C;
	fma.rn.f64 	%fd226, %fd224, %fd221, %fd225;
	mov.f64 	%fd227, 0d3F4A019FA29F7264;
	fma.rn.f64 	%fd228, %fd226, %fd221, %fd227;
	mov.f64 	%fd229, 0dBF66C16C16B2ACEC;
	fma.rn.f64 	%fd230, %fd228, %fd221, %fd229;
	mov.f64 	%fd231, 0d3FB5555555555545;
	fma.rn.f64 	%fd232, %fd230, %fd221, %fd231;
	mov.f64 	%fd233, 0d3FED67F1C864BEAE;
	fma.rn.f64 	%fd14, %fd232, %fd220, %fd233;
	setp.lt.s32	%p13, %r50, 2146435072;
	setp.gt.f64	%p14, %fd24, 0d0000000000000000;
	and.pred  	%p15, %p14, %p13;
	@%p15 bra 	BB84_23;
	bra.uni 	BB84_19;

BB84_23:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r51, %temp}, %fd24;
	}
	mov.u32 	%r52, -1023;
	setp.gt.s32	%p19, %r50, 1048575;
	@%p19 bra 	BB84_25;

	mul.f64 	%fd236, %fd24, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd236;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r51, %temp}, %fd236;
	}
	mov.u32 	%r52, -1077;

BB84_25:
	shr.u32 	%r34, %r50, 20;
	add.s32 	%r53, %r52, %r34;
	and.b32  	%r35, %r50, -2146435073;
	or.b32  	%r36, %r35, 1072693248;
	mov.b64 	%fd288, {%r51, %r36};
	setp.lt.s32	%p20, %r36, 1073127583;
	@%p20 bra 	BB84_27;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r37, %temp}, %fd288;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r38}, %fd288;
	}
	add.s32 	%r39, %r38, -1048576;
	mov.b64 	%fd288, {%r37, %r39};
	add.s32 	%r53, %r53, 1;

BB84_27:
	add.f64 	%fd238, %fd288, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd237,%fd238;
	// inline asm
	neg.f64 	%fd239, %fd238;
	fma.rn.f64 	%fd241, %fd239, %fd237, %fd217;
	fma.rn.f64 	%fd242, %fd241, %fd241, %fd241;
	fma.rn.f64 	%fd243, %fd242, %fd237, %fd237;
	add.f64 	%fd244, %fd288, 0dBFF0000000000000;
	mul.f64 	%fd245, %fd244, %fd243;
	fma.rn.f64 	%fd246, %fd244, %fd243, %fd245;
	mul.f64 	%fd247, %fd246, %fd246;
	mov.f64 	%fd248, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd249, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd250, %fd249, %fd247, %fd248;
	mov.f64 	%fd251, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd252, %fd250, %fd247, %fd251;
	mov.f64 	%fd253, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd254, %fd252, %fd247, %fd253;
	mov.f64 	%fd255, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd256, %fd254, %fd247, %fd255;
	mov.f64 	%fd257, 0d3F624924923BE72D;
	fma.rn.f64 	%fd258, %fd256, %fd247, %fd257;
	mov.f64 	%fd259, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd260, %fd258, %fd247, %fd259;
	mov.f64 	%fd261, 0d3FB5555555555554;
	fma.rn.f64 	%fd262, %fd260, %fd247, %fd261;
	sub.f64 	%fd263, %fd244, %fd246;
	add.f64 	%fd264, %fd263, %fd263;
	neg.f64 	%fd265, %fd246;
	fma.rn.f64 	%fd266, %fd265, %fd244, %fd264;
	mul.f64 	%fd267, %fd243, %fd266;
	mul.f64 	%fd268, %fd247, %fd262;
	fma.rn.f64 	%fd269, %fd268, %fd246, %fd267;
	xor.b32  	%r40, %r53, -2147483648;
	mov.u32 	%r41, 1127219200;
	mov.b64 	%fd270, {%r40, %r41};
	mov.u32 	%r42, -2147483648;
	mov.b64 	%fd271, {%r42, %r41};
	sub.f64 	%fd272, %fd270, %fd271;
	mov.f64 	%fd273, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd274, %fd272, %fd273, %fd246;
	neg.f64 	%fd275, %fd272;
	fma.rn.f64 	%fd276, %fd275, %fd273, %fd274;
	sub.f64 	%fd277, %fd276, %fd246;
	sub.f64 	%fd278, %fd269, %fd277;
	mov.f64 	%fd279, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd280, %fd272, %fd279, %fd278;
	add.f64 	%fd289, %fd274, %fd280;
	bra.uni 	BB84_28;

BB84_1:
	setp.gt.s32	%p2, %r50, 1073217535;
	@%p2 bra 	BB84_15;
	bra.uni 	BB84_2;

BB84_15:
	add.f64 	%fd145, %fd24, 0dC000000000000000;
	mov.f64 	%fd146, 0dBE71FA71D78C0EE2;
	mov.f64 	%fd147, 0d3E452636124338B3;
	fma.rn.f64 	%fd148, %fd147, %fd145, %fd146;
	mov.f64 	%fd149, 0d3E8D111F31E61306;
	fma.rn.f64 	%fd150, %fd148, %fd145, %fd149;
	mov.f64 	%fd151, 0dBEA0502BBE1B2706;
	fma.rn.f64 	%fd152, %fd150, %fd145, %fd151;
	mov.f64 	%fd153, 0d3EB06850B2970292;
	fma.rn.f64 	%fd154, %fd152, %fd145, %fd153;
	mov.f64 	%fd155, 0dBEC108474875033D;
	fma.rn.f64 	%fd156, %fd154, %fd145, %fd155;
	mov.f64 	%fd157, 0d3ED24ACCC62909DC;
	fma.rn.f64 	%fd158, %fd156, %fd145, %fd157;
	mov.f64 	%fd159, 0dBEE3CB25209E63BE;
	fma.rn.f64 	%fd160, %fd158, %fd145, %fd159;
	mov.f64 	%fd161, 0d3EF581CBBC8CDC7B;
	fma.rn.f64 	%fd162, %fd160, %fd145, %fd161;
	mov.f64 	%fd163, 0dBF078E04B85C7597;
	fma.rn.f64 	%fd164, %fd162, %fd145, %fd163;
	mov.f64 	%fd165, 0d3F1A12730CF45051;
	fma.rn.f64 	%fd166, %fd164, %fd145, %fd165;
	mov.f64 	%fd167, 0dBF2D3FD354062012;
	fma.rn.f64 	%fd168, %fd166, %fd145, %fd167;
	mov.f64 	%fd169, 0d3F40B36B0B4DE323;
	fma.rn.f64 	%fd170, %fd168, %fd145, %fd169;
	mov.f64 	%fd171, 0dBF538AC5C6D0317A;
	fma.rn.f64 	%fd172, %fd170, %fd145, %fd171;
	mov.f64 	%fd173, 0d3F67ADD6EAAB19FC;
	fma.rn.f64 	%fd174, %fd172, %fd145, %fd173;
	mov.f64 	%fd175, 0dBF7E404FC20E4D5B;
	fma.rn.f64 	%fd176, %fd174, %fd145, %fd175;
	mov.f64 	%fd177, 0d3F951322AC7DA390;
	fma.rn.f64 	%fd178, %fd176, %fd145, %fd177;
	mov.f64 	%fd179, 0dBFB13E001A5578A3;
	fma.rn.f64 	%fd180, %fd178, %fd145, %fd179;
	mov.f64 	%fd181, 0d3FD4A34CC4A60FA3;
	fma.rn.f64 	%fd182, %fd180, %fd145, %fd181;
	mov.f64 	%fd183, 0d3FDB0EE6072093CF;
	fma.rn.f64 	%fd184, %fd182, %fd145, %fd183;
	mul.f64 	%fd290, %fd145, %fd184;
	bra.uni 	BB84_29;

BB84_17:
	add.f64 	%fd185, %fd24, 0dC008000000000000;
	mov.f64 	%fd186, 0dC1122B7730207EF3;
	mov.f64 	%fd187, 0dC0AF7040BB18FB05;
	fma.rn.f64 	%fd188, %fd187, %fd185, %fd186;
	mov.f64 	%fd189, 0dC1585A0DB81DE7D0;
	fma.rn.f64 	%fd190, %fd188, %fd185, %fd189;
	mov.f64 	%fd191, 0dC18A992B8BA94677;
	fma.rn.f64 	%fd192, %fd190, %fd185, %fd191;
	mov.f64 	%fd193, 0dC1AAC5CB6957CC20;
	fma.rn.f64 	%fd194, %fd192, %fd185, %fd193;
	mov.f64 	%fd195, 0dC1BC0E2B308774BE;
	fma.rn.f64 	%fd196, %fd194, %fd185, %fd195;
	mov.f64 	%fd197, 0dC1C6BA13DCAE7F67;
	fma.rn.f64 	%fd198, %fd196, %fd185, %fd197;
	mov.f64 	%fd199, 0dC1CCF33B9C3D120C;
	fma.rn.f64 	%fd200, %fd198, %fd185, %fd199;
	add.f64 	%fd201, %fd185, 0dC08FF62E0BE189FE;
	mov.f64 	%fd202, 0dC10074FACE10C93F;
	fma.rn.f64 	%fd203, %fd201, %fd185, %fd202;
	mov.f64 	%fd204, 0dC151B662F8D75791;
	fma.rn.f64 	%fd205, %fd203, %fd185, %fd204;
	mov.f64 	%fd206, 0dC18EE64AB4D207F7;
	fma.rn.f64 	%fd207, %fd205, %fd185, %fd206;
	mov.f64 	%fd208, 0dC1B9051687C9951A;
	fma.rn.f64 	%fd209, %fd207, %fd185, %fd208;
	mov.f64 	%fd210, 0dC1D2B866BF0B853D;
	fma.rn.f64 	%fd211, %fd209, %fd185, %fd210;
	mov.f64 	%fd212, 0dC1D4E2130E9DC133;
	fma.rn.f64 	%fd213, %fd211, %fd185, %fd212;
	div.rn.f64 	%fd214, %fd200, %fd213;
	add.f64 	%fd290, %fd185, %fd214;
	bra.uni 	BB84_29;

BB84_2:
	setp.gt.s32	%p3, %r50, 1072064101;
	@%p3 bra 	BB84_14;
	bra.uni 	BB84_3;

BB84_14:
	mov.f64 	%fd100, 0d3FF0000000000000;
	sub.f64 	%fd101, %fd100, %fd24;
	mov.f64 	%fd102, 0d3FA3EB504359EB88;
	mov.f64 	%fd103, 0d3F881F6D2A4C4310;
	fma.rn.f64 	%fd104, %fd103, %fd101, %fd102;
	mov.f64 	%fd105, 0d3FAE35D8DEB06317;
	fma.rn.f64 	%fd106, %fd104, %fd101, %fd105;
	mov.f64 	%fd107, 0d3FAED469A8B6ECCE;
	fma.rn.f64 	%fd108, %fd106, %fd101, %fd107;
	mov.f64 	%fd109, 0d3FACC1B1C357BEFE;
	fma.rn.f64 	%fd110, %fd108, %fd101, %fd109;
	mov.f64 	%fd111, 0d3FAD7154DB67F79F;
	fma.rn.f64 	%fd112, %fd110, %fd101, %fd111;
	mov.f64 	%fd113, 0d3FAFCC622CF2F7BB;
	fma.rn.f64 	%fd114, %fd112, %fd101, %fd113;
	mov.f64 	%fd115, 0d3FB11747A4D1CC43;
	fma.rn.f64 	%fd116, %fd114, %fd101, %fd115;
	mov.f64 	%fd117, 0d3FB24CE16A21B8AC;
	fma.rn.f64 	%fd118, %fd116, %fd101, %fd117;
	mov.f64 	%fd119, 0d3FB3B1C21A7BCB00;
	fma.rn.f64 	%fd120, %fd118, %fd101, %fd119;
	mov.f64 	%fd121, 0d3FB556723452ED57;
	fma.rn.f64 	%fd122, %fd120, %fd101, %fd121;
	mov.f64 	%fd123, 0d3FB748C00891544F;
	fma.rn.f64 	%fd124, %fd122, %fd101, %fd123;
	mov.f64 	%fd125, 0d3FB9A0207808CF40;
	fma.rn.f64 	%fd126, %fd124, %fd101, %fd125;
	mov.f64 	%fd127, 0d3FBC80673B8AE26B;
	fma.rn.f64 	%fd128, %fd126, %fd101, %fd127;
	mov.f64 	%fd129, 0d3FC010B364B7E555;
	fma.rn.f64 	%fd130, %fd128, %fd101, %fd129;
	mov.f64 	%fd131, 0d3FC2703A1D239658;
	fma.rn.f64 	%fd132, %fd130, %fd101, %fd131;
	mov.f64 	%fd133, 0d3FC5B40CB1137E6E;
	fma.rn.f64 	%fd134, %fd132, %fd101, %fd133;
	mov.f64 	%fd135, 0d3FCA8B9C17AC4F03;
	fma.rn.f64 	%fd136, %fd134, %fd101, %fd135;
	mov.f64 	%fd137, 0d3FD151322AC7CB52;
	fma.rn.f64 	%fd138, %fd136, %fd101, %fd137;
	mov.f64 	%fd139, 0d3FD9A4D55BEAB1D4;
	fma.rn.f64 	%fd140, %fd138, %fd101, %fd139;
	mov.f64 	%fd141, 0d3FEA51A6625307D6;
	fma.rn.f64 	%fd142, %fd140, %fd101, %fd141;
	mov.f64 	%fd143, 0d3FE2788CFC6FB619;
	fma.rn.f64 	%fd144, %fd142, %fd101, %fd143;
	mul.f64 	%fd290, %fd101, %fd144;
	bra.uni 	BB84_29;

BB84_19:
	abs.f64 	%fd234, %fd24;
	setp.gtu.f64	%p16, %fd234, 0d7FF0000000000000;
	@%p16 bra 	BB84_22;
	bra.uni 	BB84_20;

BB84_22:
	add.f64 	%fd289, %fd24, %fd24;
	bra.uni 	BB84_28;

BB84_3:
	mov.f64 	%fd25, 0d3EA7B77CEB0625E8;
	mov.f64 	%fd26, 0dBE7844988BFE6590;
	fma.rn.f64 	%fd27, %fd26, %fd24, %fd25;
	mov.f64 	%fd28, 0dBE998C69C8710CC4;
	fma.rn.f64 	%fd29, %fd27, %fd24, %fd28;
	mov.f64 	%fd30, 0dBEF6527A5A11CF6E;
	fma.rn.f64 	%fd31, %fd29, %fd24, %fd30;
	mov.f64 	%fd32, 0d3F20EC2950B1B5DE;
	fma.rn.f64 	%fd33, %fd31, %fd24, %fd32;
	mov.f64 	%fd34, 0dBF2C4D80C24BA278;
	fma.rn.f64 	%fd35, %fd33, %fd24, %fd34;
	mov.f64 	%fd36, 0dBF5315B4E8CC0D09;
	fma.rn.f64 	%fd37, %fd35, %fd24, %fd36;
	mov.f64 	%fd38, 0d3F7D917F15D50020;
	fma.rn.f64 	%fd39, %fd37, %fd24, %fd38;
	mov.f64 	%fd40, 0dBF83B4ABB41CB6FA;
	fma.rn.f64 	%fd41, %fd39, %fd24, %fd40;
	mov.f64 	%fd42, 0dBFA59AF1275B7120;
	fma.rn.f64 	%fd43, %fd41, %fd24, %fd42;
	mov.f64 	%fd44, 0d3FC5512321A168A0;
	fma.rn.f64 	%fd45, %fd43, %fd24, %fd44;
	mov.f64 	%fd46, 0dBFA5815E8FDCE74C;
	fma.rn.f64 	%fd47, %fd45, %fd24, %fd46;
	mov.f64 	%fd48, 0dBFE4FCF4026ADD1A;
	fma.rn.f64 	%fd49, %fd47, %fd24, %fd48;
	mov.f64 	%fd50, 0d3FE2788CFC6FB5C8;
	fma.rn.f64 	%fd51, %fd49, %fd24, %fd50;
	mul.f64 	%fd52, %fd51, %fd24;
	fma.rn.f64 	%fd1, %fd52, %fd24, %fd24;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r46}, %fd1;
	}
	setp.gt.f64	%p4, %fd1, 0d0000000000000000;
	setp.lt.s32	%p5, %r46, 2146435072;
	and.pred  	%p6, %p4, %p5;
	@%p6 bra 	BB84_8;
	bra.uni 	BB84_4;

BB84_8:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r47, %temp}, %fd1;
	}
	mov.u32 	%r48, -1023;
	setp.gt.s32	%p10, %r46, 1048575;
	@%p10 bra 	BB84_10;

	mul.f64 	%fd55, %fd1, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r46}, %fd55;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r47, %temp}, %fd55;
	}
	mov.u32 	%r48, -1077;

BB84_10:
	shr.u32 	%r23, %r46, 20;
	add.s32 	%r49, %r48, %r23;
	and.b32  	%r24, %r46, -2146435073;
	or.b32  	%r25, %r24, 1072693248;
	mov.b64 	%fd286, {%r47, %r25};
	setp.lt.s32	%p11, %r25, 1073127583;
	@%p11 bra 	BB84_12;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r26, %temp}, %fd286;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r27}, %fd286;
	}
	add.s32 	%r28, %r27, -1048576;
	mov.b64 	%fd286, {%r26, %r28};
	add.s32 	%r49, %r49, 1;

BB84_12:
	add.f64 	%fd57, %fd286, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd56,%fd57;
	// inline asm
	neg.f64 	%fd58, %fd57;
	mov.f64 	%fd59, 0d3FF0000000000000;
	fma.rn.f64 	%fd60, %fd58, %fd56, %fd59;
	fma.rn.f64 	%fd61, %fd60, %fd60, %fd60;
	fma.rn.f64 	%fd62, %fd61, %fd56, %fd56;
	add.f64 	%fd63, %fd286, 0dBFF0000000000000;
	mul.f64 	%fd64, %fd63, %fd62;
	fma.rn.f64 	%fd65, %fd63, %fd62, %fd64;
	mul.f64 	%fd66, %fd65, %fd65;
	mov.f64 	%fd67, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd68, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd69, %fd68, %fd66, %fd67;
	mov.f64 	%fd70, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd71, %fd69, %fd66, %fd70;
	mov.f64 	%fd72, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd73, %fd71, %fd66, %fd72;
	mov.f64 	%fd74, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd75, %fd73, %fd66, %fd74;
	mov.f64 	%fd76, 0d3F624924923BE72D;
	fma.rn.f64 	%fd77, %fd75, %fd66, %fd76;
	mov.f64 	%fd78, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd79, %fd77, %fd66, %fd78;
	mov.f64 	%fd80, 0d3FB5555555555554;
	fma.rn.f64 	%fd81, %fd79, %fd66, %fd80;
	sub.f64 	%fd82, %fd63, %fd65;
	add.f64 	%fd83, %fd82, %fd82;
	neg.f64 	%fd84, %fd65;
	fma.rn.f64 	%fd85, %fd84, %fd63, %fd83;
	mul.f64 	%fd86, %fd62, %fd85;
	mul.f64 	%fd87, %fd66, %fd81;
	fma.rn.f64 	%fd88, %fd87, %fd65, %fd86;
	xor.b32  	%r29, %r49, -2147483648;
	mov.u32 	%r30, 1127219200;
	mov.b64 	%fd89, {%r29, %r30};
	mov.u32 	%r31, -2147483648;
	mov.b64 	%fd90, {%r31, %r30};
	sub.f64 	%fd91, %fd89, %fd90;
	mov.f64 	%fd92, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd93, %fd91, %fd92, %fd65;
	neg.f64 	%fd94, %fd91;
	fma.rn.f64 	%fd95, %fd94, %fd92, %fd93;
	sub.f64 	%fd96, %fd95, %fd65;
	sub.f64 	%fd97, %fd88, %fd96;
	mov.f64 	%fd98, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd99, %fd91, %fd98, %fd97;
	add.f64 	%fd287, %fd93, %fd99;
	bra.uni 	BB84_13;

BB84_20:
	setp.eq.f64	%p17, %fd24, 0d0000000000000000;
	mov.f64 	%fd289, 0dFFF0000000000000;
	@%p17 bra 	BB84_28;

	setp.eq.f64	%p18, %fd24, 0d7FF0000000000000;
	selp.f64	%fd289, %fd24, 0dFFF8000000000000, %p18;

BB84_28:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r43}, %fd289;
	}
	add.s32 	%r44, %r43, -1048576;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r45, %temp}, %fd289;
	}
	mov.b64 	%fd281, {%r45, %r44};
	add.f64 	%fd282, %fd24, 0dBFE0000000000000;
	fma.rn.f64 	%fd283, %fd281, %fd282, %fd14;
	fma.rn.f64 	%fd284, %fd281, %fd282, %fd13;
	add.f64 	%fd285, %fd283, %fd284;
	setp.eq.f64	%p21, %fd24, 0d7FF0000000000000;
	selp.f64	%fd290, %fd24, %fd285, %p21;
	bra.uni 	BB84_29;

BB84_4:
	abs.f64 	%fd53, %fd1;
	setp.gtu.f64	%p7, %fd53, 0d7FF0000000000000;
	@%p7 bra 	BB84_7;
	bra.uni 	BB84_5;

BB84_7:
	add.f64 	%fd287, %fd1, %fd1;
	bra.uni 	BB84_13;

BB84_5:
	setp.eq.f64	%p8, %fd1, 0d0000000000000000;
	mov.f64 	%fd287, 0dFFF0000000000000;
	@%p8 bra 	BB84_13;

	setp.eq.f64	%p9, %fd1, 0d7FF0000000000000;
	selp.f64	%fd287, %fd1, 0dFFF8000000000000, %p9;

BB84_13:
	neg.f64 	%fd290, %fd287;

BB84_29:
	st.param.f64	[func_retval0+0], %fd290;
	ret;
}


